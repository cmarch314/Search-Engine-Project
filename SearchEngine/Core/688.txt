{"text": "Data Compression 3 STATIC DEFINED WORD SCHEMES The classic defined word scheme was developed over 3 years ago in Huffman s well known paper on minimum redundancy coding Huffman 1952 Huffman s algorithm provided the first solution to the problem of constructing minimum redundancy codes Many people believe that Huffman coding cannot be improved upon that is that it is guaranteed to achieve the best possible compression ratio This is only true however under the constraints that each source message is mapped to a unique codeword and that the compressed text is the concatenation of the codewords for the source messages An earlier algorithm due independently to Shannon and Fano Shannon and Weaver 1949 Fano 1949 is not guaranteed to provide optimal codes but approaches optimal behavior as the number of messages approaches infinity The Huffman algorithm is also of importance because it has provided a foundation upon which other data compression techniques have built and a benchmark to which they may be compared We classify the codes generated by the Huffman and Shannon Fano algorithms as variable variable and note that they include block variable codes as a special case depending upon how the source messages are defined In Section 3 3 codes which map the integers onto binary codewords are discussed Since any finite alphabet may be enumerated this type of code has general purpose utility However a more common use of these codes called universal codes is in conjunction with an adaptive scheme This connection is discussed in Section 5 2 Arithmetic coding presented in Section 3 4 takes a significantly different approach to data compression from that of the other static methods It does not construct a code in the sense of a mapping from source messages to codewords Instead arithmetic coding replaces the source ensemble by a code string which unlike all of the other codes discussed here is not the concatenation of codewords corresponding to individual source messages Arithmetic coding is capable of achieving compression results which are arbitrarily close to the entropy of the source 3 1 Shannon Fano Coding The Shannon Fano technique has as an advantage its simplicity The code is constructed as follows the source messages a i and their probabilities p a i are listed in order of nonincreasing probability This list is then divided in such a way as to form two groups of as nearly equal total probabilities as possible Each message in the first group receives as the first digit of its codeword the messages in the second half have codewords beginning with 1 Each of these groups is then divided according to the same criterion and additional code digits are appended The process is continued until each subset contains only one message Clearly the Shannon Fano algorithm yields a minimal prefix code a 1 2 b 1 4 1 c 1 8 11 d 1 16 111 e 1 32 1111 f 1 32 11111 Figure 3 1 A Shannon Fano Code Figure 3 1 shows the application of the method to a particularly simple probability distribution The length of each codeword x is equal to lg p x This is true as long as it is possible to divide the list into subgroups of exactly equal probability When this is not possible some codewords may be of length lg p x 1 The Shannon Fano algorithm yields an average codeword length S which satisfies H S H 1 In Figure 3 2 the Shannon Fano code for ensemble EXAMPLE is given As is often the case the average codeword length is the same as that achieved by the Huffman code see Figure 1 3 That the Shannon Fano algorithm is not guaranteed to produce an optimal code is demonstrated by the following set of probabilities 35 17 17 16 15 The Shannon Fano code for this distribution is compared with the Huffman code in Section 3 2 g 8 4 f 7 4 1 e 6 4 11 d 5 4 1 space 5 4 1 1 c 4 4 11 b 3 4 111 a 2 4 1111 Figure 3 2 A Shannon Fano Code for EXAMPLE code length 117 3 2 Static Huffman Coding Huffman s algorithm expressed graphically takes as input a list of nonnegative weights w 1 w n and constructs a full binary tree a binary tree is full if every node has either zero or two children whose leaves are labeled with the weights When the Huffman algorithm is used to construct a code the weights represent the probabilities associated with the source letters Initially there is a set of singleton trees one for each weight in the list At each step in the algorithm the trees corresponding to the two smallest weights w i and w j are merged into a new tree whose weight is w i w j and whose root has two children which are the subtrees represented by w i and w j The weights w i and w j are removed from the list and w i w j is inserted into the list This process continues until the weight list contains a single value If at any time there is more than one way to choose a smallest pair of weights any such pair may be chosen In Huffman s paper the process begins with a nonincreasing list of weights This detail is not important to the correctness of the algorithm but it does provide a more efficient implementation Huffman 1952 The Huffman algorithm is demonstrated in Figure 3 3 Figure 3 3 The Huffman process a The list b the tree The Huffman algorithm determines the lengths of the codewords to be mapped to each of the source letters a i There are many alternatives for specifying the actual digits it is necessary only that the code have the prefix property The usual assignment entails labeling the edge from each parent to its left child with the digit and the edge to the right child with 1 The codeword for each source letter is the sequence of labels along the path from the root to the leaf node representing that letter The codewords for the source of Figure 3 3 in order of decreasing probability are 1 11 1 1 1 1 1 Clearly this process yields a minimal prefix code Further the algorithm is guaranteed to produce an optimal minimum redundancy code Huffman 1952 Gallager has proved an upper bound on the redundancy of a Huffman code of p n lg 2 lg e e which is approximately p n 86 where p n is the probability of the least likely source message Gallager 1978 In a recent paper Capocelli et al provide new bounds which are tighter than those of Gallagher for some probability distributions Capocelli et al 1986 Figure 3 4 shows a distribution for which the Huffman code is optimal while the Shannon Fano code is not In addition to the fact that there are many ways of forming codewords of appropriate lengths there are cases in which the Huffman algorithm does not uniquely determine these lengths due to the arbitrary choice among equal minimum weights As an example codes with codeword lengths of 1 2 3 4 4 and of 2 2 2 3 3 both yield the same average codeword length for a source with probabilities 4 2 2 1 1 Schwartz defines a variation of the Huffman algorithm which performs bottom merging that is orders a new parent node above existing nodes of the same weight and always merges the last two weights in the list The code constructed is the Huffman code with minimum values of maximum codeword length MAX l i and total codeword length SUM l i Schwartz 1964 Schwartz and Kallick describe an implementation of Huffman s algorithm with bottom merging Schwartz and Kallick 1964 The Schwartz Kallick algorithm and a later algorithm by Connell Connell 1973 use Huffman s procedure to determine the lengths of the codewords and actual digits are assigned so that the code has the numerical sequence property That is codewords of equal length form a consecutive sequence of binary numbers Shannon Fano codes also have the numerical sequence property This property can be exploited to achieve a compact representation of the code and rapid encoding and decoding S F Huffman a 1 35 1 a 2 17 1 11 a 3 17 1 1 a 4 16 11 1 a 5 15 111 Average codeword length 2 31 2 3 Figure 3 4 Comparison of Shannon Fano and Huffman Codes Both the Huffman and the Shannon Fano mappings can be generated in O n time where n is the number of messages in the source ensemble assuming that the weights have been presorted Each of these algorithms maps a source message a i with probability p to a codeword of length l lg p l lg p 1 Encoding and decoding times depend upon the representation of the mapping If the mapping is stored as a binary tree then decoding the codeword for a i involves following a path of length l in the tree A table indexed by the source messages could be used for encoding the code for a i would be stored in position i of the table and encoding time would be O l Connell s algorithm makes use of the index of the Huffman code a representation of the distribution of codeword lengths to encode and decode in O c time where c is the number of different codeword lengths Tanaka presents an implementation of Huffman coding based on finite state machines which can be realized efficiently in either hardware or software Tanaka 1987 As noted earlier the redundancy bound for Shannon Fano codes is 1 and the bound for the Huffman method is p n 86 where p n is the probability of the least likely source message so p n is less than or equal to 5 and generally much less It is important to note that in defining redundancy to be average codeword length minus entropy the cost of transmitting the code mapping computed by these algorithms is ignored The overhead cost for any method where the source alphabet has not been established prior to transmission includes n lg n bits for sending the n source letters For a Shannon Fano code a list of codewords ordered so as to correspond to the source letters could be transmitted The additional time required is then SUM l i where the l i are the lengths of the codewords For Huffman coding an encoding of the shape of the code tree might be transmitted Since any full binary tree may be a legal Huffman code tree encoding tree shape may require as many as lg 4 n 2n bits In most cases the message ensemble is very large so that the number of bits of overhead is minute by comparison to the total length of the encoded transmission However it is imprudent to ignore this cost If a less than optimal code is acceptable the overhead costs can be avoided through a prior agreement by sender and receiver as to the code mapping Rather than using a Huffman code based upon the characteristics of the current message ensemble the code used could be based on statistics for a class of transmissions to which the current ensemble is assumed to belong That is both sender and receiver could have access to a codebook with k mappings in it one for Pascal source one for English text etc The sender would then simply alert the receiver as to which of the common codes he is using This requires only lg k bits of overhead Assuming that classes of transmission with relatively stable characteristics could be identified this hybrid approach would greatly reduce the redundancy due to overhead without significantly increasing expected codeword length In addition the cost of computing the mapping would be amortized over all files of a given class That is the mapping would be computed once on a statistically significant sample and then used on a great number of files for which the sample is representative There is clearly a substantial risk associated with assumptions about file characteristics and great care would be necessary in choosing both the sample from which the mapping is to be derived and the categories into which to partition transmissions An extreme example of the risk associated with the codebook approach is provided by author Ernest V Wright who wrote a novel Gadsby 1939 containing no occurrences of the letter E Since E is the most commonly used letter in the English language an encoding based upon a sample from Gadsby would be disastrous if used with normal examples of English text Similarly the normal encoding would provide poor compression of Gadsby McIntyre and Pechura describe an experiment in which the codebook approach is compared to static Huffman coding McIntyre and Pechura 1985 The sample used for comparison is a collection of 53 source programs in four languages The codebook contains a Pascal code tree a FORTRAN code tree a COBOL code tree a PL 1 code tree and an ALL code tree The Pascal code tree is the result of applying the static Huffman algorithm to the combined character frequencies of all of the Pascal programs in the sample The ALL code tree is based upon the combined character frequencies for all of the programs The experiment involves encoding each of the programs using the five codes in the codebook and the static Huffman algorithm The data reported for each of the 53 programs consists of the size of the coded program for each of the five predetermined codes and the size of the coded program plus the size of the mapping in table form for the static Huffman method In every case the code tree for the language class to which the program belongs generates the most compact encoding Although using the Huffman algorithm on the program itself yields an optimal mapping the overhead cost is greater than the added redundancy incurred by the less than optimal code In many cases the ALL code tree also generates a more compact encoding than the static Huffman algorithm In the worst case an encoding constructed from the codebook is only 6 6 larger than that constructed by the Huffman algorithm These results suggest that for files of source code the codebook approach may be appropriate Gilbert discusses the construction of Huffman codes based on inaccurate source probabilities Gilbert 1971 A simple solution to the problem of incomplete knowledge of the source is to avoid long codewords thereby minimizing the error of underestimating badly the probability of a message The problem becomes one of constructing the optimal binary tree subject to a height restriction see Knuth 1971 Hu and Tan 1972 Garey 1974 Another approach involves collecting statistics for several sources and then constructing a code based upon some combined criterion This approach could be applied to the problem of designing a single code for use with English French German etc sources To accomplish this Huffman s algorithm could be used to minimize either the average codeword length for the combined source probabilities or the average codeword length for English subject to constraints on average codeword lengths for the other sources 3 3 Universal Codes and Representations of the Integers A code is universal if it maps source messages to codewords so that the resulting average codeword length is bounded by c1 H c2 That is given an arbitrary source with nonzero entropy a universal code achieves average codeword length which is at most a constant times the optimal possible for that source The potential compression offered by a universal code clearly depends on the magnitudes of the constants c1 and c2 We recall the definition of an asymptotically optimal code as one for which average codeword length approaches entropy and remark that a universal code with c1 1 is asymptotically optimal An advantage of universal codes over Huffman codes is that it is not necessary to know the exact probabilities with which the source messages appear While Huffman coding is not applicable unless the probabilities are known it is sufficient in the case of universal coding to know the probability distribution only to the extent that the source messages can be ranked in probability order By mapping messages in order of decreasing probability to codewords in order of increasing length universality can be achieved Another advantage to universal codes is that the codeword sets are fixed It is not necessary to compute a codeword set based upon the statistics of an ensemble any universal codeword set will suffice as long as the source messages are ranked The encoding and decoding processes are thus simplified While universal codes can be used instead of Huffman codes as general purpose static schemes the more common application is as an adjunct to a dynamic scheme This type of application will be demonstrated in Section 5 Since the ranking of source messages is the essential parameter in universal coding we may think of a universal code as representing an enumeration of the source messages or as representing the integers which provide an enumeration Elias defines a sequence of universal coding schemes which map the set of positive integers onto the set of binary codewords Elias 1975 gamma delta 1 1 1 2 1 1 3 11 1 1 4 1 11 5 1 1 11 1 6 11 111 7 111 1111 8 1 1 16 1 1 1 17 1 1 1 1 1 32 1 11 Figure 3 5 Elias Codes The first Elias code is one which is simple but not optimal This code gamma maps an integer x onto the binary value of x prefaced by floor lg x zeros The binary value of x is expressed in as few bits as possible and therefore begins with a 1 which serves to delimit the prefix The result is an instantaneously decodable code since the total length of a codeword is exactly one greater than twice the number of zeros in the prefix therefore as soon as the first 1 of a codeword is encountered its length is known The code is not a minimum redundancy code since the ratio of expected codeword length to entropy goes to 2 as entropy approaches infinity The second code delta maps an integer x to a codeword consisting of gamma floor lg x 1 followed by the binary value of x with the leading 1 deleted The resulting codeword has length floor lg x 2 floor lg 1 floor lg x 1 This concept can be applied recursively to shorten the codeword lengths but the benefits decrease rapidly The code delta is asymptotically optimal since the limit of the ratio of expected codeword length to entropy is 1 Figure 3 5 lists the values of gamma and delta for a sampling of the integers Figure 3 6 shows an Elias code for string EXAMPLE The number of bits transmitted using this mapping would be 161 which does not compare well with the 117 bits transmitted by the Huffman code of Figure 1 3 Huffman coding is optimal under the static mapping model Even an asymptotically optimal universal code cannot compare with static Huffman coding on a source for which the probabilities of the messages are known Source Frequency Rank Codeword message g 8 1 delta 1 1 f 7 2 delta 2 1 e 6 3 delta 3 1 1 d 5 4 delta 4 11 space 5 5 delta 5 11 1 c 4 6 delta 6 111 b 3 7 delta 7 1111 a 2 8 delta 8 1 Figure 3 6 An Elias Code for EXAMPLE code length 161 A second sequence of universal coding schemes based on the Fibonacci numbers is defined by Apostolico and Fraenkel Apostolico and Fraenkel 1985 While the Fibonacci codes are not asymptotically optimal they compare well to the Elias codes as long as the number of source messages is not too large Fibonacci codes have the additional attribute of robustness which manifests itself by the local containment of errors This aspect of Fibonacci codes will be discussed further in Section 7 The sequence of Fibonacci codes described by Apostolico and Fraenkel is based on the Fibonacci numbers of order m 2 where the Fibonacci numbers of order 2 are the standard Fibonacci numbers 1 1 2 3 5 8 13 In general the Fibonnaci numbers of order m are defined by the recurrence Fibonacci numbers F m 1 through F are equal to 1 the kth number for k 1 is the sum of the preceding m numbers We describe only the order 2 Fibonacci code the extension to higher orders is straightforward N R N F N 1 1 11 2 1 11 3 1 11 4 1 1 1 11 5 1 11 6 1 1 1 11 7 1 1 1 11 8 1 11 16 1 1 1 11 32 1 1 1 1 1 11 21 13 8 5 3 2 1 Figure 3 7 Fibonacci Representations and Fibonacci Codes Every nonnegative integer N has precisely one binary representation of the form R N SUM i to k d i F i where d i is in 1 k N and the F i are the order 2 Fibonacci numbers as defined above such that there are no adjacent ones in the representation The Fibonacci representations for a small sampling of the integers are shown in Figure 3 7 using the standard bit sequence from high order to low The bottom row of the figure gives the values of the bit positions It is immediately obvious that this Fibonacci representation does not constitute a prefix code The order 2 Fibonacci code for N is defined to be F N D1 where D d d 1 d 2 d k the d i defined above That is the Fibonacci representation is reversed and 1 is appended The Fibonacci code values for a small subset of the integers are given in Figure 3 7 These binary codewords form a prefix code since every codeword now terminates in two consecutive ones which cannot appear anywhere else in a codeword Fraenkel and Klein prove that the Fibonacci code of order 2 is universal with c1 2 and c2 3 Fraenkel and Klein 1985 It is not asymptotically optimal since c1 1 Fraenkel and Klein also show that Fibonacci codes of higher order compress better than the order 2 code if the source language is large enough i e the number of distinct source messages is large and the probability distribution is nearly uniform However no Fibonacci code is asymptotically optimal The Elias codeword delta N is asymptotically shorter than any Fibonacci codeword for N but the integers in a very large initial range have shorter Fibonacci codewords For m 2 for example the transition point is N 514 228 Apostolico and Fraenkel 1985 Thus a Fibonacci code provides better compression than the Elias code until the size of the source language becomes very large Figure 3 8 shows a Fibonacci code for string EXAMPLE The number of bits transmitted using this mapping would be 153 which is an improvement over the Elias code of Figure 3 6 but still compares poorly with the Huffman code of Figure 1 3 Source Frequency Rank Codeword message g 8 1 F 1 11 f 7 2 F 2 11 e 6 3 F 3 11 d 5 4 F 4 1 11 space 5 5 F 5 11 c 4 6 F 6 1 11 b 3 7 F 7 1 11 a 2 8 F 8 11 Figure 3 8 A Fibonacci Code for EXAMPLE code length 153 3 4 Arithmetic Coding The method of arithmetic coding was suggested by Elias and presented by Abramson in his text on Information Theory Abramson 1963 Implementations of Elias technique were developed by Rissanen Pasco Rubin and most recently Witten et al Rissanen 1976 Pasco 1976 Rubin 1979 Witten et al 1987 We present the concept of arithmetic coding first and follow with a discussion of implementation details and performance In arithmetic coding a source ensemble is represented by an interval between and 1 on the real number line Each symbol of the ensemble narrows this interval As the interval becomes smaller the number of bits needed to specify it grows Arithmetic coding assumes an explicit probabilistic model of the source It is a defined word scheme which uses the probabilities of the source messages to successively narrow the interval used to represent the ensemble A high probability message narrows the interval less than a low probability message so that high probability messages contribute fewer bits to the coded ensemble The method begins with an unordered list of source messages and their probabilities The number line is partitioned into subintervals based on cumulative probabilities A small example will be used to illustrate the idea of arithmetic coding Given source messages A B C D with probabilities 2 4 1 2 1 Figure 3 9 demonstrates the initial partitioning of the number line The symbol A corresponds to the first 1 5 of the interval 1 B the next 2 5 D the subinterval of size 1 5 which begins 7 of the way from the left endpoint to the right When encoding begins the source ensemble is represented by the entire interval 1 For the ensemble AADB the first A reduces the interval to 2 and the second A to 4 the first 1 5 of the previous interval The D further narrows the interval to 28 36 1 5 of the previous size beginning 7 of the distance from left to right The B narrows the interval to 296 328 and the yields a final interval of 3248 328 The interval or alternatively any number i within the interval may now be used to represent the source ensemble Source Probability Cumulative Range message probability A 2 2 2 B 4 6 2 6 C 1 7 6 7 D 2 9 7 9 1 1 9 1 Figure 3 9 The Arithmetic coding model Two equations may be used to define the narrowing process described above newleft prevleft msgleft prevsize 1 newsize prevsize msgsize 2 The first equation states that the left endpoint of the new interval is calculated from the previous interval and the current source message The left endpoint of the range associated with the current message specifies what percent of the previous interval to remove from the left in order to form the new interval For D in the above example the new left endpoint is moved over by 7 4 7 of the size of the previous interval The second equation computes the size of the new interval from the previous interval size and the probability of the current message which is equivalent to the size of its associated range Thus the size of the interval determined by D is 4 2 and the right endpoint is 28 8 36 left endpoint size The size of the final subinterval determines the number of bits needed to specify a number in that range The number of bits needed to specify a subinterval of 1 of size s is lg s Since the size of the final subinterval is the product of the probabilities of the source messages in the ensemble that is s PROD i 1 to N p source message i where N is the length of the ensemble we have lg s SUM i 1 to N lg p source message i SUM i 1 to n p a i lg p a i where n is the number of unique source messages a 1 a 2 a n Thus the number of bits generated by the arithmetic coding technique is exactly equal to entropy H This demonstrates the fact that arithmetic coding achieves compression which is almost exactly that predicted by the entropy of the source In order to recover the original ensemble the decoder must know the model of the source used by the encoder eg the source messages and associated ranges and a single number within the interval determined by the encoder Decoding consists of a series of comparisons of the number i to the ranges representing the source messages For this example i might be 325 3248 326 or 327 would all do just as well The decoder uses i to simulate the actions of the encoder Since i lies between and 2 he deduces that the first letter was A since the range 2 corresponds to source message A This narrows the interval to 2 The decoder can now deduce that the next message will further narrow the interval in one of the following ways to 4 for A to 4 12 for B to 12 14 for C to 14 18 for D and to 18 2 for Since i falls into the interval 4 he knows that the second message is again A This process continues until the entire ensemble has been recovered Several difficulties become evident when implementation of arithmetic coding is attempted The first is that the decoder needs some way of knowing when to stop As evidence of this the number could represent any of the source ensembles A AA AAA etc Two solutions to this problem have been suggested One is that the encoder transmit the size of the ensemble as part of the description of the model Another is that a special symbol be included in the model for the purpose of signaling end of message The in the above example serves this purpose The second alternative is preferable for several reasons First sending the size of the ensemble requires a two pass process and precludes the use of arithmetic coding as part of a hybrid codebook scheme see Sections 1 2 and 3 2 Secondly adaptive methods of arithmetic coding are easily developed and a first pass to determine ensemble size is inappropriate in an on line adaptive scheme A second issue left unresolved by the fundamental concept of arithmetic coding is that of incremental transmission and reception It appears from the above discussion that the encoding algorithm transmits nothing until the final interval is determined However this delay is not necessary As the interval narrows the leading bits of the left and right endpoints become the same Any leading bits that are the same may be transmitted immediately as they will not be affected by further narrowing A third issue is that of precision From the description of arithmetic coding it appears that the precision required grows without bound as the length of the ensemble grows Witten et al and Rubin address this issue Witten et al 1987 Rubin 1979 Fixed precision registers may be used as long as underflow and overflow are detected and managed The degree of compression achieved by an implementation of arithmetic coding is not exactly H as implied by the concept of arithmetic coding Both the use of a message terminator and the use of fixed length arithmetic reduce coding effectiveness However it is clear that an end of message symbol will not have a significant effect on a large source ensemble Witten et al approximate the overhead due to the use of fixed precision at 1 4 bits per source message which is also negligible The arithmetic coding model for ensemble EXAMPLE is given in Figure 3 1 The final interval size is p a 2 p b 3 p c 4 p d 5 p e 6 p f 7 p g 8 p space 5 The number of bits needed to specify a value in the interval is lg 1 44 1 35 115 7 So excluding overhead arithmetic coding transmits EXAMPLE in 116 bits one less bit than static Huffman coding Source Probability Cumulative Range message probability a 5 5 5 b 75 125 5 125 c 1 225 125 225 d 125 35 225 35 e 15 5 35 5 f 175 675 5 675 g 2 875 675 875 space 125 1 875 1 Figure 3 1 The Arithmetic coding model of EXAMPLE Witten et al provide an implementation of arithmetic coding written in C which separates the model of the source from the coding process where the coding process is defined by Equations 3 1 and 3 2 Witten et al 1987 The model is in a separate program module and is consulted by the encoder and by the decoder at every step in the processing The fact that the model can be separated so easily renders the classification static adaptive irrelevent for this technique Indeed the fact that the coding method provides compression efficiency nearly equal to the entropy of the source under any model allows arithmetic coding to be coupled with any static or adaptive method for computing the probabilities or frequencies of the source messages Witten et al implement an adaptive model similar to the techniques described in Section 4 The performance of this implementation is discussed in Section 6 ", "_id": "http://www.ics.uci.edu/~dan/pubs/DC-Sec3.html", "title": " data compression -- section 3", "html": "<HTML>\n<HEAD>\n<TITLE> Data Compression -- Section 3</TITLE>\n</HEAD><BODY>\n\n<H1> Data Compression </H1>\n\n<a name=\"Sec_3\">\n<H2> 3.  STATIC DEFINED-WORD SCHEMES</H2> </a>\n\n<A HREF=\"DC-Sec2.html\"><IMG SRC=\"prev.gif\" ALT=\"PREV section\"></A>\n<A HREF=\"DC-Sec4.html\"><IMG SRC=\"next.gif\" ALT=\"NEXT section\"></A>\n<P>\n\n\tThe classic defined-word scheme was developed over 30 years\nago in Huffman's well-known paper on minimum-redundancy coding \n[Huffman 1952].  Huffman's algorithm provided the first solution \nto the problem of constructing minimum-redundancy codes.  \nMany people believe that Huffman coding cannot be improved upon, that\nis, that it is guaranteed to achieve the best possible compression \nratio.  This is only\ntrue, however, under the constraints that each source message is\nmapped to a unique codeword and that the compressed text is the\nconcatenation of the codewords for the source messages.\nAn earlier\nalgorithm, due independently to Shannon and Fano [Shannon and Weaver\n1949; Fano 1949], is not guaranteed to provide optimal codes, but \napproaches optimal behavior as the number of messages approaches\ninfinity.\nThe Huffman algorithm is also of importance because it has provided\na foundation upon which other data compression techniques \nhave built and a benchmark to which they may be compared.  \nWe classify the codes generated by the Huffman and \nShannon-Fano algorithms as variable-variable\nand note that they include block-variable codes as a special case,\ndepending upon how the source messages are defined.\n<P>\nIn <a href=\"#Sec_3.3\">Section 3.3</a> codes which map the integers onto binary codewords\nare discussed.  Since any finite alphabet may be enumerated, this type\nof code has general-purpose utility.  However, a more common use of these\ncodes (called universal codes) is in conjunction with an adaptive \nscheme.  This connection is discussed in\n<a href=\"DC-Sec5.html#Sec_5.2\">Section 5.2</a>.\n<P>\nArithmetic coding, presented in\n<a href=\"#Sec_3.4\">Section 3.4</a>, takes a significantly \ndifferent approach to data compression from that of the other static\nmethods.  It does not construct a code, in the sense of a mapping from\nsource messages to codewords.  Instead, arithmetic coding replaces\nthe source ensemble by a code string which, unlike all of the other\ncodes discussed here, is not the concatenation of codewords \ncorresponding to individual source messages.  Arithmetic coding is\ncapable of achieving compression results which are arbitrarily close\nto the entropy of the source.\n\n<a name=\"Sec_3.1\">\n<H3> 3.1  Shannon-Fano Coding</H3> </a>\n\n\tThe Shannon-Fano technique has as an advantage its simplicity.\nThe code is constructed as follows:  the source messages <VAR>a</VAR>(<VAR>i</VAR>) and their\nprobabilities <VAR>p</VAR>( <VAR>a</VAR>(<VAR>i</VAR>) ) are listed in order of nonincreasing probability.\nThis list is then divided in such a way as to form two groups of as\nnearly equal total probabilities as possible.  Each message in the first\ngroup receives 0 as the first digit of its codeword; the messages\nin the second half have codewords beginning with 1.  Each of these\ngroups is then divided according to the same criterion and\nadditional code digits are appended.  The process is continued until\neach subset contains only one message.  Clearly the Shannon-Fano\nalgorithm yields a minimal prefix code.  \n\n<PRE>\n<VAR>a</VAR>    1/2     0\n<VAR>b</VAR>    1/4     10\n<VAR>c</VAR>    1/8     110\n<VAR>d</VAR>    1/16    1110\n<VAR>e</VAR>    1/32    11110\n<VAR>f</VAR>    1/32    11111\n\nFigure 3.1 -- A Shannon-Fano Code.\n</PRE>\n\n\tFigure 3.1 shows the application of the method to a particularly\nsimple probability distribution.  The length of each\ncodeword <VAR>x</VAR> is equal to -lg p(<VAR>x</VAR>).  This is true as long as it\nis possible to divide the list into subgroups of exactly equal\nprobability.  When this is not possible, some codewords may be\nof length -lg <VAR>p</VAR>(<VAR>x</VAR>)+1.  The Shannon-Fano algorithm yields\nan average codeword length <VAR>S</VAR> which satisfies  H &lt;= S &lt;= H + 1.\nIn Figure 3.2, the Shannon-Fano code for ensemble <VAR>EXAMPLE</VAR> is\ngiven.  As is often the case, the average codeword length is the\nsame as that achieved by the Huffman code (see Figure 1.3). \nThat the Shannon-Fano algorithm is not guaranteed to produce\nan optimal code is demonstrated by the following set of probabilities:\n{ .35, .17, .17, .16, .15 }.  The Shannon-Fano code for\nthis distribution is compared with the Huffman code in\n<a href=\"#Sec_3.2\">Section 3.2</a>.\n\n<PRE>\n<VAR>g</VAR>      8/40    00\n<VAR>f</VAR>      7/40    010\n<VAR>e</VAR>      6/40    011\n<VAR>d</VAR>      5/40    100\n<VAR>space</VAR>  5/40    101\n<VAR>c</VAR>      4/40    110\n<VAR>b</VAR>      3/40    1110\n<VAR>a</VAR>      2/40    1111\n\nFigure 3.2 -- A Shannon-Fano Code for <VAR>EXAMPLE</VAR>\n              (code length=117).\n</PRE>\n\n<a name=\"Sec_3.2\">\n<H3> 3.2.  Static Huffman Coding</H3> </a>\n\n\tHuffman's algorithm, expressed graphically, takes as input\na list of nonnegative weights {<VAR>w</VAR>(1), ... ,<VAR>w</VAR>(<VAR>n</VAR>) } and constructs\na full binary tree [a binary tree is full if every\nnode has either zero or two children]\nwhose leaves are labeled with \nthe weights.  When the Huffman algorithm is used to construct a code,\nthe weights represent the probabilities associated with\nthe source letters.  Initially there is a set of singleton trees, one for\neach weight in the list.  At each step in the algorithm the trees\ncorresponding to the two \nsmallest weights, <VAR>w</VAR>(<VAR>i</VAR>) and <VAR>w</VAR>(<VAR>j</VAR>), are merged into a new tree whose\nweight is <VAR>w</VAR>(<VAR>i</VAR>)+<VAR>w</VAR>(<VAR>j</VAR>) and whose root has two children which\nare the subtrees represented \nby <VAR>w</VAR>(<VAR>i</VAR>) and <VAR>w</VAR>(<VAR>j</VAR>).  The weights <VAR>w</VAR>(<VAR>i</VAR>) and <VAR>w</VAR>(<VAR>j</VAR>) are removed from the\nlist and <VAR>w</VAR>(<VAR>i</VAR>)+<VAR>w</VAR>(<VAR>j</VAR>) is inserted into the list.  This process\ncontinues until the weight list contains a single value.  If, at\nany time, there is more than one way to choose a smallest pair\nof weights, any such pair may be chosen.  In Huffman's paper, \nthe process begins with a nonincreasing list of weights.  This\ndetail is not important to the correctness of the\nalgorithm, but it does provide a more efficient implementation [Huffman 1952].\nThe Huffman algorithm is demonstrated in Figure 3.3.\n<P>\n<IMG SRC=\"DC-fig33.gif\" ALT=\"[FIGURE 3.3]\">\n<P>\nFigure 3.3 -- The Huffman process: (a) The list; (b) the tree.\n<P>\n\tThe Huffman algorithm determines the lengths of the codewords\nto be mapped to each of the source letters <VAR>a</VAR>(<VAR>i</VAR>).  There are many\nalternatives for specifying the actual digits; it is necessary only\nthat the code have the prefix property.  The usual assignment\nentails labeling the edge from each parent to its left child with\nthe digit 0 and the edge to the right child with 1.  The codeword for\neach source letter is the sequence of labels along the path from\nthe root to the leaf node representing that letter.\nThe codewords for the source of Figure 3.3,\nin order of decreasing probability, are\n{01,11,001,100,101,000,0001}.\nClearly, this\nprocess yields a minimal prefix code.  Further, the algorithm is \nguaranteed to produce an <EM>optimal</EM> (minimum redundancy) code [Huffman 1952].  Gallager \nhas proved an upper bound on the redundancy of a Huffman code of\n<VAR>p</VAR>(<VAR>n</VAR>) + lg [(2 lg <VAR>e</VAR>)/<VAR>e</VAR>] which is approximately <VAR>p</VAR>(<VAR>n</VAR>) + 0.086,\nwhere <VAR>p</VAR>(<VAR>n</VAR>) is the probability of the least likely source message\n[Gallager 1978].  In a recent paper, Capocelli et al. provide new\nbounds which are tighter than those of Gallagher for some probability\ndistributions [Capocelli et al. 1986].  \nFigure 3.4 shows a distribution for which the\nHuffman code is optimal while the Shannon-Fano code is not.\n<P>\n\tIn addition to the fact that there are\nmany ways of forming codewords of appropriate lengths, there are\ncases in which the Huffman algorithm does not uniquely determine\nthese lengths due to the arbitrary choice among equal\nminimum weights.\nAs an example, codes with codeword lengths of {1,2,3,4,4}\nand of {2,2,2,3,3} both yield the same average codeword length\nfor a source with probabilities {.4,.2,.2,.1,.1}.\nSchwartz defines a variation of the Huffman algorithm which\nperforms \"bottom merging\"; that is, orders a new parent node\nabove existing nodes of the same weight and always merges the last\ntwo weights in the list.  The code constructed is the Huffman code\nwith minimum values of maximum codeword length (MAX{ <VAR>l</VAR>(<VAR>i</VAR>) }) and \ntotal codeword length (SUM{ <VAR>l</VAR>(<VAR>i</VAR>) }) [Schwartz 1964]. Schwartz\nand Kallick describe an implementation of Huffman's algorithm with\nbottom merging [Schwartz and Kallick 1964].  The Schwartz-Kallick\nalgorithm and a later algorithm by Connell [Connell 1973]\nuse Huffman's procedure to determine the lengths of the codewords,\nand actual digits are assigned so that the code has the <EM>numerical\nsequence property</EM>.  That is, codewords of equal length form a \nconsecutive sequence of binary numbers.\nShannon-Fano codes also have the numerical\nsequence property.  This property can be exploited to achieve a compact representation\nof the code and rapid encoding and decoding. \n\n<PRE>\n                        S-F      Huffman\n\n<VAR>a</VAR>(1)        0.35        00       1\n<VAR>a</VAR>(2)        0.17        01       011\n<VAR>a</VAR>(3)        0.17        10       010\n<VAR>a</VAR>(4)        0.16        110      001\n<VAR>a</VAR>(5)        0.15        111      000\n\nAverage codeword length 2.31     2.30\n\nFigure 3.4 -- Comparison of Shannon-Fano and Huffman Codes.\n</PRE>\n\n  Both the Huffman and the Shannon-Fano mappings can be \ngenerated in <VAR>O</VAR>(<VAR>n</VAR>) time,\nwhere <VAR>n</VAR> is the number of messages in the\nsource ensemble (assuming that the weights have been presorted).  \nEach of these algorithms maps a source \nmessage <VAR>a</VAR>(<VAR>i</VAR>) with probability <VAR>p</VAR> to a codeword of length <VAR>l</VAR> \n(-lg <VAR>p</VAR> &lt;= l &lt;= - lg <VAR>p</VAR> + 1).  Encoding and decoding times\ndepend upon the representation of the mapping.  If the\nmapping is stored as a binary tree, then decoding the codeword for\n<VAR>a</VAR>(<VAR>i</VAR>) involves following a path of length <VAR>l</VAR> in the tree.  \nA table indexed by the source messages could be used for\nencoding; the code for <VAR>a</VAR>(<VAR>i</VAR>) would be stored in position <VAR>i</VAR> of\nthe table and encoding time would be <VAR>O</VAR>(<VAR>l</VAR>).  \nConnell's algorithm makes use of the <EM>index</EM>\nof the Huffman code, a representation of the distribution of\ncodeword lengths, to encode and decode in <VAR>O</VAR>(<VAR>c</VAR>) time where <VAR>c</VAR> is the\nnumber of different codeword lengths.  Tanaka presents an implementation\nof Huffman coding based on finite-state machines which can be realized\nefficiently in either hardware or software [Tanaka 1987].\n<P>\n\tAs noted earlier, the \nredundancy bound for Shannon-Fano codes is 1 and the bound for \nthe Huffman method is <VAR>p</VAR>(<VAR>n</VAR> + 0.086 where <VAR>p</VAR>(<VAR>n</VAR>) is the probability of the \nleast likely source message (so <VAR>p</VAR>(<VAR>n</VAR>) is less than or equal to .5, \nand generally much less).  It is important to note that in defining \nredundancy to be average codeword length minus\nentropy, the cost of transmitting the code mapping computed by \nthese algorithms is ignored.   The overhead cost for any method\nwhere the source alphabet has not been established prior to \ntransmission includes <VAR>n</VAR> lg <VAR>n</VAR> bits for sending the <VAR>n</VAR> source\nletters.  For a Shannon-Fano code, a list of codewords\nordered so as to correspond to the source letters could be \ntransmitted.  The additional time required is then SUM <VAR>l</VAR>(<VAR>i</VAR>),\nwhere the <VAR>l</VAR>(<VAR>i</VAR>) are the lengths of\nthe codewords.  For Huffman coding, an encoding of the shape of\nthe code tree might be transmitted.  Since any full binary tree may \nbe a legal Huffman\ncode tree, encoding tree shape may require as many as lg 4^<VAR>n</VAR> = 2<VAR>n</VAR> bits.\nIn most cases the message ensemble is very large, so that the\nnumber of bits of overhead is minute by comparison to the total\nlength of the encoded transmission.  However, it is imprudent to ignore\nthis cost.\n<P>\n\tIf a less-than-optimal code is acceptable, the overhead costs\ncan be avoided through a prior agreement by sender and receiver as to\nthe code mapping.  Rather than using a Huffman code based upon the\ncharacteristics of the current message ensemble, the code\nused could be based on statistics for a class of\ntransmissions to which the current ensemble is assumed\nto belong.\nThat is, both sender and receiver could have access to\na <EM>codebook</EM> with <VAR>k</VAR> mappings in it; one for Pascal source, one for\nEnglish text, etc.  The sender would then simply alert the receiver\nas to which of the common codes he is using.  This requires only\nlg <VAR>k</VAR> bits of overhead.  Assuming that classes of transmission with\nrelatively stable characteristics could be identified, this hybrid approach\nwould greatly reduce the redundancy due to overhead without\nsignificantly increasing expected codeword length.\nIn addition, the cost of computing the mapping would be amortized \nover all files of a given class.  That is, the mapping would be \ncomputed once on a statistically significant sample and then \nused on a great number of files for which the sample is \nrepresentative.  There is clearly a substantial risk associated \nwith assumptions about file characteristics and great care would \nbe necessary in choosing both the sample from which the mapping \nis to be derived and the categories into which to partition \ntransmissions.  An extreme example of the risk associated \nwith the codebook approach is provided by author Ernest V. \nWright who wrote a novel <EM>Gadsby</EM> (1939) containing no \noccurrences of the letter E.  Since E is the most commonly used \nletter in the English language, an encoding based upon a sample \nfrom <EM>Gadsby</EM> would be disastrous if used with \"normal\"\nexamples of English text.  Similarly, the \"normal\" encoding\nwould provide poor compression of <EM>Gadsby</EM>.\n<P>\n\tMcIntyre and Pechura describe an experiment in which \nthe codebook approach is compared to static Huffman coding\n[McIntyre and Pechura 1985].  The sample used for comparison\nis a collection of 530 source programs in four languages.\nThe codebook contains a Pascal code tree, a FORTRAN code tree,\na COBOL code tree, a PL/1 code tree, and an ALL code tree.\nThe Pascal code tree is the result of applying the static Huffman\nalgorithm to the combined character \nfrequencies of all of the Pascal programs in the sample.\nThe ALL code tree is based upon the combined character frequencies\nfor all of the programs.  The experiment involves encoding each\nof the programs using the five codes in the codebook and the static \nHuffman algorithm.  The data reported for each of the 530 programs\nconsists of the size of the coded program for each of the five\npredetermined codes, and the size of the coded program plus the\nsize of the mapping (in table form) for the static Huffman method. \nIn every case, the code tree for the language class to which the \nprogram belongs generates the most compact encoding.  Although using\nthe Huffman algorithm on the program itself yields an optimal\nmapping, the overhead cost is greater than the added redundancy\nincurred by the less-than-optimal code.\nIn many cases, the ALL code tree also generates a more compact encoding\nthan the static Huffman algorithm.  In the worst case, an\nencoding constructed from the codebook is only 6.6% larger\nthan that constructed by the Huffman algorithm.  These results\nsuggest that, for files of source code, the codebook approach\nmay be appropriate.\n<P>\n\tGilbert discusses the construction of Huffman codes\nbased on inaccurate source probabilities [Gilbert 1971].\nA simple solution to the problem of incomplete knowledge of\nthe source is to avoid long codewords, thereby minimizing the\nerror of underestimating badly the probability of a message.\nThe problem becomes one of constructing the optimal binary\ntree subject to a height restriction (see [Knuth 1971; Hu and \nTan 1972; Garey 1974]).  Another approach involves collecting \nstatistics for several sources and then constructing a code based \nupon some combined criterion.  This approach could be applied to the\nproblem of designing a single code for use with English, French, German,\netc., sources.  To accomplish this, Huffman's algorithm could be used to\nminimize either the average codeword length for the combined source\nprobabilities; or the average codeword length for English,\nsubject to constraints on average codeword lengths\nfor the other sources. \n\n<a name=\"Sec_3.3\">\n<H3> 3.3  Universal Codes and Representations of the Integers</H3> </a>\n\nA code is <EM>universal</EM> if it maps source messages to codewords\nso that the resulting average codeword length is bounded by <VAR>c1</VAR>(<VAR>H</VAR> + <VAR>c2</VAR>).\nThat is, given an arbitrary source with nonzero entropy, a universal\ncode achieves average codeword length which is at most a constant times\nthe optimal possible for that source.  The potential compression offered\nby a universal code clearly depends on the magnitudes of the constants\n<VAR>c1</VAR> and <VAR>c2</VAR>.  We recall the definition of an asymptotically optimal\ncode as one for which average codeword length approaches entropy and remark\nthat a universal code with <VAR>c1</VAR>=1 is asymptotically optimal.\n<P>\nAn advantage of universal codes over Huffman codes\nis that it is not necessary to know the exact probabilities with\nwhich the source messages appear.  While Huffman coding is not\napplicable unless the probabilities are known, it is sufficient\nin the case of universal coding to know the probability distribution\nonly to the extent that the source messages can be ranked in probability\norder.  By mapping messages in order of decreasing\nprobability to codewords in order of increasing length, universality\ncan be achieved.  Another advantage to universal codes is that\nthe codeword sets are fixed.  It is not necessary to compute a codeword\nset based upon the statistics of an ensemble; any universal codeword\nset will suffice as long as the source messages are ranked.  The\nencoding and decoding processes are thus simplified. \nWhile universal codes can be used instead of\nHuffman codes as general-purpose static schemes, the more common \napplication is as an adjunct to a dynamic scheme.  This type of\napplication will be demonstrated in\n<a href=\"DC-Sec5.html#Sec_5\">Section 5</a>.\n<P>\nSince the ranking of source messages is the essential parameter\nin universal coding, we may think of a universal code as \nrepresenting an enumeration of the source messages, or as\nrepresenting the integers, which provide an enumeration.\nElias defines a sequence of universal coding schemes which map the\nset of positive\nintegers onto the set of binary codewords [Elias 1975].  \n\n<PRE>\n      <VAR>gamma</VAR>          <VAR>delta</VAR>\n\n1     1              1\n2     010            0100\n3     011            0101\n4     00100          01100\n5     00101          01101\n6     00110          01110\n7     00111          01111\n8     0001000        00100000\n16    000010000      001010000\n17    000010001      001010001\n32    00000100000    0011000000\n\nFigure 3.5 -- Elias Codes.\n</PRE>\n\n\tThe first Elias code is one which\nis simple but not optimal.  This code, <VAR>gamma</VAR>, maps an integer\n<VAR>x</VAR> onto the binary value of <VAR>x</VAR> prefaced by floor( lg <VAR>x</VAR>)\nzeros.  The binary value of <VAR>x</VAR> is expressed in as few bits as possible,\nand therefore begins with a 1, which serves to\ndelimit the prefix.  The result is an instantaneously decodable code \nsince the total length of a codeword is exactly one greater than \ntwice the number of zeros in the prefix; therefore, as soon as the first\n1 of a codeword is encountered, its length is known.  \nThe code is not a minimum redundancy code since \nthe ratio of expected codeword length to entropy goes to 2 as \nentropy approaches infinity.  The second code, <VAR>delta</VAR>, maps\nan integer <VAR>x</VAR> to a codeword consisting of\n<VAR>gamma</VAR> (floor[lg <VAR>x</VAR>] +1)\nfollowed by the binary value of <VAR>x</VAR> with the leading\n1 deleted.  The resulting codeword has length\nfloor[lg <VAR>x</VAR>] + 2 floor[lg (1 + floor[lg <VAR>x</VAR>] ) ] + 1.  This\nconcept can be applied recursively to shorten the codeword lengths,\nbut the benefits decrease rapidly.  The code <VAR>delta</VAR> is asymptotically\noptimal since the limit of the ratio of expected codeword length to entropy\nis 1.  Figure 3.5 lists the values of <VAR>gamma</VAR> and <VAR>delta</VAR> for a\nsampling of the integers.  Figure 3.6 shows an Elias code for string\n<VAR>EXAMPLE</VAR>.  The number of bits transmitted using this mapping would\nbe 161, which does not compare well with the 117 bits transmitted by\nthe Huffman code of Figure 1.3.  Huffman coding is optimal under the static\nmapping model.  Even an asymptotically optimal universal code cannot\ncompare with static Huffman coding on a source for which the \nprobabilities of the messages are known.\n\n<PRE>\nSource   Frequency   Rank       Codeword\nmessage\n\n<VAR>g</VAR>           8          1        <VAR>delta</VAR>(1)=1\n<VAR>f</VAR>           7          2        <VAR>delta</VAR>(2)=0100\n<VAR>e</VAR>           6          3        <VAR>delta</VAR>(3)=0101\n<VAR>d</VAR>           5          4        <VAR>delta</VAR>(4)=01100\n<VAR>space</VAR>       5          5        <VAR>delta</VAR>(5)=01101\n<VAR>c</VAR>           4          6        <VAR>delta</VAR>(6)=01110\n<VAR>b</VAR>           3          7        <VAR>delta</VAR>(7)=01111\n<VAR>a</VAR>           2          8        <VAR>delta</VAR>(8)=00100000\n\nFigure 3.6 -- An Elias Code for <VAR>EXAMPLE</VAR> (code length=161).\n</PRE>\n\nA second sequence of universal coding schemes, based on the Fibonacci\nnumbers, is defined by  Apostolico and Fraenkel [Apostolico and \nFraenkel 1985].   While the Fibonacci codes are not asymptotically\noptimal, they compare well to the Elias codes as long as the number\nof source messages is not too large.  Fibonacci codes have the\nadditional attribute of robustness, which manifests itself by the\nlocal containment of errors.  This aspect of Fibonacci codes will\nbe discussed further in\n<a href=\"DC-Sec678.html#Sec_7\">Section 7</a>.\n<P>\nThe sequence of Fibonacci codes described by Apostolico and Fraenkel\nis based on the Fibonacci numbers of order <VAR>m</VAR> &gt;= 2, where the\nFibonacci numbers of order 2 are the standard Fibonacci numbers:\n1, 1, 2, 3, 5, 8, 13, ....  In general, the Fibonnaci numbers\nof order <VAR>m</VAR> are defined by the recurrence:  Fibonacci numbers <VAR>F</VAR>(-<VAR>m</VAR>+1)\nthrough <VAR>F</VAR>(0) are equal to 1; the <VAR>k</VAR>th number for <VAR>k</VAR> &gt;= 1 is \nthe sum of the preceding <VAR>m</VAR> numbers.  We describe only the order \n2 Fibonacci code; the extension to higher orders is straightforward.\n\n<PRE>\n<VAR>N</VAR>             <VAR>R</VAR>(<VAR>N</VAR>)               <VAR>F</VAR>(<VAR>N</VAR>)\n\n 1                           1   11\n 2                       1   0   011\n 3                   1   0   0   0011\n 4                   1   0   1   1011\n 5               1   0   0   0   00011\n 6               1   0   0   1   10011\n 7               1   0   1   0   01011\n 8           1   0   0   0   0   000011\n16       1   0   0   1   0   0   0010011\n32   1   0   1   0   1   0   0   00101011\n\n    21  13   8   5   3   2   1\n\nFigure 3.7 -- Fibonacci Representations and Fibonacci Codes.\n</PRE>\n\nEvery nonnegative integer <VAR>N</VAR> has precisely one binary representation\nof the form <VAR>R</VAR>(<VAR>N</VAR>) = SUM(<VAR>i</VAR>=0 to <VAR>k</VAR>) <VAR>d</VAR>(<VAR>i</VAR>) <VAR>F</VAR>(<VAR>i</VAR>) (where <VAR>d</VAR>(<VAR>i</VAR>) is in {0,1},\n<VAR>k</VAR> &lt;= <VAR>N</VAR>, and the <VAR>F</VAR>(<VAR>i</VAR>)\nare the order 2 Fibonacci numbers as \ndefined above) such that there are no adjacent ones in the representation.\nThe Fibonacci representations for a small sampling of the integers\nare shown in Figure 3.7, using the standard bit sequence, from high\norder to low.  The bottom row of the figure gives the values of the\nbit positions.  It is immediately obvious that this Fibonacci \nrepresentation does not constitute a prefix code.  The order 2 \nFibonacci code for <VAR>N</VAR> is defined to be:\n<VAR>F</VAR>(<VAR>N</VAR>)=<VAR>D</VAR>1 where <VAR>D</VAR>=d(0)d(1)d(2)...d(<VAR>k</VAR>)\n(the <VAR>d</VAR>(<VAR>i</VAR>) defined above).  That is, the Fibonacci representation is\nreversed and 1 is appended.  The Fibonacci code values for a small\nsubset of the integers are given in Figure 3.7.  These binary\ncodewords form a prefix code since every codeword now terminates\nin two consecutive ones, which cannot appear anywhere else in a\ncodeword.  \n<P>\nFraenkel and Klein prove that the Fibonacci code of order 2 is universal,\nwith <VAR>c1</VAR>=2 and <VAR>c2</VAR>=3 [Fraenkel and Klein 1985].  It is not asymptotically\noptimal since <VAR>c1</VAR>&gt;1.  Fraenkel and Klein also show that Fibonacci\ncodes of higher order compress better than the order 2 code if the\nsource language is large enough (i.e., the number of distinct source messages\nis large) and the probability distribution is nearly uniform.\nHowever, no Fibonacci code is asymptotically optimal.  The Elias \ncodeword <VAR>delta</VAR>(<VAR>N</VAR>) is asymptotically shorter than any Fibonacci\ncodeword for <VAR>N</VAR>, but the integers in a very large initial range have\nshorter Fibonacci codewords.  For <VAR>m</VAR>=2, for example, the \ntransition point is <VAR>N</VAR>=514,228 [Apostolico and Fraenkel 1985].  \nThus, a Fibonacci code provides better compression\nthan the Elias code until the size of the source language becomes very\nlarge.  Figure 3.8 shows a Fibonacci code for string <VAR>EXAMPLE</VAR>.\nThe number of bits transmitted using this mapping would be 153, which\nis an improvement over the Elias code of Figure 3.6 but still \ncompares poorly with the Huffman code of Figure 1.3.\n\n<PRE>\nSource   Frequency   Rank       Codeword\nmessage\n\n<VAR>g</VAR>           8          1        <VAR>F</VAR>(1)=11\n<VAR>f</VAR>           7          2        <VAR>F</VAR>(2)=011\n<VAR>e</VAR>           6          3        <VAR>F</VAR>(3)=0011\n<VAR>d</VAR>           5          4        <VAR>F</VAR>(4)=1011\n<VAR>space</VAR>       5          5        <VAR>F</VAR>(5)=00011\n<VAR>c</VAR>           4          6        <VAR>F</VAR>(6)=10011\n<VAR>b</VAR>           3          7        <VAR>F</VAR>(7)=01011\n<VAR>a</VAR>           2          8        <VAR>F</VAR>(8)=000011\n\nFigure 3.8 -- A Fibonacci Code for <VAR>EXAMPLE</VAR> (code length=153).\n</PRE>\n\n<a name=\"Sec_3.4\">\n<H3> 3.4 Arithmetic Coding</H3> </a>\n\nThe method of arithmetic coding was suggested by Elias, and presented\nby Abramson in his text on Information Theory [Abramson 1963].\nImplementations of Elias' technique were developed by Rissanen, Pasco,\nRubin, and, most recently, Witten et al. [Rissanen 1976; Pasco 1976;\nRubin 1979; Witten et al. 1987].  We present the concept of arithmetic\ncoding first and follow with a discussion of implementation details \nand performance.\n<P>\nIn arithmetic coding a source ensemble is represented by an interval\nbetween 0 and 1 on the real number line.  Each symbol of the ensemble\nnarrows this interval.  As the interval becomes smaller, the number of\nbits needed to specify it grows.  Arithmetic coding assumes an explicit\nprobabilistic model of the source.  It is a defined-word scheme which\nuses the probabilities of the source messages to successively narrow\nthe interval used to represent the ensemble.  A high probability\nmessage narrows the interval less than a low probability message, so\nthat high probability messages contribute fewer bits to the coded ensemble.\nThe method begins with an unordered list of source messages and their\nprobabilities.  The number line is partitioned into subintervals based on\ncumulative probabilities.  \n<P>\nA small example will be used to illustrate the idea of arithmetic coding.\nGiven source messages {<VAR>A,B,C,D,#</VAR>} with probabilities \n{.2, .4, .1, .2, .1}, Figure 3.9 demonstrates the initial partitioning\nof the number line.  The symbol <VAR>A</VAR> corresponds to the first 1/5 of\nthe interval [0,1); <VAR>B</VAR> the next 2/5; <VAR>D</VAR> the subinterval of size\n1/5 which begins 70% of the way from the left endpoint to the right.\nWhen encoding begins, the source ensemble is represented by the entire \ninterval [0,1).  For the ensemble <VAR>AADB#</VAR>, the first <VAR>A</VAR> reduces the\ninterval to [0,.2) and the second <VAR>A</VAR> to [0,.04) (the first 1/5\nof the previous interval).  The <VAR>D</VAR> further narrows the interval to\n[.028,.036) (1/5 of the previous size, beginning 70% of the distance\nfrom left to right).  The <VAR>B</VAR> narrows the interval to [.0296,.0328), \nand the <VAR>#</VAR> yields a final interval of [.03248,.0328).  The interval,\nor alternatively any number <VAR>i</VAR> within the interval, may now be used to\nrepresent the source ensemble.\n\n<PRE>\nSource    Probability   Cumulative    Range\nmessage                 probability\n\n   <VAR>A</VAR>            .2          .2        [0,.2)\n   <VAR>B</VAR>            .4          .6        [.2,.6)\n   <VAR>C</VAR>            .1          .7        [.6,.7)\n   <VAR>D</VAR>            .2          .9        [.7,.9)\n   <VAR>#</VAR>            .1         1.0        [.9,1.0)\n\nFigure 3.9 -- The Arithmetic coding model.\n</PRE>\n\nTwo equations may be used to define the narrowing process described above: <BR>\n<PRE>\n   newleft = prevleft + msgleft*prevsize      (1)\n   newsize = prevsize * msgsize               (2)\n</PRE>\n\nThe first equation states that the left endpoint of the new interval is\ncalculated from the previous interval and the current source message.\nThe left endpoint of the range associated with the current message\nspecifies what percent of the previous interval to remove from the left\nin order to form the new interval.  For <VAR>D</VAR> in the above example, the new left\nendpoint is moved over by .7 * .04 (70% of the size of the previous\ninterval).  The second equation computes the\nsize of the new interval from the previous interval size and the probability\nof the current message (which is equivalent to the size of its associated\nrange).  Thus, the size of the interval determined by <VAR>D</VAR> is .04*.2, and the\nright endpoint is .028+.008=.036 (left endpoint + size).\n<P>\nThe size of the final subinterval determines the number of bits needed to\nspecify a number in that range.  The number of bits needed to specify a\nsubinterval of [0,1) of size <VAR>s</VAR> is -lg <VAR>s</VAR>.  Since the size of the final\nsubinterval is the product of the probabilities of the source messages\nin the ensemble (that is, <VAR>s</VAR>=PROD{<VAR>i</VAR>=1 to <VAR>N</VAR>} <VAR>p</VAR>(source message <VAR>i</VAR>) where \n<VAR>N</VAR> is the length of the ensemble), we have \n-lg <VAR>s</VAR> = -SUM{<VAR>i</VAR>=1 to <VAR>N</VAR> lg <VAR>p</VAR>(source message <VAR>i</VAR>) =\n- SUM{<VAR>i</VAR>=1 to <VAR>n</VAR>} <VAR>p</VAR>(<VAR>a</VAR>(<VAR>i</VAR>)) lg <VAR>p</VAR>(<VAR>a</VAR>(<VAR>i</VAR>)), where <VAR>n</VAR> is the number of unique\nsource messages <VAR>a</VAR>(1), <VAR>a</VAR>(2), ... <VAR>a</VAR>(<VAR>n</VAR>).\nThus, the number of bits generated\nby the arithmetic coding technique is exactly equal to entropy, <VAR>H</VAR>.\nThis demonstrates the fact that arithmetic coding achieves compression\nwhich is almost exactly that predicted by the entropy of the source.\n<P>\nIn order to recover the original ensemble, the decoder must\nknow the model of the source used by the encoder (eg., the source messages\nand associated ranges) and a single number within the interval determined\nby the encoder.  Decoding consists of a series of comparisons of the number\n<VAR>i</VAR> to the ranges representing the source messages.  For this example,\n<VAR>i</VAR> might be .0325 (.03248, .0326, or .0327 would all do just as well).\nThe decoder uses <VAR>i</VAR> to simulate the actions of the encoder.  Since <VAR>i</VAR> lies \nbetween 0 and .2, he deduces that the first letter was <VAR>A</VAR> (since the range\n[0,.2) corresponds to source message <VAR>A</VAR>).  This narrows the interval\nto [0,.2).  The decoder can now deduce that the next message will further\nnarrow the interval in one of the following ways:  to [0,.04) for <VAR>A</VAR>,\nto [.04,.12) for <VAR>B</VAR>, to [.12,.14) for <VAR>C</VAR>, to [.14,.18) for <VAR>D</VAR>,\nand to [.18,.2) for <VAR>#</VAR>.  Since <VAR>i</VAR> falls into the interval [0,.04),\nhe knows that the second message is again <VAR>A</VAR>.  This process continues until\nthe entire ensemble has been recovered.\n<P>\nSeveral difficulties become evident when implementation of arithmetic coding\nis attempted.  The first is that the decoder needs some way of knowing\nwhen to stop.  As evidence of this, the number 0 could represent any of\nthe source ensembles <VAR>A, AA, AAA,</VAR> etc.  Two solutions to this problem\nhave been suggested.  One is that the encoder transmit the size of the\nensemble as part of the description of the model.  Another is that\na special symbol be included in the model for the purpose of signaling\nend of message.  The <VAR>#</VAR> in the above example serves this purpose.\nThe second alternative is preferable for several reasons.  First, sending\nthe size of the ensemble requires a two-pass process and precludes the use\nof arithmetic coding as part of a hybrid codebook scheme (see\n<a href=\"DC-Sec1.html#Sec_1.2\">Sections 1.2</a> and <a href=\"#Sec_3.2\">3.2</a>).\nSecondly, adaptive methods of arithmetic coding are easily\ndeveloped and a first pass to determine ensemble size is inappropriate in\nan on-line adaptive scheme.\n<P>\nA second issue left unresolved by the fundamental concept of arithmetic\ncoding is that of incremental transmission and reception.  It appears\nfrom the above discussion that the encoding algorithm transmits nothing\nuntil the final interval is determined.  However, this delay is not\nnecessary.  As the interval narrows, the leading bits  of the left\nand right endpoints become the same.  Any leading bits that are the same may be\ntransmitted immediately, as they will not be affected by further narrowing.\nA third issue is that of precision.  From the description of arithmetic\ncoding it appears that the precision required grows without bound as the\nlength of the ensemble grows.  Witten et al. and Rubin address this issue\n[Witten et al. 1987; Rubin 1979].  Fixed precision registers may be used\nas long as underflow and overflow are detected and managed.  The degree\nof compression achieved by an implementation of arithmetic coding is\nnot exactly <VAR>H</VAR>, as implied by the concept of arithmetic coding.  Both the use\nof a message terminator and the use of fixed-length arithmetic reduce\ncoding effectiveness.  However, it is clear that an end-of-message symbol\nwill not have a significant effect on a large source ensemble.\nWitten et al. approximate the overhead due to the use of fixed\nprecision at 10^-4 bits per source message, which is also negligible.\n<P>\nThe arithmetic coding model for ensemble <VAR>EXAMPLE</VAR> is given in\nFigure 3.10.  The final interval size is\n<VAR>p</VAR>(a)^2*<VAR>p</VAR>(b)^3*<VAR>p</VAR>(c)^4*<VAR>p</VAR>(d)^5*<VAR>p</VAR>(e)^6*<VAR>p</VAR>(f)^7*<VAR>p</VAR>(g)^8*<VAR>p</VAR>(<VAR>space</VAR>)^5.\nThe number of bits needed to specify a value in the interval \nis -lg(1.44*10^-35)=115.7.  So excluding overhead, arithmetic coding\ntransmits <VAR>EXAMPLE</VAR> in 116 bits, one less bit than static Huffman coding.\n\n<PRE>\nSource    Probability   Cumulative    Range\nmessage                 probability\n\n   <VAR>a</VAR>            .05         .05       [0,.05)\n   <VAR>b</VAR>            .075        .125      [.05,.125)\n   <VAR>c</VAR>            .1          .225      [.125,.225)\n   <VAR>d</VAR>            .125        .35       [.225,.35)\n   <VAR>e</VAR>            .15         .5        [.35,.5)\n   <VAR>f</VAR>            .175        .675      [.5,.675)\n   <VAR>g</VAR>            .2          .875      [.675,.875)\n   <VAR>space</VAR>        .125       1.0        [.875,1.0)\n\nFigure 3.10 -- The Arithmetic coding model of <VAR>EXAMPLE</VAR>.\n</PRE>\n\nWitten et al. provide an implementation of arithmetic coding, written in C,\nwhich separates the model of the source from the coding process (where the\ncoding process is defined by Equations 3.1 and 3.2) [Witten et al. 1987].\nThe model is in a separate program module and is consulted by the encoder\nand by the decoder at every step in the processing.  The fact that the\nmodel can be separated so easily renders the classification static/adaptive\nirrelevent for this technique.  Indeed, the fact that the coding method\nprovides compression efficiency nearly equal to the entropy of the source\nunder any model allows arithmetic coding to be coupled with any static or\nadaptive method for computing the probabilities (or frequencies) of the\nsource messages.  Witten et al. implement an adaptive model similar to the\ntechniques described in <a href=\"DC-Sec4.html#Sec_4\">Section 4</a>.  The performance\nof this implementation is discussed in <a href=\"DC-Sec678.html#Sec_6\">Section 6</a>.\n\n<P>\n<A HREF=\"DC-Sec2.html\"><IMG SRC=\"prev.gif\" ALT=\"PREV section\"></A>\n<A HREF=\"DC-Sec4.html\"><IMG SRC=\"next.gif\" ALT=\"NEXT section\"></A>\n<P>\n</BODY></HTML>\n", "id": 688.0}