{"text": "Interface 1 Bioinformatics Day Saturday June 16th Costa Mesa Room Bioinformatics Day will bring together experts in statistics computer science biology and medicine The day will consist of 4 consecutive sessions on topical themes in bioinformatics with invited talks from leading experts in each area Registration Inforamtion Bioinformatics Day is included in the regular registration for Interface 1 Single Day Registration for Bioinfo Day only is 12 with a reduced registration fee of 5 for students Biological Sequence Analysis Costa Mesa Room 8 3 am 1 15am Comparative Genomics and the Future of Biological Knowledge 8 3 am 9 15am Anthony Kerlavage Celera At Celera Genomics we have set our goal to become the definitive source of genomic and associated biological information that will be used by scientists to develop a better understanding of the biological processes in humans and deliver improved healthcare in the future Using breakthrough DNA analysis technology applied to sequencing strategies pioneered at TIGR the company has completed the sequencing of the Drosophila melanogaster human and mouse genomes The whole genome shotgun method we employed enables the identification of a large number of computationally derived single nucleotide polymorphisms SNPs Celera s SNP Reference Database is intended to support the discovery and characterization of human genetic variation involved in disease drug efficacy and drug toxicity The genetic variation data are fully integrated with the genome gene and protein structure data and linked to specific base pair locations on the genome The database also features allele frequency population data validation status and trace data for all variations The sequencing of the human genome along with those of important model organisms enables the application of comparative genomics methodologies to the study of biological function The identification of synteny between the human and mouse genomes improves the identification of genes in both organisms enriches the functional annotation of the genes and allows the wealth of mouse genetics data to be linked to human orthologs The method also enables the identification of conserved regulatory regions We have also developed techniques for creating protein families across all known proteins This method creates Hidden Markov Models of protein families and sub families These HMMs can be used to classify novel proteins as they are discovered The Public Working Draft of the Human Genome 9 2 am 9 45am David Haussler University of California Santa Cruz haussler cse ucsc edu A working group headed by Francis Collins Eric Lander Bob Waterston and John Sulston in association with researchers at NCBI EBI UCSC and the major public sequencing centers has produced and annotated the initial working draft of the human genome and published their findings in the Feb 15 issue of Nature Jim Kent at UCSC has made tremendous contributions to this effort The public working draft sequence is currently assembled and made available at UCSC http genome ucsc edu and a browser for the annotation of this sequence is available there as well as at EBI http www ensembl org We discuss the bioinformatic challenges in assembling annotating and periodically updating this current public working draft of the human genome Identification of Post Translationally Modified and Mutated Proteins via Mass Spectrometry 9 5 am 1 15am Pavel Pevzner University of California San Diego ppevzner cs ucsd edu Although protein identification by matching tandem mass spectra MS MS against protein databases is a widespread tool in mass spectrometry the question about reliability of such searches remains open Most MS MS database search algorithms rely on variations of the Shared Peaks Count approach that scores pairs of spectra by the peaks masses they have in common Although this approach proved to be useful it has high error rate in identification of mutated and modified peptides We describe new MS MS database search algorithms that implement the spectral convolution and spectral alignment approaches to peptide identification We further analyze these approaches to identification of modified peptides and demonstrate their advantages over the Shared Peaks Count Gene Expression Data Analysis Costa Mesa Room 1 45am 12 3 pm Improved Statistical Inference from DNA Microarray Data Using Analysis of Variance and a Bayesian Statistical Framework 1 45am 11 3 am G Wesley Hatfield University of California Irvine gwhatfie uci edu The recent availability of complete genomic sequences and or large numbers of cDNA clones from model organisms coupled with technical advances in DNA arraying technology have made it possible to study genome wide patterns of gene expression However despite these rapid technological developments the statistical tools required to analyze DNA microarray data are not in place DNA microarray data often consist of expression measures for thousands of genes but experimental replication at the level of single genes is often low This creates problems of statistical inferences since many genes show large changes in gene expression by chance alone Therefore to interpret data from DNA microarrays it is necessary to employ statistical methods capable of distinguishing chance occurrences from biologically meaningful data Commonly used software packages are poorly suited for the statistical analysis of DNA microarray data However we have created a program Cyber T which accommodates this approach Cyber T is available for online use at the genomics Web site at the University of California at Irvine http www genomics uci edu This program is ideally suited to experimental designs in which replicate control measurements are being compared to replicate experimental measurements In the study reported here we use the statistical tools incorporated into Cyber T to compare and analyze the gene expression profiles obtained from a wild type strain of it Escherichia coli and an otherwise isogenic strain lacking the gene for the global regulatory protein integration host factor IHF Several decades of work with this model organism have produced a wealth of information about its operon specific and global gene regulation patterns This information makes it possible to evaluate the accuracy of data obtained from DNA microarray experiments and to identify data analysis methods that optimize the differentiation of genes expressed because of biological reasons from false positives genes that appear to be differentially expressed due to chance occurrences We apply different statistical methods for identifying genes showing changes in expression to this data set and show that a Bayesian approach identifies a stronger set of genes as being significantly up or down regulated based on our biological understanding of IHF regulation We show that commonly used approaches for identifying genes as being up or down regulated i e simple t test or fold change thresholds require more replication to approach the same level of reliability as Bayesian statistical approaches applied to data sets with more modest levels of replication We further show that statistical tests identify a different set of genes than those based on fold change and argue that a set of genes identified by fold change is more likely to harbor experimental artifacts Statistical Issues Data Analysis and Modelling for Gene Expression Profiling 11 35am 12 pm Mike West Duke University mw stat duke edu The talk will cover aspects of statistical analysis of oligonuceotide microarray data with a specific focus expression profiling in cancer Among topics to be discussed and exemplified include issues arising in developing predictive regression models for evaluating and characterising expression patterns associated with clinical or physiological states i e formal statistical approaches to the canonical molecular phenotyping problem I will discuss the use of singular value regression ideas within a Bayesian framework that addresses the large p small n regression problem posed in such applications Breast cancer studies highlight critical challenges including data quality definition variable gene selection and aspects of model evaluation using cross validation Two specific applications in breast cancer will be used to convey and review basic data analysis and modelling issues as well as to highlight conceptual and practical research challenges in this area The use of such expression data in developing models of gene interactions that may relate to underlying regulatory networks will be mentioned as will a range of issues germane to the oligonucleotide technology Plaid Models for DNA Microarrays 12 5pm 12 3 pm Art Owen Stanford art stat stanford edu This talk describes the plaid model a tool for exploratory analysis of multivariate data The motivating application is the search for interpretable biological structure in gene expression microarray data Interpretable structure can mean that a set of genes has a similar expression pattern in the samples under study or in just a subset of them such as the cancerous samples A set of genes behaving similarly in a set of samples defines what we call a layer These are very much like clusters except that genes can belong to more than one layer or to none of them the layer may be defined with respect to only a subset of the samples and the role of genes and samples is symmetric in our formulation The plaid model is a superposition of two way anova models each defined over subsets of genes and samples We will present the plaid model an interior point style algorithm for fitting it and some examples from yeast DNA arrays and other problems This is joint work with Laura Lazzeroni Medical Informatics Costa Mesa Room 2 pm 3 pm Integrating Data and Disciplines Biostatistics and Biomedical Informatics 2 pm 2 25pm Joyce Niland City of Hope JNiland coh org As predicted by Dr Richard Klausner Director of the National Cancer Institute NCI Biomedical informatics is the future of integrating science and medical research In response to the explosion of data from the sequencing of the human genome and the critical objective of correlating genotypic and phenotypic data informatics tools for collecting managing retrieving and analyzing both clinical and genetic data are needed At City of Hope we have formed a Division of Information Sciences to create and maintain such systems linking the highly synergistic and inter dependent disciplines of Biostatistics and Biomedical Informatics These systems support the conduct of clinical research the mining of genetic data and the ultimate integration of data stemming from numerous source data systems Internet based interfaces and data warehousing principles factor strongly in the systems under development through collaborative work between faculty and staff members in Biostatistics and Biomedical Informatics Several key data systems will be described along with the human information and technological issues that must be considered in their creation and maintenance Our research data warehouse architecture will be outlined representing an optimal form of data integration in support of clinical and genetic research The Trouble with Text Challenges and Promises of Biomedical Information Retrieval Technology 2 3 pm 2 55pm Wanda Pratt University of California Irvine pratt ics uci edu The ultimate motivation for bioinformatics researchers is to improve health care but the traditional publication and retrieval of results in textual journal articles has become a bottleneck in the dissemination of scientific advances The millions of available articles even within a narrow subfield easily overwhelm both biomedical researchers and health care providers regardless of whether they are pursuing a research hypothesis or deciding how to care for a particular patient Many characteristics of traditional word based approaches make efficient and effective retrieval of biomedical texts particularly challenging In this presentation I will discuss examples of recent approaches that take advantage of common knowledge of biomedicine and representation standards to address these challenges and provide improved organization and retrieval of biomedical textual information Automated Analysis of Brain Images Costa Mesa Room 3 3 pm 5 15pm On Metrics and Variational Equations of Computational Anatomy 3 3 pm 4 15pm Michael Miller Johns Hopkins University mim cis jhu edu We review recent advances in the Emerging Discipline of Computational Anatomy We begin by defining anatomy as an orbit under groups of diffeomorphisms Metrics on the orbits are defined via geodesic distance in the spaces of diffeomorphisms This induces a metric on the space of anatomical images Estimation problems are defined associated with estimating the underlying diffeomorphisms generating the anatomies from Medical images The variational problems defined are associated with finding the infimum length Energy diffeomorphism connecting the Medical imagery with the underlying anatomical structures Euler Lagrange equations will be discussed Applications will be described to the study of the neocortex of the macaque tumor generation and hypothesis testing in neuropsychiatric applications in Aging and Schizophrenia Visual Analysis of Variance A Tool for Quantitative Assessment of fMRI Data Processing and Analysis 4 2 pm 4 45pm William F Eddy Carnegie Mellon University bill stat cmu edu R L McNamee University of Pittsburgh rlandes neuronet pitt edu The raw data from an fMRI experiment are pre processed before statistical analysis in order to produce useful results This includes an inverse Fourier transform and other steps each intended to improve the quality of the resulting images Each pre processing step has been justified by careful study of some prior experiment On the other hand the formal statistical analysis often contains within it some internal assessment of the quality of the experimental data such as the residual sum of squares from a fitted model Here we propose an analog to the analysis of variance which we call a Visual Analysis of Variance VANOVA as a tool for assessing the importance of each pre processing step A VANOVA provides both quantitative and visual information which aid in the assignment of causes of variability and the determination of statistical significance In fact a VANOVA is the natural extension to the pre processing of an ANOVA assessment of the statistical modeling Because the VANOVA is not intended to be as formal as an ANOVA it can include evaluation of pre processing steps which are neither linear nor orthogonal both requirements of the partitions in an ANOVA Positron Emission Tomography Image Formation and Analysis 4 5 pm 5 15pm Richard Leahy University of Southern California leahy sipi usc edu Positron Emission Tomography is a powerful medical imaging modality for investigating human and animal biochemistry and physiology Detection of photon pairs produced by positron electron annihilation produces tomographic projections of the spatial distribution of positron emitting nuclei Tomographic reconstruction methods can then be used to form volumetric images By labelling biochemicals with positron emitting nuclei we can produce images of a wide variety of biochemical and physiological processes PET is now widely used in detecting and staging cancer through imaging of glucose In the brain receptor and transmitter ligands have been developed to study the dopamine and other neurochemical systems The most exciting recent development in PET is the ability to directly image gene expression through the use of PET tracer reporter gene combinations In humans this technique can be used for example to monitor the efficacy of gene therapy techniques PET gene expression imaging is also being increasingly used to studygene expression in transgenic animals Essential to the success of positron tomography in these diverse applications is a combination of instrumentation design optimized for specific applications e g humans vs small animals and image processing methods that maximize image quality when forming volumetric images from PET data After reviewing the instrumentation and principles behind PET I will describe statistically based approaches to reconstructing PET images Using a Bayesian formulation we combine accurate physical models of the physics underlying PET systems with accurate statistical models for photon limited data collected in these systems I will illustrate the impact of this approach on image quality through examples for clinical and animal studies I will also describe our current work on evaluating image quality through a combination of theoretical Monte Carlo and human observer studies ", "_id": "http://www.ics.uci.edu/~interfac/bioinfo.html", "title": "interface '01: bioinformatics day", "html": "<html>\n<head>\n<title>Interface '01: Bioinformatics Day</title>\n</head>\n<body>\n<center>\n<h1>\nInterface '01: Bioinformatics Day<br>\n</h1>\n<h2>\nSaturday, June 16th<br>\nCosta Mesa Room\n</h2>\n</center>\n<p>\nBioinformatics Day will bring together experts in statistics, computer\nscience, biology, and medicine. The day will consist of 4 consecutive\nsessions on topical themes in bioinformatics, with invited talks from\nleading experts in each area.\n<p>\n\n<h4>\nRegistration Inforamtion\n</h4>\n<ul>\n<li>\nBioinformatics Day is included in the regular registration for Interface '01.\n</li>\n<li>\nSingle Day Registration for Bioinfo Day only is $120 (with a reduced registration fee of $50 for students). \n</li>\n</ul>\n<hr><center>\n<h2>Biological Sequence Analysis</h2>\nCosta Mesa Room<br>\n8:30am -- 10:15am<br>\n</center>\n<a name=\"AnthonyKerlavage\"></a>\n<h4>Comparative Genomics and the Future of Biological Knowledge</h4>\n8:30am -- 9:15am<center>\nAnthony Kerlavage (Celera)\n</center>\n<p>\nAt Celera Genomics we have set our goal to become the definitive\nsource of genomic and associated biological information that will be used by\nscientists to develop a better understanding of the biological processes in\nhumans and deliver improved healthcare in the future. Using breakthrough DNA\nanalysis technology applied to sequencing strategies pioneered at TIGR, the\ncompany has completed the sequencing of the Drosophila melanogaster, human,\nand mouse genomes.  The whole genome shotgun method we employed enables the\nidentification of a large number of computationally derived single\nnucleotide polymorphisms (SNPs). Celera's SNP Reference Database is intended\nto support the discovery and characterization of human genetic variation\ninvolved in disease, drug efficacy, and drug toxicity. The genetic variation\ndata are fully integrated with the genome, gene, and protein structure data\nand linked to specific base pair locations on the genome. The database also\nfeatures allele frequency, population data, validation status, and trace\ndata for all variations. \n\nThe sequencing of the human genome along with those of important\nmodel organisms enables the application of comparative genomics\nmethodologies to the study of biological function.  The identification of\nsynteny between the human and mouse genomes improves the identification of\ngenes in both organisms, enriches the functional annotation of the genes,\nand allows the wealth of mouse genetics data to be linked to human\northologs. The method also enables the identification of conserved\nregulatory regions. We have also developed techniques for creating protein\nfamilies across all known proteins. This method creates Hidden Markov Models\nof protein families and sub-families. These HMMs can be used to classify\nnovel proteins as they are discovered.\n</p>\n<a name=\"DavidHaussler\"></a>\n<h4>The Public Working Draft of the Human Genome</h4>\n9:20am -- 9:45am<center>\nDavid Haussler (University of California, Santa Cruz), <tt>haussler@cse.ucsc.edu</tt>\n</center>\n<p>\nA working group headed by Francis Collins, Eric Lander, Bob Waterston\nand John Sulston, in association with researchers at NCBI, EBI, UCSC\nand the major public sequencing centers, has produced and annotated the\ninitial working draft of the human genome and published their findings\nin the Feb. 15 issue of Nature. Jim Kent at UCSC has made tremendous\ncontributions to this effort. The public working draft sequence is\ncurrently assembled and made available at UCSC\n(<a href=\"http://genome.ucsc.edu\">http://genome.ucsc.edu</a>) and a browser\nfor the annotation of this sequence is available there, as well as at\nEBI (<a href=\"http://www.ensembl.org/\">http://www.ensembl.org/</a>).  We discuss the bioinformatic challenges\nin assembling, annotating and periodically updating this current public\nworking draft of the human genome.\n</p>\n<a name=\"PavelPevzner\"></a>\n<h4>Identification of Post-Translationally Modified and Mutated Proteins via Mass-Spectrometry</h4>\n9:50am -- 10:15am<center>\nPavel Pevzner (University of California, San Diego), <tt>ppevzner@cs.ucsd.edu</tt>\n</center>\n<p>\nAlthough protein identification by matching tandem mass spectra (MS/MS)\nagainst protein databases is a widespread tool in mass-spectrometry,\nthe question about reliability of such searches remains open.  Most MS/MS\ndatabase search algorithms rely on variations of the Shared Peaks Count\napproach that scores pairs of  spectra by the peaks (masses)  they have\nin common. Although this approach proved to be  useful, it has high error\nrate in identification of mutated and modified peptides. We describe new\nMS/MS database search algorithms that implement the spectral convolution\nand spectral alignment approaches to peptide identification.  We further\nanalyze these  approaches to identification of modified peptides and\ndemonstrate  their advantages over  the  Shared Peaks Count.\n</p>\n<hr><center>\n<h2>Gene Expression Data Analysis</h2>\nCosta Mesa Room<br>\n10:45am -- 12:30pm<br>\n</center>\n<a name=\"GWesleyHatfield\"></a>\n<h4>Improved Statistical Inference from DNA Microarray Data Using Analysis of Variance and a Bayesian Statistical Framework</h4>\n10:45am -- 11:30am<center>\nG. Wesley Hatfield (University of California, Irvine), <tt>gwhatfie@uci.edu</tt>\n</center>\n<p>\nThe recent availability of complete genomic sequences and/or large numbers\nof cDNA clones from model organisms coupled with technical advances\nin DNA arraying technology have made it possible to study genome-wide\npatterns of gene expression. However, despite these rapid technological\ndevelopments, the statistical tools required to analyze DNA microarray\ndata are not in place. DNA microarray data often consist of expression\nmeasures for thousands of genes, but experimental replication at the\nlevel of single genes is often low. This creates problems of statistical\ninferences since many genes show large changes in gene expression by\nchance alone. Therefore, to interpret data from DNA microarrays it is\nnecessary to employ statistical methods capable of distinguishing chance\noccurrences from biologically meaningful data.\n\nCommonly used software packages are poorly suited for the statistical\nanalysis of DNA microarray data. However, we have created a program,\nCyber-T, which accommodates this approach.  Cyber-T is available for\nonline use at the genomics Web site at the University of California\nat Irvine <a href=\"http://www.genomics.uci.edu\">http://www.genomics.uci.edu</a>. This program is ideally suited to\nexperimental designs in which replicate control measurements are being\ncompared to replicate experimental measurements.\n\nIn the study reported here, we use the statistical tools incorporated\ninto Cyber-T to compare and analyze the gene expression profiles obtained\nfrom a wild-type strain of {\\it Escherichia coli\\/} and an otherwise isogenic\nstrain lacking the gene for the global regulatory protein, integration\nhost factor (IHF). Several decades of work with this model organism have\nproduced a wealth of information about its operon-specific and global gene\nregulation patterns. This information makes it possible to evaluate the\naccuracy of data obtained from DNA microarray experiments, and to identify\ndata analysis methods that optimize the differentiation of genes expressed\nbecause of biological reasons from false positives (genes that appear to\nbe differentially expressed due to chance occurrences). We apply different\nstatistical methods for identifying genes showing changes in expression\nto this data set and show that a Bayesian approach identifies a stronger\nset of genes as being significantly up- or down- regulated based on our\nbiological understanding of IHF regulation. We show that commonly used\napproaches for identifying genes as being up- or down- regulated (i.e.,\nsimple t-test or fold change thresholds) require more replication to\napproach the same level of reliability as Bayesian statistical approaches\napplied to data sets with more modest levels of replication. We further\nshow that statistical tests identify a different set of genes than those\nbased on fold-change, and argue that a set of genes identified by fold\nchange is more likely to harbor experimental artifacts.\n</p>\n<a name=\"MikeWest\"></a>\n<h4>Statistical Issues, Data Analysis, and Modelling for Gene Expression Profiling</h4>\n11:35am -- 12:00pm<center>\nMike West (Duke University), <tt>mw@stat.duke.edu</tt>\n</center>\n<p>\nThe talk will cover aspects of statistical analysis of oligonuceotide\nmicroarray data, with a specific focus expression profiling in cancer.\nAmong topics to be discussed and exemplified include issues arising\nin developing predictive regression models for evaluating and\ncharacterising expression patterns associated with clinical or\nphysiological states --- i.e., formal statistical approaches to\nthe canonical molecular phenotyping problem. I will discuss the\nuse of singular-value regression ideas within a Bayesian framework that\naddresses the ``large $p$, small $n$'' regression problem posed in such\napplications. Breast cancer studies highlight critical challenges,\nincluding data quality, definition, variable (gene) selection, and\naspects of model evaluation using cross-validation. Two specific\napplications in breast cancer will be used to convey and review\nbasic data analysis and modelling issues, as well as to highlight\nconceptual and practical research challenges in this area. The\nuse of such expression data in developing models of gene interactions\nthat may relate to underlying regulatory networks will be mentioned,\nas will a range of issues germane to the oligonucleotide technology.\n</p>\n<a name=\"ArtOwen\"></a>\n<h4>Plaid Models for DNA Microarrays</h4>\n12:05pm -- 12:30pm<center>\nArt Owen (Stanford), <tt>art@stat.stanford.edu</tt>\n</center>\n<p>\nThis talk describes the plaid model, a tool for exploratory analysis\nof multivariate data.  The motivating application is the search for\ninterpretable biological structure in gene expression microarray data.\nInterpretable structure can mean that a set of genes has a similar\nexpression pattern, in the samples under study, or in just a subset of\nthem (such as the cancerous samples).\n\nA set of genes behaving similarly in a set of samples, defines what\nwe call a ``layer''.  These are very much like clusters, except that:\ngenes can belong to more than one layer or to none of them, the layer\nmay be defined with respect to only a subset of the samples, and the\nrole of genes and samples is symmetric in our formulation.\n\nThe plaid model is a superposition of two way anova models, each defined\nover subsets of genes and samples.  We will present the plaid model,\nan interior point style algorithm for fitting it, and some examples from\nyeast DNA arrays and other problems.\n\nThis is joint work with Laura Lazzeroni.\n</p>\n<hr><center>\n<h2>Medical Informatics</h2>\nCosta Mesa Room<br>\n2:00pm -- 3:00pm<br>\n</center>\n<a name=\"JoyceNiland\"></a>\n<h4>Integrating Data and Disciplines: Biostatistics and Biomedical Informatics</h4>\n2:00pm -- 2:25pm<center>\nJoyce Niland (City of Hope), <tt>JNiland@coh.org</tt>\n</center>\n<p>\nAs predicted by Dr. Richard Klausner, Director of the National Cancer\nInstitute (NCI), ``Biomedical informatics is the future of integrating\nscience and medical research.'' In response to the explosion of data\nfrom the sequencing of the human genome and the critical objective\nof correlating genotypic and phenotypic data, informatics tools for\ncollecting, managing, retrieving, and analyzing both clinical and\ngenetic data are needed.  At City of Hope we have formed a Division of\nInformation Sciences to create and maintain such systems, linking the\nhighly synergistic and inter-dependent disciplines of Biostatistics and\nBiomedical Informatics. These systems support the conduct of clinical\nresearch, the mining of genetic data, and the ultimate integration of data\nstemming from numerous source data systems.  Internet-based interfaces\nand data warehousing principles factor strongly in the systems under\ndevelopment through collaborative work between faculty and staff members\nin Biostatistics and Biomedical Informatics.  Several key data systems\nwill be described, along with the human, information and technological\nissues that must be considered in their creation and maintenance.\nOur research data warehouse architecture will be outlined, representing\nan optimal form of data integration in support of clinical and genetic\nresearch.\n</p>\n<a name=\"WandaPratt\"></a>\n<h4>The Trouble with Text: Challenges and Promises of Biomedical Information Retrieval Technology</h4>\n2:30pm -- 2:55pm<center>\nWanda Pratt (University of California, Irvine), <tt>pratt@ics.uci.edu</tt>\n</center>\n<p>\nThe ultimate motivation for bioinformatics researchers is to improve\nhealth care, but the traditional publication and retrieval of results in\ntextual journal articles has become a bottleneck in the dissemination\nof scientific advances. The millions of available articles, even\nwithin a narrow subfield, easily overwhelm both biomedical researchers\nand health-care providers, regardless of whether they are pursuing a\nresearch hypothesis or deciding how to care for a particular patient. Many\ncharacteristics of traditional word-based approaches make efficient and\neffective retrieval of biomedical texts particularly challenging. In\nthis presentation, I will discuss examples of recent approaches that\ntake advantage of common knowledge of biomedicine and representation\nstandards to address these challenges and provide improved organization\nand retrieval of biomedical textual information.\n</p>\n<hr><center>\n<h2>Automated Analysis of Brain Images</h2>\nCosta Mesa Room<br>\n3:30pm -- 5:15pm<br>\n</center>\n<a name=\"MichaelMiller\"></a>\n<h4>On Metrics and Variational Equations of Computational Anatomy</h4>\n3:30pm -- 4:15pm<center>\nMichael Miller (Johns Hopkins University), <tt>mim@cis.jhu.edu</tt>\n</center>\n<p>\nWe review recent advances in the Emerging Discipline of Computational\nAnatomy.  We begin by defining anatomy as an orbit under groups of\ndiffeomorphisms. Metrics on the orbits are defined via geodesic distance\nin the spaces of diffeomorphisms. This induces a metric on the space\nof anatomical images.  Estimation problems are defined associated with\nestimating the underlying diffeomorphisms generating the anatomies\nfrom Medical images.  The variational problems defined are associated\nwith finding the infimum length (Energy) diffeomorphism connecting the\nMedical imagery with the underlying anatomical structures.  Euler-Lagrange\nequations will be discussed.\n\nApplications will be described to the study of the neocortex of the\nmacaque, tumor generation, and hypothesis testing in neuropsychiatric\napplications in Aging and Schizophrenia.\n</p>\n<a name=\"WilliamFEddy\"></a>\n<h4>Visual Analysis of Variance: A Tool for Quantitative Assessment of fMRI Data Processing and Analysis</h4>\n4:20pm -- 4:45pm<center>\nWilliam F. Eddy (Carnegie Mellon University), <tt>bill@stat.cmu.edu</tt><br>\nR. L. McNamee (University of Pittsburgh), <tt>rlandes@neuronet.pitt.edu</tt>\n</center>\n<p>\nThe raw data from an fMRI experiment are pre-processed before\nstatistical analysis in order to produce useful results.  This includes\nan inverse Fourier transform and other steps, each intended to improve\nthe quality of the resulting images.  Each pre-processing step has been\njustified by careful study of some prior experiment. On the other hand,\nthe formal statistical analysis often contains within it some internal\nassessment of the quality of the experimental data, such as the residual\nsum of squares from a fitted model.  Here, we propose an analog to\nthe analysis of variance, which we call a Visual Analysis of Variance\n(VANOVA), as a tool for assessing the importance of each pre-processing\nstep. A VANOVA provides both quantitative and visual information which\naid in the assignment of causes of variability and the determination of\nstatistical significance. In fact, a VANOVA is the natural extension, to\nthe pre-processing, of an ANOVA assessment of the statistical modeling.\nBecause the VANOVA is not intended to be as formal as an ANOVA, it can\ninclude evaluation of pre-processing steps which are neither linear nor\northogonal, both requirements of the partitions in an ANOVA.\n</p>\n<a name=\"RichardLeahy\"></a>\n<h4>Positron Emission Tomography: Image Formation and Analysis</h4>\n4:50pm -- 5:15pm<center>\nRichard Leahy (University of Southern California), <tt>leahy@sipi.usc.edu</tt>\n</center>\n<p>\nPositron Emission Tomography is a powerful medical imaging modality\nfor investigating human and animal biochemistry and physiology.\nDetection of photon pairs produced by positron-electron annihilation\nproduces tomographic projections of the spatial distribution of\npositron-emitting nuclei. Tomographic reconstruction methods can then\nbe used to form volumetric images. By labelling biochemicals with\npositron-emitting nuclei, we can produce images of a wide variety of\nbiochemical and physiological processes. PET is now widely used in\ndetecting and staging cancer through imaging of glucose. In the brain,\nreceptor and transmitter ligands have been developed to study the\ndopamine and other neurochemical systems. The most exciting recent\ndevelopment in PET is the ability to directly image gene expression\nthrough the use of PET tracer/reporter gene combinations. In humans\nthis technique can be used, for example, to monitor the efficacy of\ngene therapy techniques. PET gene expression imaging is also being\nincreasingly used to studygene expression in transgenic animals.\nEssential to the success of positron tomography in these diverse\napplications is a combination of instrumentation-design optimized for\nspecific applications (e.g. humans vs. small animals) and image\nprocessing methods that maximize image quality when forming volumetric\nimages from PET data. After reviewing the instrumentation and\nprinciples behind PET, I will describe statistically-based approaches\nto reconstructing PET images. Using a Bayesian formulation, we combine\naccurate physical models of the physics underlying PET systems with\naccurate statistical models for photon limited data collected in these\nsystems. I will illustrate the impact of this approach on image\nquality through examples for clinical and animal studies. I will also\ndescribe our current work on evaluating image quality through a\ncombination of theoretical, Monte-Carlo and human-observer studies.\n</p>\n</body>\n</html>\n", "id": 16641.0}