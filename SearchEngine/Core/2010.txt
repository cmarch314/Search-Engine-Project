{"text": "Data Compression 1 FUNDAMENTAL CONCEPTS A brief introduction to information theory is provided in this section The definitions and assumptions necessary to a comprehensive discussion and evaluation of data compression methods are discussed The following string of characters is used to illustrate the concepts defined EXAMPLE aa bbb cccc ddddd eeeeee fffffffgggggggg 1 1 Definitions A code is a mapping of source messages words from the source alphabet alpha into codewords words of the code alphabet beta The source messages are the basic units into which the string to be represented is partitioned These basic units may be single symbols from the source alphabet or they may be strings of symbols For string EXAMPLE alpha a b c d e f g space For purposes of explanation beta will be taken to be 1 Codes can be categorized as block block block variable variable block or variable variable where block block indicates that the source messages and codewords are of fixed length and variable variable codes map variable length source messages into variable length codewords A block block code for EXAMPLE is shown in Figure 1 1 and a variable variable code is given in Figure 1 2 If the string EXAMPLE were coded using the Figure 1 1 code the length of the coded message would be 12 using Figure 1 2 the length would be 3 source message codeword source message codeword a aa b 1 bbb 1 c 1 cccc 1 d 11 ddddd 11 e 1 eeeeee 1 f 1 1 fffffff 1 1 g 11 gggggggg 11 space 111 space 111 Figure 1 1 A block block code Figure 1 2 A variable variable code The oldest and most widely used codes ASCII and EBCDIC are examples of block block codes mapping an alphabet of 64 or 256 single characters onto 6 bit or 8 bit codewords These are not discussed as they do not provide compression The codes featured in this survey are of the block variable variable variable and variable block types When source messages of variable length are allowed the question of how a message ensemble sequence of messages is parsed into individual messages arises Many of the algorithms described here are defined word schemes That is the set of source messages is determined prior to the invocation of the coding scheme For example in text file processing each character may constitute a message or messages may be defined to consist of alphanumeric and non alphanumeric strings In Pascal source code each token may represent a message All codes involving fixed length source messages are by default defined word codes In free parse methods the coding algorithm itself parses the ensemble into variable length sequences of symbols Most of the known data compression methods are defined word schemes the free parse model differs in a fundamental way from the classical coding paradigm A code is distinct if each codeword is distinguishable from every other i e the mapping from source messages to codewords is one to one A distinct code is uniquely decodable if every codeword is identifiable when immersed in a sequence of codewords Clearly each of these features is desirable The codes of Figure 1 1 and Figure 1 2 are both distinct but the code of Figure 1 2 is not uniquely decodable For example the coded message 11 could be decoded as either ddddd or bbbbbb A uniquely decodable code is a prefix code or prefix free code if it has the prefix property which requires that no codeword is a proper prefix of any other codeword All uniquely decodable block block and variable block codes are prefix codes The code with codewords 1 1 is an example of a code which is uniquely decodable but which does not have the prefix property Prefix codes are instantaneously decodable that is they have the desirable property that the coded message can be parsed into codewords without the need for lookahead In order to decode a message encoded using the codeword set 1 1 lookahead is required For example the first codeword of the message 1 1 is 1 but this cannot be determined until the last tenth symbol of the message is read if the string of zeros had been of odd length then the first codeword would have been 1 A minimal prefix code is a prefix code such that if x is a proper prefix of some codeword then x sigma is either a codeword or a proper prefix of a codeword for each letter sigma in beta The set of codewords 1 1 is an example of a prefix code which is not minimal The fact that 1 is a proper prefix of the codeword 1 requires that 11 be either a codeword or a proper prefix of a codeword and it is neither Intuitively the minimality constraint prevents the use of codewords which are longer than necessary In the above example the codeword 1 could be replaced by the codeword 1 yielding a minimal prefix code with shorter codewords The codes discussed in this paper are all minimal prefix codes In this section a code has been defined to be a mapping from a source alphabet to a code alphabet we now define related terms The process of transforming a source ensemble into a coded message is coding or encoding The encoded message may be referred to as an encoding of the source ensemble The algorithm which constructs the mapping and uses it to transform the source ensemble is called the encoder The decoder performs the inverse operation restoring the coded message to its original form 1 2 Classification of Methods In addition to the categorization of data compression schemes with respect to message and codeword lengths these methods are classified as either static or dynamic A static method is one in which the mapping from the set of messages to the set of codewords is fixed before transmission begins so that a given message is represented by the same codeword every time it appears in the message ensemble The classic static defined word scheme is Huffman coding Huffman 1952 In Huffman coding the assignment of codewords to source messages is based on the probabilities with which the source messages appear in the message ensemble Messages which appear more frequently are represented by short codewords messages with smaller probabilities map to longer codewords These probabilities are determined before transmission begins A Huffman code for the ensemble EXAMPLE is given in Figure 1 3 If EXAMPLE were coded using this Huffman mapping the length of the coded message would be 117 Static Huffman coding is discussed in Section 3 2 Other static schemes are discussed in Sections 2 and 3 source message probability codeword a 2 4 1 1 b 3 4 1 c 4 4 11 d 5 4 1 e 6 4 111 f 7 4 11 g 8 4 space 5 4 1 1 Figure 1 3 A Huffman code for the message EXAMPLE code length 117 A code is dynamic if the mapping from the set of messages to the set of codewords changes over time For example dynamic Huffman coding involves computing an approximation to the probabilities of occurrence on the fly as the ensemble is being transmitted The assignment of codewords to messages is based on the values of the relative frequencies of occurrence at each point in time A message x may be represented by a short codeword early in the transmission because it occurs frequently at the beginning of the ensemble even though its probability of occurrence over the total ensemble is low Later when the more probable messages begin to occur with higher frequency the short codeword will be mapped to one of the higher probability messages and x will be mapped to a longer codeword As an illustration Figure 1 4 presents a dynamic Huffman code table corresponding to the prefix aa bbb of EXAMPLE Although the frequency of space over the entire message is greater than that of b at this point in time b has higher frequency and therefore is mapped to the shorter codeword source message probability codeword a 2 6 1 b 3 6 space 1 6 11 Figure 1 4 A dynamic Huffman code table for the prefix aa bbb of message EXAMPLE Dynamic codes are also referred to in the literature as adaptive in that they adapt to changes in ensemble characteristics over time The term adaptive will be used for the remainder of this paper the fact that these codes adapt to changing characteristics is the source of their appeal Some adaptive methods adapt to changing patterns in the source Welch 1984 while others exploit locality of reference Bentley et al 1986 Locality of reference is the tendency common in a wide variety of text types for a particular word to occur frequently for short periods of time then fall into disuse for long periods All of the adaptive methods are one pass methods only one scan of the ensemble is required Static Huffman coding requires two passes one pass to compute probabilities and determine the mapping and a second pass for transmission Thus as long as the encoding and decoding times of an adaptive method are not substantially greater than those of a static method the fact that an initial scan is not needed implies a speed improvement in the adaptive case In addition the mapping determined in the first pass of a static coding scheme must be transmitted by the encoder to the decoder The mapping may preface each transmission that is each file sent or a single mapping may be agreed upon and used for multiple transmissions In one pass methods the encoder defines and redefines the mapping dynamically during transmission The decoder must define and redefine the mapping in sympathy in essence learning the mapping as codewords are received Adaptive methods are discussed in Sections 4 and 5 An algorithm may also be a hybrid neither completely static nor completely dynamic In a simple hybrid scheme sender and receiver maintain identical codebooks containing k static codes For each transmission the sender must choose one of the k previously agreed upon codes and inform the receiver of his choice by transmitting first the name or number of the chosen code Hybrid methods are discussed further in Section 2 and Section 3 2 1 3 A Data Compression Model In order to discuss the relative merits of data compression techniques a framework for comparison must be established There are two dimensions along which each of the schemes discussed here may be measured algorithm complexity and amount of compression When data compression is used in a data transmission application the goal is speed Speed of transmission depends upon the number of bits sent the time required for the encoder to generate the coded message and the time required for the decoder to recover the original ensemble In a data storage application although the degree of compression is the primary concern it is nonetheless necessary that the algorithm be efficient in order for the scheme to be practical For a static scheme there are three algorithms to analyze the map construction algorithm the encoding algorithm and the decoding algorithm For a dynamic scheme there are just two algorithms the encoding algorithm and the decoding algorithm Several common measures of compression have been suggested redundancy Shannon and Weaver 1949 average message length Huffman 1952 and compression ratio Rubin 1976 Ruth and Kreutzer 1972 These measures are defined below Related to each of these measures are assumptions about the characteristics of the source It is generally assumed in information theory that all statistical parameters of a message source are known with perfect accuracy Gilbert 1971 The most common model is that of a discrete memoryless source a source whose output is a sequence of letters or messages each letter being a selection from some fixed alphabet a The letters are taken to be random statistically independent selections from the alphabet the selection being made according to some fixed probability assignment p a Gallager 1968 Without loss of generality the code alphabet is assumed to be 1 throughout this paper The modifications necessary for larger code alphabets are straightforward It is assumed that any cost associated with the code letters is uniform This is a reasonable assumption although it omits applications like telegraphy where the code symbols are of different durations The assumption is also important since the problem of constructing optimal codes over unequal code letter costs is a significantly different and more difficult problem Perl et al and Varn have developed algorithms for minimum redundancy prefix coding in the case of arbitrary symbol cost and equal codeword probability Perl et al 1975 Varn 1971 The assumption of equal probabilities mitigates the difficulty presented by the variable symbol cost For the more general unequal letter costs and unequal probabilities model Karp has proposed an integer linear programming approach Karp 1961 There have been several approximation algorithms proposed for this more difficult problem Krause 1962 Cot 1977 Mehlhorn 198 When data is compressed the goal is to reduce redundancy leaving only the informational content The measure of information of a source message x in bits is lg p x lg denotes the base 2 logarithm This definition has intuitive appeal in the case that p x 1 it is clear that x is not at all informative since it had to occur Similarly the smaller the value of p x the more unlikely x is to appear hence the larger its information content The reader is referred to Abramson for a longer more elegant discussion of the legitimacy of this technical definition of the concept of information Abramson 1963 pp 6 13 The average information content over the source alphabet can be computed by weighting the information content of each source letter by its probability of occurrence yielding the expression SUM i 1 to n p a i lg p a i This quantity is referred to as the entropy of a source letter or the entropy of the source and is denoted by H Since the length of a codeword for message a i must be sufficient to carry the information content of a i entropy imposes a lower bound on the number of bits required for the coded message The total number of bits must be at least as large as the product of H and the length of the source ensemble Since the value of H is generally not an integer variable length codewords must be used if the lower bound is to be achieved Given that message EXAMPLE is to be encoded one letter at a time the entropy of its source can be calculated using the probabilities given in Figure 1 3 H 2 894 so that the minimum number of bits contained in an encoding of EXAMPLE is 116 The Huffman code given in Section 1 2 does not quite achieve the theoretical minimum in this case Both of these definitions of information content are due to Shannon A derivation of the concept of entropy as it relates to information theory is presented by Shannon Shannon and Weaver 1949 A simpler more intuitive explanation of entropy is offered by Ash Ash 1965 The most common notion of a good code is one which is optimal in the sense of having minimum redundancy Redundancy can be defined as SUM p a i l i SUM p a i lg p a i where l i is the length of the codeword representing message a i The expression SUM p a i l i represents the lengths of the codewords weighted by their probabilities of occurrence that is the average codeword length The expression SUM p a i lg p a i is entropy H Thus redundancy is a measure of the difference between average codeword length and average information content If a code has minimum average codeword length for a given discrete probability distribution it is said to be a minimum redundancy code We define the term local redundancy to capture the notion of redundancy caused by local properties of a message ensemble rather than its global characteristics While the model used for analyzing general purpose coding techniques assumes a random distribution of the source messages this may not actually be the case In particular applications the tendency for messages to cluster in predictable patterns may be known The existence of predictable patterns may be exploited to minimize local redundancy Examples of applications in which local redundancy is common and methods for dealing with local redundancy are discussed in Section 2 and Section 6 2 Huffman uses average message length SUM p a i l i as a measure of the efficiency of a code Clearly the meaning of this term is the average length of a coded message We will use the term average codeword length to represent this quantity Since redundancy is defined to be average codeword length minus entropy and entropy is constant for a given probability distribution minimizing average codeword length minimizes redundancy A code is asymptotically optimal if it has the property that for a given probability distribution the ratio of average codeword length to entropy approaches 1 as entropy tends to infinity That is asymptotic optimality guarantees that average codeword length approaches the theoretical minimum entropy represents information content which imposes a lower bound on codeword length The amount of compression yielded by a coding scheme can be measured by a compression ratio The term compression ratio has been defined in several ways The definition C average message length average codeword length captures the common meaning which is a comparison of the length of the coded message to the length of the original ensemble Cappellini 1985 If we think of the characters of the ensemble EXAMPLE as 6 bit ASCII characters then the average message length is 6 bits The Huffman code of Section 1 2 represents EXAMPLE in 117 bits or 2 9 bits per character This yields a compression ratio of 6 2 9 representing compression by a factor of more than 2 Alternatively we may say that Huffman encoding produces a file whose size is 49 of the original ASCII file or that 49 compression has been achieved A somewhat different definition of compression ratio by Rubin C S O OR S includes the representation of the code itself in the transmission cost Rubin 1976 In this definition S represents the length of the source ensemble O the length of the output coded message and OR the size of the output representation eg the number of bits required for the encoder to transmit the code mapping to the decoder The quantity OR constitutes a charge to an algorithm for transmission of information about the coding scheme The intention is to measure the total size of the transmission or file to be stored 1 4 Motivation As discussed in the Introduction data compression has wide application in terms of information storage including representation of the abstract data type string Standish 198 and file compression Huffman coding is used for compression in several file archival systems ARC 1986 PKARC 1987 as is Lempel Ziv coding one of the adaptive schemes to be discussed in Section 5 An adaptive Huffman coding technique is the basis for the compact command of the UNIX operating system and the UNIX compress utility employs the Lempel Ziv approach UNIX 1984 In the area of data transmission Huffman coding has been passed over for years in favor of block block codes notably ASCII The advantage of Huffman coding is in the average number of bits per character transmitted which may be much smaller than the lg n bits per character where n is the source alphabet size of a block block system The primary difficulty associated with variable length codewords is that the rate at which bits are presented to the transmission channel will fluctuate depending on the relative frequencies of the source messages This requires buffering between the source and the channel Advances in technology have both overcome this difficulty and contributed to the appeal of variable length codes Current data networks allocate communication resources to sources on the basis of need and provide buffering as part of the system These systems require significant amounts of protocol and fixed length codes are quite inefficient for applications such as packet headers In addition communication costs are beginning to dominate storage and processing costs so that variable length coding schemes which reduce communication costs are attractive even if they are more complex For these reasons one could expect to see even greater use of variable length coding in the future It is interesting to note that the Huffman coding algorithm originally developed for the efficient transmission of data also has a wide variety of applications outside the sphere of data compression These include construction of optimal search trees Zimmerman 1959 Hu and Tucker 1971 Itai 1976 list merging Brent and Kung 1978 and generating optimal evaluation trees in the compilation of expressions Parker 198 Additional applications involve search for jumps in a monotone function of a single variable sources of pollution along a river and leaks in a pipeline Glassey and Karp 1976 The fact that this elegant combinatorial algorithm has influenced so many diverse areas underscores its importance ", "_id": "http://www.ics.uci.edu/~dan/pubs/DC-Sec1.html", "title": " data compression -- section 1 ", "html": "<HTML>\n<HEAD>\n<TITLE> Data Compression -- Section 1 </TITLE>\n</HEAD><BODY>\n\n<H1> Data Compression </H1>\n\n<a name=\"Sec_1\">\n<H2> 1.  FUNDAMENTAL CONCEPTS</H2> </a>\n\n<A HREF=\"DataCompression.html\"><IMG SRC=\"prev.gif\" ALT=\"PREV section\"></A>\n<A HREF=\"DC-Sec2.html\"><IMG SRC=\"next.gif\" ALT=\"NEXT section\"></A>\n<P>\n\n\tA brief introduction to information theory is\nprovided in this section.  The definitions and assumptions\nnecessary to a comprehensive discussion and evaluation of\ndata compression methods are discussed.  The following string of \ncharacters is used to illustrate the concepts defined:  \n<VAR>EXAMPLE</VAR> = <kbd>aa bbb cccc ddddd eeeeee fffffffgggggggg</kbd>.\n\n<a name=\"Sec_1.1\">\n<H3> 1.1  Definitions</H3> </a>\n\n\tA code is a mapping of <EM>source messages</EM> (words from the source \nalphabet <VAR>alpha</VAR>) into <EM>codewords</EM> (words of the code alphabet <VAR>beta</VAR>).  \nThe source messages are the basic units into which the string to be\nrepresented is partitioned.  These basic units may be single symbols\nfrom the source alphabet, or they may be strings of symbols. \nFor string <VAR>EXAMPLE</VAR>, <VAR>alpha</VAR> = { a, b, c, d, e, f, g, <VAR>space</VAR>}.  \nFor purposes of explanation, <VAR>beta</VAR> will be taken to be { 0, 1 }. \nCodes can be categorized as block-block, \nblock-variable, variable-block or variable-variable, where block-block\nindicates that the source messages and codewords are of fixed\nlength and variable-variable codes map variable-length source messages\ninto variable-length codewords.  A block-block code for <VAR>EXAMPLE</VAR> \nis shown in Figure 1.1 and a variable-variable code is given in Figure 1.2.\nIf the string <VAR>EXAMPLE</VAR> were coded using the Figure 1.1 code, the \nlength of the coded message would be 120; using Figure 1.2 the length would\nbe 30.  \n\n<PRE>\nsource message   codeword             source message   codeword\n\n     <VAR>a</VAR>             000                    <VAR>aa</VAR>             0\n     <VAR>b</VAR>             001                    <VAR>bbb</VAR>            1\n     <VAR>c</VAR>             010                    <VAR>cccc</VAR>           10\n     <VAR>d</VAR>             011                    <VAR>ddddd</VAR>          11\n     <VAR>e</VAR>             100                    <VAR>eeeeee</VAR>         100\n     <VAR>f</VAR>             101                    <VAR>fffffff</VAR>        101\n     <VAR>g</VAR>             110                    <VAR>gggggggg</VAR>       110\n   <VAR>space</VAR>           111                    <VAR>space</VAR>          111\n\nFigure 1.1: A block-block code     Figure 1.2: A variable-variable code.\n</PRE>\n\nThe oldest and most widely used codes,\nASCII and EBCDIC, are examples of block-block codes, mapping \nan alphabet of 64 (or 256) single\ncharacters onto 6-bit (or 8-bit) codewords.  These are not\ndiscussed, as they do not provide compression.\nThe codes featured in this survey are of the block-variable,\nvariable-variable, and variable-block types.  \n<P>\nWhen source \nmessages of variable length are allowed, the question\nof how a message <EM>ensemble</EM> (sequence of messages) is parsed into\nindividual messages arises.  Many of the algorithms \ndescribed here are <EM>defined-word schemes</EM>.  That is, the set of \nsource messages is determined prior to the invocation of the \ncoding scheme.  \nFor example, in text file processing each character may constitute\na message, or messages may be defined to consist of alphanumeric\nand non-alphanumeric strings.  In Pascal source code, each token\nmay represent a message.  All codes involving fixed-length source\nmessages are, by default, defined-word codes.\nIn <EM>free-parse</EM> methods, the coding algorithm itself parses the ensemble\ninto variable-length sequences of symbols.  Most of the known data\ncompression methods are defined-word schemes; the free-parse model\ndiffers in a fundamental way from the classical coding paradigm.\n<P>\n\tA code is <EM>distinct</EM> if each codeword is distinguishable\nfrom every other (i.e., the mapping from source messages to codewords is one-to-one).\nA distinct code is <EM>uniquely decodable</EM> if every codeword is identifiable\nwhen immersed in a sequence of codewords.  Clearly, each of these features is \ndesirable.  The codes of Figure 1.1 and Figure 1.2 are both distinct, but\nthe code of Figure 1.2 is not uniquely decodable.  For example, the coded\nmessage 11 could be decoded as either <kbd>ddddd</kbd> or <kbd>bbbbbb</kbd>.\nA uniquely decodable code\nis a <EM>prefix code</EM> (or <EM>prefix-free code</EM>) if it has the prefix property,\nwhich requires that no codeword is a proper prefix\nof any other codeword.  All uniquely decodable block-block and \nvariable-block codes are\nprefix codes.  The code with codewords { 1, 100000, 00 } is an example  \nof a code which is uniquely decodable but which does not have the prefix\nproperty.  Prefix codes are <EM>instantaneously\ndecodable</EM>; that is, they have the desirable property that\nthe coded message can be parsed into codewords\nwithout the need for lookahead.  In order to decode a message encoded using\nthe codeword set { 1, 100000, 00 }, lookahead is required.  \nFor example, the first codeword of the message 1000000001 is 1,\nbut this cannot be determined until the last (tenth) symbol of the message \nis read (if the string of zeros had been of odd length, then the first \ncodeword would have been 100000).\n<P>\nA <EM>minimal</EM> prefix\ncode is a prefix code such that if <VAR>x</VAR> is a proper prefix of some codeword, \nthen <VAR>x sigma</VAR> is either a codeword or a proper prefix of a codeword,\nfor each letter <VAR>sigma</VAR> in <VAR>beta</VAR>.  The set of codewords { <kbd>00, 01, 10</kbd> }\nis an example of a prefix code which is not minimal.  The fact that <kbd>1</kbd> is\na proper prefix of the codeword <kbd>10</kbd> requires that <kbd>11</kbd> be either a codeword\nor a proper prefix of a codeword, and it is neither.\nIntuitively, the minimality constraint prevents \nthe use of codewords which are longer than necessary.  In the above example \nthe codeword <kbd>10</kbd> could be replaced by the codeword <kbd>1</kbd>, yielding a \nminimal prefix code with shorter codewords.  The codes discussed\nin this paper are all minimal prefix codes.\n<P>\nIn this section, a <EM>code</EM> has been defined to be a mapping from a \nsource alphabet to a code alphabet; we now define related terms.\nThe process of transforming a source ensemble into a coded message\nis <EM>coding</EM> or <EM>encoding</EM>.  The encoded message may be\nreferred to as an <EM>encoding</EM> of the source ensemble.  The\nalgorithm which constructs the mapping and uses it to transform the\nsource ensemble is called the <EM>encoder</EM>.  The <EM>decoder</EM>\nperforms the inverse operation, restoring the coded message to its\noriginal form.\n\n<a name=\"Sec_1.2\">\n<H3> 1.2  Classification of Methods</H3> </a>\n\n\tIn addition to the categorization of data compression schemes\nwith respect to message and codeword lengths, these methods are\nclassified as either static or dynamic.\nA <EM>static</EM> method is one in which the mapping from the set of messages\nto the set of codewords is fixed before transmission begins, so that\na given message is represented by the same codeword\nevery time it appears in the message ensemble.  The classic static\ndefined-word scheme is Huffman coding [Huffman 1952].  In Huffman \ncoding, the assignment of codewords to source messages is based on the \nprobabilities with which the source messages appear in the \nmessage ensemble.  Messages which appear more frequently are represented\nby short codewords; messages with smaller probabilities map to longer \ncodewords.  These probabilities are determined before transmission begins.\nA Huffman code for the ensemble <VAR>EXAMPLE</VAR> is given in Figure 1.3.\nIf <VAR>EXAMPLE</VAR> were coded using this Huffman mapping, the length of the\ncoded message would be 117.\nStatic Huffman coding is discussed in\n<a href=\"DC-Sec3.html#Sec_3.2\">Section 3.2</a>.  Other  static schemes are\ndiscussed in\n<a href=\"DC-Sec2.html#Sec_2\">Sections 2</a> and <a href=\"DC-Sec3.html#Sec_3\">3</a>. \n\n<PRE>\n   source message   probability      codeword\n\n        <VAR>a</VAR>             2/40           1001\n        <VAR>b</VAR>             3/40           1000\n        <VAR>c</VAR>             4/40           011\n        <VAR>d</VAR>             5/40           010\n        <VAR>e</VAR>             6/40           111\n        <VAR>f</VAR>             7/40           110\n        <VAR>g</VAR>             8/40           00\n      <VAR>space</VAR>           5/40           101\n\nFigure 1.3 -- A Huffman code for the message <VAR>EXAMPLE</VAR> (code length=117).\n</PRE>\n\n\tA code is <EM>dynamic</EM> if the mapping from the set of messages to the\nset of codewords changes over time.  For example, dynamic Huffman coding \ninvolves computing an approximation to the probabilities of occurrence \n\"on the fly\", as the ensemble is being\ntransmitted.  The assignment of codewords to messages is based on\nthe values of the relative frequencies of occurrence at each point in time.  \nA message <VAR>x</VAR> may\nbe represented by a short codeword early in the transmission because \nit occurs frequently at the beginning of the ensemble, even though its \nprobability of occurrence over the total ensemble is low.  Later, \nwhen the more probable messages begin to occur with higher frequency, \nthe short codeword will be mapped to one of the higher probability\nmessages and <VAR>x</VAR> will be mapped to a longer codeword.  As an illustration,\nFigure 1.4 presents a dynamic Huffman code table corresponding to the\nprefix <kbd>aa bbb</kbd> of <VAR>EXAMPLE</VAR>.  Although the frequency of <VAR>space</VAR>\nover the entire message is greater than that of <VAR>b</VAR>,  at this\npoint in time <VAR>b</VAR> has higher frequency and therefore is \nmapped to the shorter codeword.\n\n<PRE>\n   source message   probability      codeword\n\n        <VAR>a</VAR>             2/6            10\n        <VAR>b</VAR>             3/6            0\n      <VAR>space</VAR>           1/6            11\n\nFigure 1.4 -- A dynamic Huffman code table for the prefix\n              <kbd>aa bbb</kbd> of message <VAR>EXAMPLE</VAR>.\n</PRE>\n\n\tDynamic codes are also referred to in the literature as\n<EM>adaptive</EM>, in that they adapt to changes in ensemble characteristics\nover time.  The term adaptive will be used for the remainder of this\npaper; the fact that these codes adapt to changing characteristics is\nthe source of their appeal.  Some adaptive methods adapt to changing patterns\nin the source [Welch 1984] while others exploit locality of reference \n[Bentley et al. 1986].  Locality of reference is the tendency, common \nin a wide variety of text types, for a particular word to occur\nfrequently for short periods of time then fall into disuse for long\nperiods.\n<P>\nAll of the adaptive methods are <EM>one-pass</EM> methods; only\none scan of the ensemble is required. \nStatic Huffman coding requires two passes:\none pass to compute probabilities and determine the mapping, and a\nsecond pass for transmission.  Thus, as long as the encoding and decoding \ntimes of an adaptive method are not substantially greater than those of\na static method, the fact that an initial scan is not needed implies\na speed improvement in the adaptive case.   In addition, the mapping \ndetermined in the first pass of a static coding scheme \nmust be transmitted by the encoder to the decoder.  The mapping may\npreface each transmission (that is, each file sent), or a single mapping \nmay be agreed upon and used for multiple transmissions.\nIn one-pass methods the encoder defines and redefines the mapping dynamically, \nduring transmission.  The decoder must define and redefine the mapping in\nsympathy, in essence \"learning\" the mapping as codewords are received.\nAdaptive methods are discussed in\n<a href=\"DC-Sec4.html#Sec_4\">Sections 4</a> and <a href=\"DC-Sec5.html#Sec_5\">5</a>.  \n<P>\nAn algorithm may also be a <EM>hybrid</EM>, neither completely\nstatic nor completely dynamic.  In a simple hybrid scheme,\nsender and receiver maintain identical <EM>codebooks</EM> \ncontaining <VAR>k</VAR> static codes.  For each transmission, \nthe sender must choose one of the <VAR>k</VAR> previously-agreed-upon codes \nand inform the receiver of his choice (by transmitting first the\n\"name\" or number of the chosen code).\nHybrid methods are discussed further in\n<a href=\"DC-Sec2.html#Sec_2\">Section 2</a> and <a href=\"DC-Sec3.html#Sec_3.2\">Section 3.2</a>.\n\n<a name=\"Sec_1.3\">\n<H3> 1.3  A Data Compression Model</H3> </a>\n\n\tIn order to discuss the relative merits of data compression\ntechniques, a framework for comparison must be established.  There\nare two dimensions along which each of the schemes discussed here\nmay be measured, algorithm complexity and amount of compression.\nWhen data compression is used in a data transmission application,\nthe goal is speed.  Speed of transmission depends upon the number\nof bits sent, the time required for the encoder to generate the\ncoded message, and the time required for the decoder to recover\nthe original ensemble.  In a data storage application, although the\ndegree of compression is the primary concern, it is nonetheless\nnecessary that the algorithm be efficient in order for the\nscheme to be practical. \nFor a static scheme, there are three algorithms to analyze:\nthe map construction algorithm, the encoding algorithm, and the\ndecoding algorithm.  For a dynamic scheme, there are just two algorithms:\nthe encoding algorithm, and the decoding algorithm.\n<P>\n\tSeveral common measures of compression have been \nsuggested:  redundancy [Shannon and Weaver 1949], average message\nlength [Huffman 1952], and compression ratio [Rubin 1976; \nRuth and Kreutzer 1972].  These measures are defined below.  Related to each of these measures\nare assumptions about the characteristics of the source.\nIt is generally assumed in information theory that all statistical\nparameters of a message source are known with perfect accuracy\n[Gilbert 1971].  The most common model is that of a discrete\nmemoryless source; a source whose output is a sequence of letters\n(or messages),\neach letter being a selection from some fixed alphabet <kbd>a</kbd>,...\nThe letters are taken to be random, statistically independent\nselections from the alphabet, the selection being made according\nto some fixed probability assignment <VAR>p</VAR>(<kbd>a</kbd>),... [Gallager 1968].\nWithout loss of generality, the code alphabet is assumed\nto be {0,1} throughout this paper.  The modifications\nnecessary for larger code alphabets are straightforward.\n<P>\n\tIt is assumed that any cost associated with the code \nletters is uniform.  This is a reasonable assumption, although it\nomits applications like telegraphy where the code symbols are of\ndifferent durations.  The assumption is also important, since \nthe problem of constructing optimal codes over unequal code letter costs \nis a significantly different and more difficult problem.\nPerl et al. and Varn have developed algorithms for minimum-redundancy \nprefix coding in the case of arbitrary symbol cost and equal \ncodeword probability [Perl et al. 1975; Varn 1971].  \nThe assumption of equal probabilities mitigates the difficulty\npresented by the variable symbol cost.  For the more general unequal \nletter costs and unequal probabilities model, Karp has proposed an \ninteger linear programming approach [Karp 1961].  There have been \nseveral approximation algorithms proposed for this more difficult \nproblem [Krause 1962; Cot 1977; Mehlhorn 1980].\n<P>\n\tWhen data is compressed, the goal is to reduce redundancy,\nleaving only the informational content.  The measure of information\nof a source message <VAR>x</VAR> (in bits) is -lg <VAR>p</VAR>(<VAR>x</VAR>)\n[lg denotes the base 2 logarithm].  This definition\nhas intuitive appeal; in the case that <VAR>p</VAR>(<VAR>x</VAR>=1,\nit is clear that <VAR>x</VAR> is not at all informative since it had to occur.\nSimilarly, the smaller the value of <VAR>p</VAR>(<VAR>x</VAR>,\nthe more unlikely <VAR>x</VAR>\nis to appear, hence the larger its information content.  The reader\nis referred to Abramson for a longer, more elegant\ndiscussion of the legitimacy of this technical definition of the\nconcept of information [Abramson 1963, pp. 6-13].  \nThe average information content over\nthe source alphabet can be computed by weighting the information content\nof each source letter by its probability of occurrence, yielding the\nexpression SUM{i=1 to n} [-<VAR>p</VAR>(<VAR>a</VAR>(<VAR>i</VAR>)) lg <VAR>p</VAR>(<VAR>a</VAR>(<VAR>i</VAR>))].  This quantity is\nreferred to as the <VAR>entropy</VAR> of a source letter, or the entropy of the \nsource, and is denoted by <VAR>H</VAR>.   \nSince the length of a codeword for message <VAR>a</VAR>(<VAR>i</VAR>)\nmust be sufficient to carry the information content of <VAR>a</VAR>(<VAR>i</VAR>),\nentropy imposes a lower bound on the number of bits required for the\ncoded message.  The total number of bits must be at least as large as\nthe product of <VAR>H</VAR> and the length of the source ensemble.\nSince the value of <VAR>H</VAR> is generally not an integer, variable length\ncodewords must be used if the lower bound is to be achieved.\nGiven that message <VAR>EXAMPLE</VAR> is to be encoded one letter at a time,\nthe entropy of its source can be calculated using the probabilities \ngiven in Figure 1.3:\n<VAR>H</VAR> = 2.894, so that the minimum number of bits contained \nin an encoding of <VAR>EXAMPLE</VAR> is 116.\nThe Huffman code given in <a href=\"#Sec_1.2\">Section 1.2</a> does not quite\nachieve the theoretical minimum in this case.\n<P>\nBoth of these definitions of information content are due to \nShannon.  A derivation of the concept of entropy as it relates \nto information theory is presented by Shannon [Shannon and Weaver 1949].\nA simpler, more intuitive explanation of entropy is offered by Ash\n[Ash 1965].\n<P>\n\tThe most common notion of a \"good\" code is one which \nis <EM>optimal</EM> in the sense of having minimum redundancy.  <EM>Redundancy</EM>\ncan be defined as:  SUM <VAR>p</VAR>(<VAR>a</VAR>(<VAR>i</VAR>)) <VAR>l</VAR>(<VAR>i</VAR>)\n- SUM [-<VAR>p</VAR>(<VAR>a</VAR>(<VAR>i</VAR>)) lg <VAR>p</VAR>(<VAR>a</VAR>(<VAR>i</VAR>))] where <VAR>l</VAR>(<VAR>i</VAR>) is\nthe length of the codeword representing message <VAR>a</VAR>(<VAR>i</VAR>).  The expression\nSUM <VAR>p</VAR>(<VAR>a</VAR>(<VAR>i</VAR>)) <VAR>l</VAR>(<VAR>i</VAR>) represents the lengths of the codewords weighted\nby their probabilities of occurrence, that is, the average codeword length.\nThe expression SUM [-<VAR>p</VAR>(<VAR>a</VAR>(<VAR>i</VAR>)) lg <VAR>p</VAR>(<VAR>a</VAR>(<VAR>i</VAR>))] is entropy, <VAR>H</VAR>.  Thus,\nredundancy is a measure of the difference between average \ncodeword length and average information content.  If a code has \nminimum average codeword length for a given discrete probability distribution,\nit is said to be a minimum redundancy code.\n<P>\nWe define the term <EM>local redundancy</EM> to capture the notion\nof redundancy caused by local properties of a message ensemble,\nrather than its global characteristics.  While the model used for\nanalyzing general-purpose coding techniques assumes a random distribution\nof the source messages, this may not actually be the case.  In particular\napplications the tendency for messages to cluster in predictable patterns\nmay be known.  The existence of predictable patterns may be exploited \nto minimize local redundancy.  Examples of applications in which local\nredundancy is common, and methods for dealing with local redundancy,\nare discussed in\n<a href=\"DC-Sec2.html#Sec_2\">Section 2</a> and <a href=\"DC-Sec678.html#Sec_6.2\">Section 6.2</a>.\n<P>\nHuffman uses <EM>average message length</EM>, SUM <VAR>p</VAR>(<VAR>a</VAR>(<VAR>i</VAR>)) <VAR>l</VAR>(<VAR>i</VAR>), as\na measure of the efficiency of a code.  Clearly the meaning of\nthis term is the average length of a <EM>coded</EM> message. \nWe will use the term <EM>average codeword length</EM> to represent\nthis quantity.  Since redundancy is defined to be average codeword\nlength minus entropy and entropy is constant\nfor a given probability distribution, minimizing average codeword\nlength minimizes redundancy.\n<P>\nA code is <EM>asymptotically optimal</EM> if it has the property\nthat for a given probability distribution, the ratio of average\ncodeword length to entropy approaches 1 as entropy tends to infinity.\nThat is, asymptotic optimality guarantees that average codeword length\napproaches the theoretical minimum (entropy represents information content,\nwhich imposes a lower bound on codeword length).\n<P>\n\tThe amount of compression yielded by a coding scheme can be\nmeasured by a <EM>compression ratio</EM>.  The term compression ratio\nhas been defined in several ways.  The definition   \n<VAR>C</VAR> = (average message length)/(average codeword length) \ncaptures the common meaning, which is a comparison of the length of the coded\nmessage to the length of the original ensemble [Cappellini 1985].  \nIf we think of the characters of the ensemble <VAR>EXAMPLE</VAR> as 6-bit ASCII\ncharacters, then the average message length is 6 bits.  The Huffman\ncode of\n<a href=\"#Sec_1.2\">Section 1.2</a> represents <VAR>EXAMPLE</VAR> in 117 bits,\nor 2.9 bits per character.  This yields a compression ratio of 6/2.9,\nrepresenting compression by a factor of more than 2.  Alternatively,\nwe may say that Huffman encoding produces a file whose size is\n49% of the original ASCII file, or that 49% compression has been achieved.\n \nA somewhat different definition of compression ratio, by Rubin,\n<VAR>C</VAR> = (<VAR>S</VAR> - <VAR>O</VAR> - <VAR>OR</VAR>)/<VAR>S</VAR>, includes the representation \nof the code itself in the transmission cost [Rubin 1976].  In this\ndefinition <VAR>S</VAR> represents the length of the source ensemble, <VAR>O</VAR> the \nlength of the output (coded message), and <VAR>OR</VAR> the size of the \"output \nrepresentation\" (eg., the number of bits required for the encoder to\ntransmit the code mapping to the decoder).  The quantity <VAR>OR</VAR> \nconstitutes a \"charge\" to an algorithm for transmission of information \nabout the coding scheme.  The intention is to measure the total size\nof the transmission (or file to be stored).\n\n<a name=\"Sec_1.4\">\n<H3> 1.4  Motivation</H3> </a>\n\n\tAs discussed in the Introduction, data compression has \nwide application in terms of information\nstorage, including representation of the abstract data type <EM>string</EM>\n[Standish 1980] and file compression.  Huffman coding is used for \ncompression in several file archival systems [ARC 1986; PKARC 1987], \nas is Lempel-Ziv coding, one of the adaptive schemes to be discussed \nin <a href=\"DC-Sec5.html#Sec_5\">Section 5</a>.\nAn adaptive Huffman coding technique is the basis for the <EM>compact</EM>\ncommand of the UNIX operating system, and the UNIX \n<EM>compress</EM> utility employs the Lempel-Ziv approach [UNIX 1984].\n<P>\n\tIn the area of data transmission, Huffman coding has been\npassed over for years in favor of block-block codes, notably ASCII.\nThe advantage of Huffman coding is in the average number of bits per character\ntransmitted, which may be much smaller than the lg <VAR>n</VAR> bits per\ncharacter (where <VAR>n</VAR> is the source alphabet size) of a block-block\nsystem.  The primary difficulty associated with variable-length\ncodewords is that the rate at which bits are presented to the \ntransmission channel will fluctuate, depending on the relative\nfrequencies of the source messages.  This requires buffering between\nthe source and the channel.  Advances in technology have both overcome\nthis difficulty and contributed to the appeal of variable-length\ncodes.  Current data networks allocate communication resources to\nsources on the basis of need and provide buffering as part of the \nsystem.  These systems require significant amounts of protocol, and \nfixed-length codes are quite inefficient for applications such as \npacket headers.  In addition, communication costs are beginning to\ndominate storage and processing costs, so that variable-length coding\nschemes which reduce communication costs are attractive even if they\nare more complex.  For these reasons, one could expect to see even\ngreater use of variable-length coding in the future.\n<P>\n\tIt is interesting to note that the Huffman coding algorithm,\noriginally developed for the efficient transmission of data, also\nhas a wide variety of applications outside the sphere of data\ncompression.  These include construction of optimal search trees\n[Zimmerman 1959; Hu and Tucker 1971; Itai 1976], list merging [Brent\nand Kung 1978], and generating optimal evaluation trees in the \ncompilation of expressions [Parker 1980].  Additional applications\ninvolve search for jumps in a monotone function of a single variable,\nsources of pollution along a river, and leaks in a pipeline [Glassey\nand Karp 1976].  The fact that this elegant combinatorial algorithm\nhas influenced so many diverse areas underscores its importance. \n\n<P>\n<A HREF=\"DataCompression.html\"><IMG SRC=\"prev.gif\" ALT=\"PREV section\"></A>\n<A HREF=\"DC-Sec2.html\"><IMG SRC=\"next.gif\" ALT=\"NEXT section\"></A>\n<P>\n</BODY></HTML>\n", "id": 2010.0}