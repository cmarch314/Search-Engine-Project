{"text": "CompSci 267 Homework set 2 Consider the following two codings with source alphabet a b c d e f and code alphabet 1 For each code Is it uniquely decodable If not prove it by giving two source messages that encode to the same code message If it is uniquely decodable is it a prefix code If not a prefix code give an equivalent prefix code with the same lengths of code words changing as few codings as possible abcdefcode 1 1 111 1 11 111code 21 1 11 1 111 11 11 A full binary tree is right justified if at each level of the tree all the nonterminal vertices appear to the right of all the leaf vertices Show that there must be a lossless encoding method which employs fewer than 85N codebits to encode each right justified tree having N leaves if N is sufficiently large Compact codes come from conservative expansions of a trivial code Prove that for each compact code C there exists exactly one expansion sequence from a trivial code to C in which each expansion S S is an expansion of either the longest codelength L or the codelength L 1 in S Write a program to determine the number of compact codes of size 4 Consider the following generalization of Shannon Fano coding to create a binary codetree for the set S of words wi with associated probabilities pi create root vertex r and associate S with it initialize queue Q to contain r while Q is not empty do dequeue a vertex v having associated set V partition V into two sets V and V1 so that the sum of probabilities in each set is as nearly equal as possible create two new vertices v with set V associated with it and v1 with set V1 associated with it attach v and v1 as sons of v via edges labeled and 1 if V 1 then enqueue v if V1 1 then enqueue v1 Examine the cases when set S contains 4 or 5 words to see how much the compression rate of the codetree created by this approach can exceed entropy Comment on the practicality and effectiveness of this algorithm as compared with that of Shannon Fano Use a Shannon s method of coding b Shannon Fano coding and c Huffman coding to determine the average codelength of 1 a binary code and 2 a ternary code for a source having symbol probability distribution 22 2 18 15 1 8 7 ", "_id": "http://www.ics.uci.edu/~dan/class/267/hw2.htm", "title": "", "html": "<HTML>\n<center>\n<H3>CompSci 267 Homework set #2</H3>\n</center>\n<OL>\n<LI> Consider the following two codings with source alphabet\n     {a,b,c,d,e,f} and code alphabet {0,1}.\n     For each code,\n   <UL>\n   <LI> Is it uniquely decodable?\n        If not, prove it by giving two source messages that encode\n        to the same code message.\n   <LI> If it is uniquely decodable, is it a prefix code?\n        If not a prefix code, give an equivalent prefix code with the same\n        lengths of code words changing as few codings as possible.\n   </UL>\n  <TABLE>\n  <TR> <TD>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </TD> <TD>a</TD> <TD>b</TD> <TD>c</TD>\n       <TD>d</TD> <TD>e</TD> <TD>f</TD>\n  <TR> <TD>code 1</TD> <TD>01</TD> <TD>011</TD> <TD>10</TD>\n       <TD>1000</TD> <TD>0011</TD> <TD>0111</TD>\n  <TR> <TD>code 2</TD> <TD>1010</TD> <TD>001</TD> <TD>101</TD>\n       <TD>0001</TD> <TD>1101</TD> <TD>1011</TD>\n  </TABLE>\n<BR> &nbsp;\n\n<!---\n  <LI> In a comma code, one code-symbol is used to end every codeword\n     and is used in no other way.  Suppose for <I>a</I>-ary encoding of\n     an <I>m</I>-letter alphabet, that the <I>m</I> words consist of the\n     <I>m</I> shortest distinct strings of non-comma code letters\n     followed by a comma.  Such a code is plainly prefix-free.\n     Verify that it satisfies the Kraft inequality.\n  </UL>\n<BR> &nbsp;\n--->\n\n<LI> A full binary tree is <I>right justified</I> if, at each level of\nthe tree, all the nonterminal vertices appear to the right of all the\nleaf vertices.  Show that there must be a lossless encoding method which\nemploys fewer than 0.85<I>N</I> codebits to encode each right justified\ntree having <I>N</I> leaves, if <I>N</I> is sufficiently large.\n<BR> &nbsp;\n<BR> &nbsp;\n\n<LI> Compact codes come from conservative expansions of a trivial code.\n  <UL type=a>\n  <LI> Prove that, for each compact code <I>C</I>, there exists exactly\n      one expansion sequence from a trivial code to <I>C</I>, in which\n      each expansion <I>S</I> &rArr; <I>S'</I>\n      is an expansion of either the longest codelength, <I>L</I>,\n      or the codelength <I>L</I>-1 in <I>S</I>.\n<BR> &nbsp;\n\n   <LI> Write a program to determine\n        the number of compact codes of size 40.\n   </UL>\n<BR> &nbsp;\n\n<!--\n<LI> Let the random variable <I>X</I> have five possible outcomes\n{1,2,3,4,5}.  Consider two distributions <I>p</I>(<I>x</I>)\nand <I>q</I>(<I>x</I>) on <I>X</I>.\n<table><tr><td width=25%>&nbsp;</td><td><TABLE border=1 cellspacing=0>\n<TR><TD>Symbol</TD>\n    <TD><I>p</I>(<I>x</I>)</TD> <TD><I>q</I>(<I>x</I>)</TD>\n    <TD>C<sub>1</sub>(<I>x</I>)</TD> <TD>C<sub>2</sub>(<I>x</I>)</TD>\n<TR><TD align=center>1</TD>\n    <TD>1/2</TD> <TD>1/2</TD> <TD>0</TD> <TD>0</TD>\n<TR><TD align=center>2</TD>\n    <TD>1/4</TD> <TD>1/8</TD> <TD>10</TD> <TD>100</TD>\n<TR><TD align=center>3</TD>\n    <TD>1/8</TD> <TD>1/8</TD> <TD>110</TD> <TD>101</TD>\n<TR><TD align=center>4</TD>\n    <TD>1/16</TD> <TD>1/8</TD> <TD>1110</TD> <TD>110</TD>\n<TR><TD align=center>5</TD>\n    <TD>1/16</TD> <TD>1/8</TD> <TD>1111</TD> <TD>111</TD>\n</TABLE>\n</td></table>\n  <UL type=a>\n  <LI> Calculate H(<I>p</I>), H(<I>q</I>), D(<I>p</I>||<I>q</I>),\n       and D(<I>q</I>||<I>p</I>).\n  <LI> The last two columns represent codes for the random variable.\n      <BR>Verify that the average length of C<sub>1</sub> under <I>p</I>\n       is equal to the entropy H(<I>p</I>), and thus C<sub>1</sub>\n       is optimal for <I>p</I>.\n      <BR>Verify that C<sub>2</sub> is optimal for <I>q</I>.\n  <LI> Now assume that we use code C<sub>2</sub> when the distribution\n       is <I>p</I>.\n      <BR>What is the average length of the codewords, and by how much does\n       it exceed the entropy of <I>p</I>.\n  <LI> What is the loss if we use code C<sub>1</sub> when the\n       distribution is <I>q</I>?\n  </UL>\n<BR> &nbsp;\n-->\n\n<!--\n<LI> A deck of <I>n</I> cards in order 1,2,...,<I>n</I> is provided.\n     One card is removed at random, then replaced at random.\n     What is the entropy of the resulting deck when <I>n</I>=16?\n     For general <I>n</I>?\n<BR> &nbsp;\n-->\n\n<LI> Consider the following generalization of Shannon-Fano coding to\n     create a binary codetree for the set S of words {w<sub>i</sub>}\n     with associated probabilities {p<sub>i</sub>}:\n   <UL type=SQUARE>\n   <LI> create root vertex <I>r</I> and associate S with it\n   <LI> initialize queue Q to contain <I>r</I>\n   <LI> while Q is not empty do\n       <UL type=*>\n       <LI> dequeue a vertex <I>v</I> having associated set V\n       <LI> partition V into two sets V<sub>0</sub> and V<sub>1</sub>\n            so that the sum of probabilities in each set is as\n            nearly equal as possible\n       <LI> create two new vertices, v<sub>0</sub>\n            (with set V<sub>0</sub> associated with it) and\n            v<sub>1</sub> (with set V<sub>1</sub> associated with it)\n       <LI> attach v<sub>0</sub> and v<sub>1</sub>\n            as sons of <I>v</I> via edges labeled 0 and 1\n       <LI> if |V<sub>0</sub>| > 1 then enqueue v<sub>0</sub>;\n            if |V<sub>1</sub>| > 1 then enqueue v<sub>1</sub>\n       </UL>\n   </UL>\n<P>\n<UL type=a>\n<LI> Examine the cases when set S contains 4 or 5 words to see\n     how much the compression rate of the codetree created by\n     this approach can exceed entropy.\n<LI> Comment on the practicality and effectiveness of this algorithm\n     as compared with that of Shannon-Fano.\n</UL>\n<BR> &nbsp;\n<BR> &nbsp;\n\n<LI> Use (a) Shannon's method of coding,\n     (b) Shannon-Fano coding,\n     and (c) Huffman coding,\n     <BR>to determine the average codelength of\n     (1) a binary code, and (2) a ternary code,\n     <BR>for a source having symbol probability distribution\n     <BR>.22, .2, .18, .15, .1, .08, .07.\n\n</OL>\n</HTML>\n", "id": 2082.0}