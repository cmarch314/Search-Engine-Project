{"text": "R42a Value Iteration And Policy Iteration Algorithms For Markov Decision Problem Elena Pashenkova Irina Rish irinar ics uci edu Rina Dechter dechter ics uci edu Abstract In this paper we consider computational aspects of decision theoretic planning modeled by Markov decision processes MDPs Commonly used algorithms such as value iteration VI Bellman 1957 and several versions of modified policy iteration MPI Puterman 1994 a modification of the original Howard s policy iteration PI Howard 196 are compared on a class of problems from the motion planning domain Policy iteration and its modifications are usually recommended as algorithms demonstrating a better performance than value iteration Russel Norvig 1995 Puterman 1994 However our results show that their performance is not always superior and depends on the parameters of a problem and the parameters of the algorithms such as number of iterations in the value determination procedure in MPI Moreover policy iteration applied to non discounted models without special restrictions might not even converge to an optimal policy as in case of the policy iteration algorithm introduced in Russel Norvig 1995 We also introduce a new stopping criterion into value iteration based on policy changes The combined value policy iteration CVPI algorithm proposed in the paper implements this criterion and generates an optimal policy faster then both policy and value iteration algorithms ps pdf ", "_id": "http://www.ics.uci.edu/~dechter/publications/r42a.html", "title": "\r\n        dr. rina dechter @ uci\r\n      ", "html": "<html>\r\n    <head>\r\n      <title>\r\n        Dr. Rina Dechter @ UCI\r\n      </title>\r\n      <LINK REL=\"Stylesheet\" HREF=\"/~dechter/basic.css\">\t\t\r\n    </HEAD>\r\n  \r\n  <BODY bgcolor=\"#ffffff\" alink=\"00aaaa\" link=\"008080\" vlink=\"008080\">\r\n  \r\n  <!-- Begin Header -->\r\n    <!--#include virtual=\"/~dechter/header.html\" -->\r\n  <!-- End Header -->\r\n  \r\n  \r\n  <!-- Begin Body -->\r\n  \r\n  <!--#include file=\"pubs-nav.html\"--> \r\n<center>\r\n<table width=\"80%\" cellspacing=\"0\" cellpadding=\"0\" border=\"0\">\r\n<tr valign=top>\r\n<td><b>R42a</td>\r\n | \r\n<br></td>\r\n</tr>\r\n\r\n<tr>\r\n<td colspan=2><div class=title>Value Iteration And Policy Iteration Algorithms For Markov Decision Problem</div>\r\n<tt>\r\nElena Pashenkova, Irina Rish (<A href=\"mailto:irinar@ics.uci.edu\">irinar@ics.uci.edu</A>) &\r\nRina Dechter (<A href=\"mailto:dechter@ics.uci.edu\">dechter@ics.uci.edu</A>)\r\n</tt></td></tr>\r\n</table>\r\n\r\n<table width=\"80%\" cellspacing=\"0\" cellpadding=\"0\" border=\"0\">\r\n<tr><td>\r\n<br><div class=abstract>\r\n<b>Abstract</b><BR>\r\nIn this paper we consider computational aspects of decision-theoretic planning\r\nmodeled by Markov decision processes (MDPs). Commonly used algorithms, such as \r\nvalue iteration (VI) [Bellman, 1957] and several versions of modified policy \r\niteration (MPI) [Puterman, 1994] (a modification of the original Howard's policy\r\niteration (PI) [Howard, 1960]), are compared on a class of problems from the motion \r\nplanning domain. Policy iteration and its modifications are usually recommended \r\nas algorithms demonstrating a better performance than value iteration\r\n[Russel & Norvig, 1995], [Puterman, 1994]. However, our results show that their\r\nperformance is not always superior and depends on the parameters of a problem\r\nand the parameters of the algorithms, such as number of iterations in the value determination \r\nprocedure in MPI. Moreover, policy iteration applied to non-discounted\r\nmodels without special restrictions might not even converge to an optimal policy, as\r\nin case of the policy iteration algorithm introduced in [Russel & Norvig, 1995]. We\r\nalso introduce a new stopping criterion into value iteration based on policy changes.\r\nThe combined value-policy iteration (CVPI) algorithm proposed in the paper implements \r\nthis criterion and generates an optimal policy faster then both policy and\r\nvalue iteration algorithms.\r\n</div><br>\r\n\r\n<A href=\"r42a-mdp_report.ps\">\r\n<img align=left border=\"0\" src=\"/~dechter/images/down.gif\">&nbsp;&nbsp;<b>[ps] </b></a>\r\n<A target=blank href=\"r42a-mdp_report.pdf\">\r\n<b>[pdf]</b></a>\r\n</td></tr></table></center><br>\r\n\r\n<!-- End Body-->\r\n\r\n<!--- Begin Footer -->\r\n     <!--#include virtual=\"/~dechter/footer.html\" -->\r\n<!--- End Footer -->\r\n\r\n</body>\r\n<html>\r\n", "id": 4787.0}