{"text": "Data Compression 4 ADAPTIVE HUFFMAN CODING Adaptive Huffman coding was first conceived independently by Faller and Gallager Faller 1973 Gallager 1978 Knuth contributed improvements to the original algorithm Knuth 1985 and the resulting algorithm is referred to as algorithm FGK A more recent version of adaptive Huffman coding is described by Vitter Vitter 1987 All of these methods are defined word schemes which determine the mapping from source messages to codewords based upon a running estimate of the source message probabilities The code is adaptive changing so as to remain optimal for the current estimates In this way the adaptive Huffman codes respond to locality In essence the encoder is learning the characteristics of the source The decoder must learn along with the encoder by continually updating the Huffman tree so as to stay in synchronization with the encoder Another advantage of these systems is that they require only one pass over the data Of course one pass methods are not very interesting if the number of bits they transmit is significantly greater than that of the two pass scheme Interestingly the performance of these methods in terms of number of bits transmitted can be better than that of static Huffman coding This does not contradict the optimality of the static method as the static method is optimal only over all methods which assume a time invariant mapping The performance of the adaptive methods can also be worse than that of the static method Upper bounds on the redundancy of these methods are presented in this section As discussed in the introduction the adaptive method of Faller Gallager and Knuth is the basis for the UNIX utility compact The performance of compact is quite good providing typical compression factors of 3 4 4 1 Algorithm FGK The basis for algorithm FGK is the Sibling Property defined by Gallager Gallager 1978 A binary code tree has the sibling property if each node except the root has a sibling and if the nodes can be listed in order of nonincreasing weight with each node adjacent to its sibling Gallager proves that a binary prefix code is a Huffman code if and only if the code tree has the sibling property In algorithm FGK both sender and receiver maintain dynamically changing Huffman code trees The leaves of the code tree represent the source messages and the weights of the leaves represent frequency counts for the messages At any point in time k of the n possible source messages have occurred in the message ensemble Figure 4 1 Algorithm FGK processing the ensemble EXAMPLE a Tree after processing aa bb 11 will be transmitted for the next b b After encoding the third b 1 1 will be transmitted for the next space the tree will not change 1 will be transmitted for the first c c Tree after update following first c Initially the code tree consists of a single leaf node called the node The node is a special node used to represent the n k unused messages For each message transmitted both parties must increment the corresponding weight and recompute the code tree to maintain the sibling property At the point in time when t messages have been transmitted k of them distinct and k n the tree is a legal Huffman code tree with k 1 leaves one for each of the k messages and one for the node If the t 1 st message is one of the k already seen the algorithm transmits a t 1 s current code increments the appropriate counter and recomputes the tree If an unused message occurs the node is split to create a pair of leaves one for a t 1 and a sibling which is the new node Again the tree is recomputed In this case the code for the node is sent in addition the receiver must be told which of the n k unused messages has appeared At each node a count of occurrences of the corresponding message is stored Nodes are numbered indicating their position in the sibling property ordering The updating of the tree can be done in a single traversal from the a t 1 node to the root This traversal must increment the count for the a t 1 node and for each of its ancestors Nodes may be exchanged to maintain the sibling property but all of these exchanges involve a node on the path from a t 1 to the root Figure 4 2 shows the final code tree formed by this process on the ensemble EXAMPLE Figure 4 2 Tree formed by algorithm FGK for ensemble EXAMPLE Disregarding overhead the number of bits transmitted by algorithm FGK for the EXAMPLE is 129 The static Huffman algorithm would transmit 117 bits in processing the same data The overhead associated with the adaptive method is actually less than that of the static algorithm In the adaptive case the only overhead is the n lg n bits needed to represent each of the n different source messages when they appear for the first time This is in fact conservative rather than transmitting a unique code for each of the n source messages the sender could transmit the message s position in the list of remaining messages and save a few bits in the average case In the static case the source messages need to be sent as does the shape of the code tree As discussed in Section 3 2 an efficient representation of the tree shape requires 2n bits Algorithm FGK compares well with static Huffman coding on this ensemble when overhead is taken into account Figure 4 3 illustrates an example on which algorithm FGK performs better than static Huffman coding even without taking overhead into account Algorithm FGK transmits 47 bits for this ensemble while the static Huffman code requires 53 Figure 4 3 Tree formed by algorithm FGK for ensemble e eae de eabe eae dcf Vitter has proved that the total number of bits transmitted by algorithm FGK for a message ensemble of length t containing n distinct messages is bounded below by S n 1 where S is the performance of the static method and bounded above by 2S t 4n 2 Vitter 1987 So the performance of algorithm FGK is never much worse than twice optimal Knuth provides a complete implementation of algorithm FGK and a proof that the time required for each encoding or decoding operation is O l where l is the current length of the codeword Knuth 1985 It should be noted that since the mapping is defined dynamically during transmission the encoding and decoding algorithms stand alone there is no additional algorithm to determine the mapping as in static methods 4 2 Algorithm V Figure 4 4 FGK tree with non level order numbering The adaptive Huffman algorithm of Vitter algorithm V incorporates two improvements over algorithm FGK First the number of interchanges in which a node is moved upward in the tree during a recomputation is limited to one This number is bounded in algorithm FGK only by l 2 where l is the length of the codeword for a t 1 when the recomputation begins Second Vitter s method minimizes the values of SUM l i and MAX l i subject to the requirement of minimizing SUM w i l i The intuitive explanation of algorithm V s advantage over algorithm FGK is as follows as in algorithm FGK the code tree constructed by algorithm V is the Huffman code tree for the prefix of the ensemble seen so far The adaptive methods do not assume that the relative frequencies of a prefix represent accurately the symbol probabilities over the entire message Therefore the fact that algorithm V guarantees a tree of minimum height height MAX l i and minimum external path length SUM l i implies that it is better suited for coding the next message of the ensemble given that any of the leaves of the tree may represent that next message These improvements are accomplished through the use of a new system for numbering nodes The numbering called an implicit numbering corresponds to a level ordering of the nodes from bottom to top and left to right Figure 4 4 illiustrates that the numbering of algorithm FGK is not always a level ordering The following invariant is maintained in Vitter s algorithm For each weight w all leaves of weight w precede in the implicit numbering all internal nodes of weight w Vitter proves that this invariant enforces the desired bound on node promotions Vitter 1987 The invariant also implements bottom merging as discussed in Section 3 2 to minimize SUM l i and MAX l i The difference between Vitter s method and algorithm FGK is in the way the tree is updated between transmissions In order to understand the revised update operation the following definition of a block of nodes is necessary Blocks are equivalence classes of nodes defined by u is equivalent to v iff weight u weight v and u and v are either both leaves or both internal nodes The leader of a block is the highest numbered in the implicit numbering node in the block Blocks are ordered by increasing weight with the convention that a leaf block always precedes an internal block of the same weight When an exchange of nodes is required to maintain the sibling property algorithm V requires that the node being promoted be moved to the position currently occupied by the highest numbered node in the target block In Figure 4 5 the Vitter tree corresponding to Figure 4 1c is shown This is the first point in EXAMPLE at which algorithm FGK and algorithm V differ significantly At this point the Vitter tree has height 3 and external path length 12 while the FGK tree has height 4 and external path length 14 Algorithm V transmits codeword 1 for the second c FGK transmits 11 1 This demonstrates the intuition given earlier that algorithm V is better suited for coding the next message The Vitter tree corresponding to Figure 4 2 representing the final tree produced in processing EXAMPLE is only different from Figure 4 2 in that the internal node of weight 5 is to the right of both leaf nodes of weight 5 Algorithm V transmits 124 bits in processing EXAMPLE as compared with the 129 bits of algorithm FGK and 117 bits of static Huffman coding It should be noted that these figures do not include overhead and as a result disadvantage the adaptive methods Figure 4 5 Algorithm V processing the ensemble aa bbb c Figure 4 6 ilustrates the tree built by Vitter s method for the ensemble of Figure 4 3 Both SUM l i and MAX l i are smaller in the tree of Figure 4 6 The number of bits transmitted during the processing of the sequence is 47 the same used by algorithm FGK However if the transmission continues with d b c f or an unused letter the cost of algorithm V will be less than that of algorithm FGK This again illustrates the benefit of minimizing the external path length SUM l i and the height MAX l i Figure 4 6 Tree formed by algorithm V for the ensemble of Fig 4 3 It should be noted again that the strategy of minimizing external path length and height is optimal under the assumption that any source letter is equally likely to occur next Other reasonable strategies include one which assumes locality To take advantage of locality the ordering of tree nodes with equal weights could be determined on the basis of recency Another reasonable assumption about adaptive coding is that the weights in the current tree correspond closely to the probabilities associated with the source This assumption becomes more reasonable as the length of the ensemble increases Under this assumption the expected cost of transmitting the next letter is SUM p i l i which is approximately SUM w i l i so that neither algorithm FGK nor algorithm V has any advantage Vitter proves that the performance of his algorithm is bounded by S n 1 from below and S t 2n 1 from above Vitter 1987 At worst then Vitter s adaptive method may transmit one more bit per codeword than the static Huffman method The improvements made by Vitter do not change the complexity of the algorithm algorithm V encodes and decodes in O l time as does algorithm FGK ", "_id": "http://www.ics.uci.edu/~dan/pubs/DC-Sec4.html", "title": " data compression -- section 4", "html": "<HTML>\n<HEAD>\n<TITLE> Data Compression -- Section 4</TITLE>\n</HEAD><BODY>\n\n<H1> Data Compression </H1>\n\n<a name=\"Sec_4\">\n<H2> 4.  ADAPTIVE HUFFMAN CODING</H2> </a>\n\n<A HREF=\"DC-Sec3.html\"><IMG SRC=\"prev.gif\" ALT=\"PREV section\"></A>\n<A HREF=\"DC-Sec5.html\"><IMG SRC=\"next.gif\" ALT=\"NEXT section\"></A>\n<P>\n\n\tAdaptive Huffman coding was first conceived independently\nby Faller and Gallager [Faller 1973; Gallager 1978].  Knuth \ncontributed improvements to the original algorithm [Knuth 1985] and the \nresulting algorithm is referred to as algorithm FGK.\nA more recent version of adaptive Huffman coding\nis described by Vitter [Vitter 1987].  All of these methods are \ndefined-word schemes which determine\nthe mapping from source messages to codewords based upon \na running estimate of the source message probabilities.  The\ncode is adaptive, changing so as to remain optimal for the\ncurrent estimates.  In this way, the adaptive Huffman codes\nrespond to locality.  In essence, the encoder is \"learning\" the\ncharacteristics of the source.  The decoder must learn along\nwith the encoder by continually updating the Huffman tree so\nas to stay in synchronization with the encoder.\n<P>\nAnother advantage of these systems is that\nthey require only one pass over\nthe data.  Of course, one-pass methods are not very interesting if\nthe number of bits they transmit is significantly greater than that\nof the two-pass scheme.  Interestingly, the performance of these methods, in terms \nof number of bits transmitted, can be better than that of static\nHuffman coding.  This does not contradict the optimality of the\nstatic method as the static method is optimal only over all methods which\nassume a time-invariant mapping.  The performance of the adaptive\nmethods can also be worse than that of the static method.  Upper \nbounds on the redundancy of these methods are presented in this section.\nAs discussed in the introduction, the adaptive method of Faller, Gallager\nand Knuth is the basis for the UNIX utility <EM>compact</EM>.  The\nperformance of <EM>compact</EM> is quite good, providing typical compression\nfactors of 30-40%.\n\n<a name=\"Sec_4.1\">\n<H3> 4.1  Algorithm FGK</H3> </a>\n\n\tThe basis for algorithm FGK is the Sibling Property, defined\nby Gallager [Gallager 1978]:  A binary code tree has the sibling\nproperty if each node (except the root) has a sibling and if the nodes\ncan be listed in order of nonincreasing weight with each node \nadjacent to its sibling.  Gallager proves that a\nbinary prefix code is a Huffman code if and only if the code tree \nhas the sibling property.  In algorithm FGK, both sender and receiver\nmaintain dynamically changing Huffman code trees.  The leaves of the\ncode tree represent the source messages and the weights of the\nleaves represent frequency counts for the messages.  At any point in\ntime, <VAR>k</VAR> of the <VAR>n</VAR> possible source messages have occurred in the\nmessage ensemble.  \n<P>\n<IMG SRC=\"DC-fig41.gif\" ALT=\"[FIGURE 4.1]\">\n<P>\nFigure 4.1 -- Algorithm FGK processing the ensemble \n<VAR>EXAMPLE</VAR> (a) Tree after processing \"<VAR>aa bb</VAR>\";\n11 will be transmitted for the next <VAR>b</VAR>.\n(b) After encoding the third <VAR>b</VAR>;\n101 will be transmitted for the next <VAR>space</VAR>;\nthe tree will not change;\n100 will be transmitted for the first <VAR>c</VAR>.\n(c) Tree after update following first <VAR>c</VAR>.\n<P>\n\tInitially, the code tree consists of a single\nleaf node, called the 0-node.  The 0-node is a special node used\nto represent the <VAR>n</VAR>-<VAR>k</VAR> unused messages.  For each message transmitted,\nboth parties must increment the corresponding weight and recompute\nthe code tree to maintain the sibling property.  At the point in\ntime when <VAR>t</VAR> messages have been transmitted, <VAR>k</VAR> of them distinct,\nand <VAR>k</VAR> &lt; <VAR>n</VAR>, the tree is a legal Huffman code tree with <VAR>k</VAR>+1 leaves,\none for each of the <VAR>k</VAR> messages and one for the 0-node.  If the \n(<VAR>t</VAR>+1)st message is one of the <VAR>k</VAR> already seen, the algorithm \ntransmits <VAR>a</VAR>(<VAR>t</VAR>+1)'s current code, increments the appropriate\ncounter and recomputes the tree.  If an unused message occurs, \nthe 0-node is split to create a pair of leaves, one for <VAR>a</VAR>(<VAR>t</VAR>+1),\nand a sibling which is the new 0-node.  Again the tree is recomputed.  \nIn this case, the code for the 0-node is sent; in addition, the\nreceiver must be told which of the <VAR>n</VAR>-<VAR>k</VAR> unused messages has\nappeared.\nAt each node a count of occurrences of the corresponding\nmessage is stored.  Nodes are numbered indicating their position in\nthe sibling property ordering.  The updating of the tree can be\ndone in a single traversal from the <VAR>a</VAR>(<VAR>t</VAR>+1) node to the root.\nThis traversal must increment the count for the <VAR>a</VAR>(<VAR>t</VAR>+1) node\nand for each of its ancestors.  Nodes may be exchanged to maintain\nthe sibling property, but all of these exchanges involve a\nnode on the path from <VAR>a</VAR>(<VAR>t</VAR>+1) to the root.\nFigure 4.2 shows the final code tree formed by this process on\nthe ensemble <VAR>EXAMPLE</VAR>.\n<P>\n<IMG SRC=\"DC-fig42.gif\" ALT=\"[FIGURE 4.2]\">\n<P>\nFigure 4.2 -- Tree formed by algorithm FGK for ensemble <VAR>EXAMPLE</VAR>.\n<P>\n\tDisregarding overhead, the number of bits transmitted by \nalgorithm FGK for the <VAR>EXAMPLE</VAR> is 129.  The \nstatic Huffman algorithm would transmit 117 bits in processing the \nsame data. The overhead\nassociated with the adaptive method is actually less than that\nof the static algorithm. In the adaptive case the only overhead\nis the <VAR>n</VAR> lg <VAR>n</VAR> bits needed to represent each of the <VAR>n</VAR> different \nsource messages when they appear for the first time.  (This is in fact\nconservative; rather than transmitting a unique code for each of the\n<VAR>n</VAR> source messages, the sender could transmit the message's position\nin the list of remaining messages and save a few bits in the average\ncase.)  In the static case, the source messages need to be\nsent as does the shape of the code tree.  As discussed in\n<a href=\"DC-Sec3.html#Sec_3.2\">Section 3.2</a>,\nan efficient representation of the tree shape requires 2<VAR>n</VAR> bits.\nAlgorithm FGK compares well with static Huffman coding on this ensemble\nwhen overhead is taken into account.\nFigure 4.3 illustrates an example on which algorithm FGK performs\nbetter than static Huffman coding even without taking overhead\ninto account. Algorithm FGK transmits 47 bits for this ensemble\nwhile the static Huffman code requires 53.\n<P>\n<IMG SRC=\"DC-fig43.gif\" ALT=\"[FIGURE 4.3]\">\n<P>\nFigure 4.3 -- Tree formed by algorithm FGK for ensemble\n\"<VAR>e eae de eabe eae dcf</VAR>\".\n<P>\nVitter has proved that\nthe total number of bits transmitted by algorithm FGK for a message\nensemble of length <VAR>t</VAR> containing <VAR>n</VAR> distinct messages is bounded below \nby <VAR>S - n</VAR> + 1, where <VAR>S</VAR> is the performance of the static method, and \nbounded above by 2<VAR>S</VAR> + <VAR>t</VAR> - 4<VAR>n</VAR> + 2 [Vitter 1987].  So the performance \nof algorithm FGK is never much worse than twice optimal.  \nKnuth provides a complete implementation of algorithm FGK \nand a proof that the time required for each encoding or decoding \noperation is <VAR>O</VAR>(<VAR>l</VAR>), where <VAR>l</VAR> is the current length of the codeword [Knuth 1985].\nIt should be noted that since the mapping is defined dynamically, during\ntransmission, the encoding and decoding algorithms stand alone; there is\nno additional algorithm to determine the mapping as in static methods.\n\n<a name=\"Sec_4.2\">\n<H3> 4.2  Algorithm V</H3> </a>\n\n<P>\n<IMG SRC=\"DC-fig44.gif\" ALT=\"[FIGURE 4.4]\">\n<P>\nFigure 4.4 -- FGK tree with non-level order numbering.\n<P>\n\tThe adaptive Huffman algorithm of Vitter (algorithm V)\nincorporates two improvements over algorithm FGK.  First, the \nnumber of interchanges in which a node is moved upward in the \ntree during a recomputation is limited to one.  This \nnumber is bounded in algorithm FGK only by l/2 where <VAR>l</VAR> is the \nlength of the codeword for <VAR>a</VAR>(<VAR>t</VAR>+1) when the recomputation begins.  \nSecond, Vitter's method minimizes the values of SUM{ <VAR>l</VAR>(<VAR>i</VAR>) } and \nMAX{<VAR>l</VAR>(<VAR>i</VAR>)} subject to the requirement of minimizing\nSUM{ <VAR>w</VAR>(<VAR>i</VAR>) <VAR>l</VAR>(<VAR>i</VAR>) }.\nThe intuitive explanation of algorithm V's advantage over algorithm\nFGK is as follows:  as in algorithm FGK, the code tree constructed\nby algorithm V is the Huffman code tree for the prefix of the \nensemble seen so far. The adaptive methods do not assume that the\nrelative frequencies of a prefix represent accurately the symbol \nprobabilities over the entire message. Therefore, the fact that algorithm V\nguarantees a tree of minimum height (height = MAX{ <VAR>l</VAR>(<VAR>i</VAR>) } \nand minimum external path length (SUM{ <VAR>l</VAR>(<VAR>i</VAR>) }) implies that it is\nbetter suited for coding the next message of the ensemble, given that\nany of the leaves of the tree may represent that next message.\n<P>\nThese improvements are accomplished through the use of a new system\nfor numbering nodes.  The numbering, called an implicit numbering,\ncorresponds to a level ordering of the nodes (from bottom to top\nand left to right).  Figure 4.4 illiustrates that the numbering of\nalgorithm FGK is not always a level ordering.  The following invariant\nis maintained in Vitter's algorithm:  For each weight <VAR>w</VAR>, all leaves\nof weight <VAR>w</VAR> precede (in the implicit numbering) all internal nodes\nof weight <VAR>w</VAR>.  Vitter proves that this invariant enforces the \ndesired bound on node promotions [Vitter 1987].  The invariant\nalso implements bottom merging, as discussed in\n<a href=\"DC-Sec3.html#Sec_3.2\">Section 3.2</a>, to\nminimize SUM{ <VAR>l</VAR>(<VAR>i</VAR>) } and MAX{ <VAR>l</VAR>(<VAR>i</VAR>) }.  The difference between Vitter's\nmethod and algorithm FGK is in the way the tree is updated between\ntransmissions.  In order to understand the revised update operation,\nthe following definition of a block of nodes is necessary:  Blocks\nare equivalence classes of nodes defined by\n<VAR>u</VAR> is equivalent to <VAR>v</VAR> iff <VAR>weight(u)</VAR> =\n<VAR>weight(v)</VAR> and <VAR>u</VAR> and <VAR>v</VAR> are either both leaves or both internal nodes.\nThe leader of a block is the highest-numbered (in the implicit numbering)\nnode in the block.  Blocks are ordered by increasing weight with the\nconvention that a leaf block always precedes an internal block of the\nsame weight.  When an exchange of nodes is required to maintain\nthe sibling property, algorithm V requires that the node being\npromoted be moved to the position currently occupied by the\nhighest-numbered node in the target block.\n<P>\nIn Figure 4.5, the Vitter tree corresponding to Figure 4.1c is   \nshown.  This is the first point in <VAR>EXAMPLE</VAR> at which \nalgorithm FGK and algorithm V differ significantly.  At this\npoint, the Vitter tree has height 3 and external path length 12 while\nthe FGK tree has height 4 and external path length 14.  Algorithm V\ntransmits codeword 001 for the second <VAR>c</VAR>; FGK transmits 1101.\nThis demonstrates the intuition given earlier that algorithm V\nis better suited for coding the next message.\nThe Vitter tree corresponding to Figure 4.2, representing the final\ntree produced in processing <VAR>EXAMPLE</VAR>, is only different from\nFigure 4.2 in that the internal node of weight 5 is to the right\nof both leaf nodes of weight 5.\nAlgorithm V transmits 124 bits in processing <VAR>EXAMPLE</VAR>, as compared\nwith the 129 bits of algorithm FGK and 117 bits of static Huffman \ncoding.  It should be noted that these figures do not include overhead\nand, as a result, disadvantage the adaptive methods.\n<P>\n<IMG SRC=\"DC-fig45.gif\" ALT=\"[FIGURE 4.5]\">\n<P>\nFigure 4.5 -- Algorithm V processing the ensemble \"<VAR>aa bbb c</VAR>\".\n<P>\nFigure 4.6 ilustrates the tree built by Vitter's method for the ensemble\nof Figure 4.3.  Both SUM{<VAR>l</VAR>(<VAR>i</VAR>)} and MAX{<VAR>l</VAR>(<VAR>i</VAR>)} are \nsmaller in the tree of Figure 4.6.  The number of bits transmitted\nduring the processing of the sequence is 47, the same used by \nalgorithm FGK.  However, if the transmission continues with <VAR>d,b,c,f</VAR> \nor an unused letter, the cost of algorithm V will be less than\nthat of algorithm FGK.  This again illustrates the benefit of\nminimizing the external path length SUM{<VAR>l</VAR>(<VAR>i</VAR>)}\nand the height MAX{<VAR>l</VAR>(<VAR>i</VAR>)}.\n<P>\n<IMG SRC=\"DC-fig46.gif\" ALT=\"[FIGURE 4.6]\">\n<P>\nFigure 4.6 -- Tree formed by algorithm V for the ensemble of Fig. 4.3.\n<P>\nIt should be noted again that the strategy of minimizing external path \nlength and height is optimal under the assumption that any source\nletter is equally likely to occur next.  Other reasonable strategies\ninclude one which assumes locality.  To take advantage of locality,\nthe ordering of tree nodes with equal weights could be determined\non the basis of recency.\n  Another reasonable assumption about adaptive coding\nis that the weights in the current tree correspond closely to the\nprobabilities associated with the source.  This assumption becomes\nmore reasonable as the length of the ensemble increases.\nUnder this assumption, the expected cost of transmitting the next letter\nis SUM{ <VAR>p</VAR>(<VAR>i</VAR>) <VAR>l</VAR>(<VAR>i</VAR>) } which is approximately\nSUM{ <VAR>w</VAR>(<VAR>i</VAR>) <VAR>l</VAR>(<VAR>i</VAR>) }, so that neither algorithm FGK nor \nalgorithm V has any advantage.\n<P>\nVitter proves that the performance of his \nalgorithm is bounded by <VAR>S - n</VAR> + 1 from below and <VAR>S + t</VAR> - 2<VAR>n</VAR> + 1 \nfrom above [Vitter 1987].  At worst then, Vitter's adaptive method\nmay transmit one more bit per codeword than the static Huffman \nmethod.  The improvements made by Vitter do not change the complexity of\nthe algorithm; algorithm V encodes and decodes in <VAR>O</VAR>(<VAR>l</VAR>) time as does\nalgorithm FGK.\n\n<P>\n<A HREF=\"DC-Sec3.html\"><IMG SRC=\"prev.gif\" ALT=\"PREV section\"></A>\n<A HREF=\"DC-Sec5.html\"><IMG SRC=\"next.gif\" ALT=\"NEXT section\"></A>\n<P>\n</BODY></HTML>\n", "id": 2096.0}