{"text": "Time series sensor observations of a Pioneer 1 mobile robot Data Type The data is time series multivariate Most variables are continuous valued a few are binary coded and 1 Two categorical variables are included to delineate the trials within the datasets Abstract This dataset contains time series sensor readings of the Pioneer 1 mobile robot The data is broken into experiences in which the robot takes action for some period of time and experiences a controlled interaction with its environment i e bumping into a garbage can Sources Original Owner and Donor Matthew D Schmill Paul R Cohen Experimental Knowledge Systems Laboratory Department of Computer Science Box 3461 University of Massachusetts Amherst Amherst MA 1 3 461 schmill cs umass edu cohen cs umass edu Date Donated January 28 1999 Data Characteristics The data were collected over a series of specifically designed trials Our hope was to cover most of the types of sensory interactions that a Pioneer might be reasonably expected to encounter things like passing by visible objects pushing visible objects crashing into walls etc Many of these interactions are repeated throughout the dataset This data was collected to serve as the basis for work in learning and conceptual development Our first goal was to be able to have the robot cluster these experiences by their dynamics on their own into clusters of experiences with a common outcome Each data file contains time series data in which each row of data corresponds to a single observation of the sensor array Included in each row are two additional variables id and description which indicate the experience number that the observation belongs to and a description of that experience respectively Observations within an experience are taken every 1 ms Variable Descriptions TRIAL ID categorical the trial id of the experience that the observation belongs to DESCRIPTION a symbolic description of the experience design TIME SECS a reading of the Pioneer s internal clock in seconds BATTERY LEVEL a reading of battery level in volts SONAR sonar depth reading in mm of the left 9 pointing sonar SONAR 1 sonar depth reading in mm of a 15 pointing sonar SONAR 2 sonar depth reading in mm of a 7 5 pointing sonar SONAR 3 sonar depth reading in mm of a forward pointing sonar SONAR 4 sonar depth reading in mm of a 7 5 pointing sonar SONAR 5 sonar depth reading in mm of a 15 pointing sonar SONAR 6 sonar depth reading in mm of a right 9 pointing sonar HEADING heading reading in degrees from the robot s true north R WHEEL VEL right wheel velocity in mm sec L WHEEL VEL left wheel velocity in mm sec TRANS VEL translational velocity mm sec ROT VEL rotational velocity mm sec R STALL right wheel stall sensor binary 1 L STALL left wheel stall sensor binary 1 ROBOT STATUS robot status 2 stationary 3 moving GRIP STATE gripper state GRIP FRONT BEAM gripper break beam binary 1 broken GRIP REAR BEAM gripper break beam binary 1 broken GRIP BUMPER gripper bumper binary 1 in contact VIS A AREA area of dominant visible object for channel A in pixels VIS A X X location of object in channel A on image plane 14 14 VIS A Y Y location of channel A on image plane VIS A H height of object in channel A on plane in pixels VIS A W width of object in A on image plane in pixels VIS A DIST distance to object in channel A in mm VIS B AREA area of dominant visible object for channel B in pixels VIS B X X location of object in channel B on image plane 14 14 VIS B Y Y location of channel B on image plane VIS B H height of object in channel B on plane in pixels VIS B W width of object in B on image plane in pixels VIS B DIST distance to object in channel B in mm VIS C AREA area of dominant visible object for channel C in pixels VIS C X X location of object in channel C on image plane 14 14 VIS C Y Y location of channel C on image plane VIS C H height of object in C on image plane in pixels VIS C W width of object in C on image plane in pixels VIS C DIST distance to object in channel C in mm For the visual variables when there is no visible object width height area distance 1 Y X 14 The sonars report 52 1 as their maximum distance Other Relevant Information 1 2 move experiences 42 turn experiences 16 gripper experiences Channel B was not used in these experiments although noise was apparent Data Format The data is stored in three text files one file for experiences in which the Pioneer was moving in a straight line one in which it was turning in place and one in which it was raising or lowering its gripper The description variable is a string of symbols The string breaks down as follows u or o unobstructed or obstructed x xs activity lasted x x seconds activity the activity and speed if applicable i e move1 move forward at 1 mm sec visual objects in the visual array are listed in sequence cAHEAD indicates an object visible to channel c directly AHEAD of the Pioneer visual X visual descriptions followed by a and one character indicate that something special happens with the visible object V means the object Vanishes from sight during the activity D indicates that the object is Discovered becomes visible during the activity P indicates that the object is pushed An example u 3 5s retr 1 aRIGHT D An unobstructed retreat move at 1 mm sec for 3 5 seconds with an object being discovered in channel A It should be noted that particularly with respect to the visual channels the description may not be 1 accurate Since the visual channels respond to colors that they are trained on visual a red visual b yellow visual c blue it was possible but infrequent for some extraneous object in the environment generated a response in visual channels that were not supposed to show activity in a particular trial Rows are seperated by carriage returns columns by commas Past Usage Oates Tim Schmill Matthew D and Cohen Paul R Identifying Qualitatively Different Experiences Experiments with a Mobile Robot Under review Schmill Matthew D Oates Tim and Cohen Paul R Learned Models for Continuous Planning To appear in Preliminary Papers of the Seventh International Workshop on Artificial Intelligence and Statistics Acknowledgements Copyright Information and Availability The work represented here was funded by DARPA contracts F4962 97 1 485 and N66 1 96 C 85 4 For research use only References and Further Information The Experimental Knowledge Systems Laboratory at the University of Massachusetts ActivMedia s Pioneer Software and Technical Support The UCI KDD Archive Information and Computer Science University of California Irvine Irvine CA 92697 3425 Last modified July 12 1999", "_id": "http://kdd.ics.uci.edu/databases/pioneer/pioneer.data.html", "title": " time series sensor observations of a pioneer-1 mobile robot.  ", "html": "<!------------------------------------------------------------------------\n<!  Time series sensor observations of a Pioneer-1 mobile robot \n<!----------------------------------------------------------------------->\n<HTML>\n<HEAD>\n<TITLE> Time series sensor observations of a Pioneer-1 mobile robot.  </TITLE>\n</HEAD>\n<BODY BGCOLOR=\"#FFFFFF\">\n\n<!------------------------------------------------------------------------\n<!  Title \n<!----------------------------------------------------------------------->\n<H1> Time series sensor observations of a Pioneer-1 mobile robot.  </H1>\n\n<!------------------------------------------------------------------------\n<!  Data Type \n<!----------------------------------------------------------------------->\n<H2>Data Type</H2>\nThe data is time series, multivariate. Most variables are continuous\nvalued, a few are binary coded 0.0 and 1.0. Two categorical variables\nare included to delineate the trials within the datasets.\n\n<!------------------------------------------------------------------------\n<!  Abstract \n<!----------------------------------------------------------------------->\n<H2>Abstract</H2>\n<p>\nThis dataset contains time series sensor readings of the Pioneer-1\nmobile robot. The data is broken into \"experiences\" in which the robot\ntakes action for some period of time and experiences a controlled\ninteraction with its environment (i.e. bumping into a garbage can).\n</p>\n\n<!------------------------------------------------------------------------\n<!  Sources\n<!----------------------------------------------------------------------->\n<H2> Sources</H2>\n<H4> Original Owner and Donor</H4>\n<PRE>\nMatthew D. Schmill, Paul R. Cohen\nExperimental Knowledge Systems Laboratory \nDepartment of Computer Science \nBox 34610 \nUniversity of Massachusetts, Amherst \nAmherst, MA 01003-4610 \n<A HREF=\"mailto:schmill@cs.umass.edu\">schmill@cs.umass.edu</A>, <A HREF=\"mailto:cohen@cs.umass.edu\">cohen@cs.umass.edu</A>\n</PRE>\n<B>Date Donated: </B>January 28, 1999\n\n<!------------------------------------------------------------------------\n<!  Data Characteristics\n<!----------------------------------------------------------------------->\n<H2> Data Characteristics</H2>\n<P>\nThe data were collected over a series of specifically designed\ntrials. Our hope was to cover most of the types of sensory\ninteractions that a Pioneer might be reasonably expected to encounter:\nthings like passing by visible objects, pushing visible objects,\ncrashing into walls, etc. Many of these interactions are repeated\nthroughout the dataset.\n</P>\n<P>\nThis data was collected to serve as the basis for work in learning and\nconceptual development. Our first goal was to be able to have the\nrobot cluster these experiences by their dynamics on their own into\nclusters of experiences with a common outcome.\n</P>\n<P>\nEach data file contains time series data in which each row of data\ncorresponds to a single observation of the sensor array. Included in\neach row are two additional variables, 'id' and 'description', which\nindicate the experience number that the observation belongs to, and a\ndescription of that experience, respectively. Observations within an\nexperience are taken every 100ms.\n</P>\n\n<H4>Variable Descriptions</H4>\n<PRE>\nTRIAL-ID\t: categorical, the trial id of the experience that the observation belongs to\nDESCRIPTION\t: a symbolic description of the experience design\nTIME-SECS\t: a reading of the Pioneer's internal clock, in seconds\nBATTERY-LEVEL\t: a reading of battery level, in volts\nSONAR-0\t\t: sonar depth reading, in mm, of the left (90) pointing sonar\nSONAR-1\t\t: sonar depth reading, in mm, of a (15) pointing sonar\nSONAR-2 \t: sonar depth reading, in mm, of a (7.5) pointing sonar\nSONAR-3 \t: sonar depth reading, in mm, of a forward (0) pointing sonar\nSONAR-4 \t: sonar depth reading, in mm, of a (-7.5) pointing sonar\nSONAR-5 \t: sonar depth reading, in mm, of a (-15) pointing sonar\nSONAR-6 \t: sonar depth reading, in mm, of a right (-90) pointing sonar\nHEADING\t\t: heading reading, in degrees, from the robot's \"true north\"\nR-WHEEL-VEL\t: right wheel velocity, in mm/sec\nL-WHEEL-VEL\t: left wheel velocity, in mm/sec\nTRANS-VEL\t: translational velocity, mm/sec\nROT-VEL\t\t: rotational velocity, mm/sec\nR-STALL\t\t: right wheel stall sensor, binary (0/1)\nL-STALL\t\t: left wheel stall sensor, binary (0/1)\nROBOT-STATUS\t: robot status, 2.0 = stationary, 3.0 = moving\nGRIP-STATE\t: gripper state\nGRIP-FRONT-BEAM : gripper break beam, binary, 1.0 = broken\nGRIP-REAR-BEAM\t: gripper break beam, binary, 1.0 = broken\nGRIP-BUMPER\t: gripper bumper, binary, 1.0 = in contact\nVIS-A-AREA\t: area of dominant visible object for channel A, in pixels\nVIS-A-X\t\t: X location of object in channel A on image plane, -140 ... 140\nVIS-A-Y\t\t: Y location of channel A on image plane\nVIS-A-H\t\t: height of object in channel A on plane, in pixels\nVIS-A-W\t\t: width of object in A on image plane, in pixels\nVIS-A-DIST\t: distance to object in channel A, in mm\nVIS-B-AREA\t: area of dominant visible object for channel B, in pixels\nVIS-B-X\t\t: X location of object in channel B on image plane, -140 ... 140\nVIS-B-Y\t\t: Y location of channel B on image plane\nVIS-B-H\t\t: height of object in channel B on plane, in pixels\nVIS-B-W\t\t: width of object in B on image plane, in pixels\nVIS-B-DIST\t: distance to object in channel B, in mm\nVIS-C-AREA\t: area of dominant visible object for channel C, in pixels\nVIS-C-X\t\t: X location of object in channel C on image plane, -140 ... 140\nVIS-C-Y\t\t: Y location of channel C on image plane\nVIS-C-H\t\t: height of object in C on image plane, in pixels\nVIS-C-W\t\t: width of object in C on image plane, in pixels\nVIS-C-DIST\t: distance to object in channel C, in mm\n</PRE>\n<P>\nFor the visual variables, when there is no visible object, width = 0,\nheight = 0, area = 0, distance = 10000.0, Y = 0, X = 140.0. The sonars\nreport 5201.0 as their maximum distance.\n</P>\n\n<!------------------------------------------------------------------------\n<!  Other Relevant Information \n<!----------------------------------------------------------------------->\n<H2> Other Relevant Information</H2>\n102 move experiences,\n42 turn experiences,\n16 gripper experiences\n\nChannel B was not used in these experiments, although noise was apparent.\n\n<!------------------------------------------------------------------------\n<!  Data Format \n<!----------------------------------------------------------------------->\n<H2>Data Format</H2>\n<P>\nThe data is stored in three text files: one file for experiences in\nwhich the Pioneer was moving in a straight line, one in which it was\nturning in place, and one in which it was raising or lowering its\ngripper.\n</P>\n<P>\nThe description variable is a string of symbols. The string breaks\ndown as follows:\n</P>\n<PRE>\n\"u\" or \"o\" -  unobstructed or obstructed\n\"x.xs\"     -  activity lasted x.x seconds\nactivity   -  the activity and speed, if applicable, i.e. move100 =\n\t      move forward at 100mm/sec\nvisual     -  objects in the visual array are listed in\n\t      sequence. \"cAHEAD\" indicates an object visible to\n\t      channel c directly AHEAD of the Pioneer.\n[visual.X] -  visual descriptions followed by a '.' and one character\n\t      indicate that something special happens with the visible\n\t      object. .V means the object Vanishes from sight during\n\t      the activity. .D indicates that the object is Discovered\n\t      (becomes visible) during the activity. .P indicates that\n\t      the object is pushed. \n\nAn example: \"u-3.5s-retr-100-aRIGHT.D\"\n\tAn unobstructed retreat (move) at -100 mm/sec for 3.5 seconds\n\twith an object being discovered in channel A.\n</PRE>\n<P>\nIt should be noted that, particularly with respect to the visual\nchannels, the description may not be 100% accurate. Since the visual\nchannels respond to colors that they are trained on (visual a=red,\nvisual b=yellow, visual c=blue), it was possible, but infrequent, for\nsome extraneous object in the environment generated a response in\nvisual channels that were not supposed to show activity in a\nparticular trial.\n</P>\n<P>\nRows are seperated by carriage returns, columns by commas.\n</P>\n\n<!------------------------------------------------------------------------\n<!  Past Usage \n<!----------------------------------------------------------------------->\n<H2>Past Usage</H2>\n\n<p>\nOates, Tim; Schmill, Matthew D. and Cohen, Paul R. Identifying\nQualitatively Different Experiences: Experiments with a Mobile\nRobot. Under review.\n</p>\n\n<p>\nSchmill, Matthew D.; Oates, Tim; and Cohen, Paul R. Learned Models for\nContinuous Planning. To appear in Preliminary Papers of the Seventh\nInternational Workshop on Artificial Intelligence and Statistics.\n</p>\n\n<!------------------------------------------------------------------------\n<!  Acknowledgements\n<!----------------------------------------------------------------------->\n<H2> Acknowledgements, Copyright Information, and Availability</H2>\n<P>\nThe work represented here was funded by DARPA contracts\nF49620-97-1-0485 and N66001-96-C-8504.\n</P>\n<P>\nFor research use only.\n</P>\n\n<!------------------------------------------------------------------------\n<!  References \n<!----------------------------------------------------------------------->\n<H2>References and Further Information</H2>\n<P>\n<A HREF=\"http://www-eksl.cs.umass.edu/\">\nThe Experimental Knowledge Systems Laboratory</A> at the University of Massachusetts.\n</P>\n\n<P>\n<A HREF=\"http://robots.activmedia.com/\">ActivMedia's Pioneer Software and Technical Support</A>.\n</P>\n\n\n<!------------------------------------------------------------------------\n<!  Signature \n<!----------------------------------------------------------------------->\n<p>\n<hr>\n<ADDRESS>\n<A href=\"http://kdd.ics.uci.edu/\">The UCI KDD Archive</A><br>\n<a href=\"http://www.ics.uci.edu/\">Information and Computer Science</a><br>\n<a href=\"http://www.uci.edu/\">University of California, Irvine</a><br>\nIrvine, CA 92697-3425 <br>\n</ADDRESS> \nLast modified: July 12, 1999 </BODY>\n</HTML>\n", "id": 31244.0}