{"text": "Michael J Pazzani Research Group Graduate Students Ph D Graduates Stephen Bay Daniel Billsus Eamonn Keogh James Wogulis An Approach to Repairing and Evaluating First Order Theories Containing Multiple Concepts and Negation 1 2M This dissertation addresses the problem of theory revision in machine learning The task requires the learner to minimally revise an initial incorrect theory such that the revised theory explains a given set of training data A learning system A3 is presented that solves this task The main contributions of this dissertation include the learning system A3 that can revise theories containing multiple concepts expressed as function free first order Horn clauses an approach to repairing theories containing negation and the introduction of a distance metric between theories to evaluate the degree of revision performed Experimental evidence is presented that demonstrates A3 s ability to solve the theory revision task Assumptions commonly made by other approaches to theory revision such as whether a theory needs to be generalized or specialized with respect to misclassified examples are shown to be incorrect for theories containing negation A3 is able to repair theories containing negation and demonstrates a simple general approach to identifying types of errors in a theory using a single mechanism for handling positive and negative examples as well as examples of multiple concepts The syntactic distance between two theories is proposed as an evaluation metric for theory revision systems This distance is defined in terms of the minimum number of edit operations required to transform one theory into another This allows for a precise measurement of how much a theory has been revised and allows for comparison of different systems abilities to perform minimal revisions This distance metric is also used by A3 in order to bias it towards finding minimal revisions that accurately explain the data The distance metric also leads to insights about the theory revision task In particular it is shown that the theory revision task is underconstrained if the additional goal of learning a particular correct theory is to be met Without additional constraints there are potentially many accurate revisions that are far apart syntactically It is shown that providing examples of multiple concepts in the theory can provide some of these constraints David Schulenburg Learning and Using Context in a connectionist Model of Language Understanding 2 5M Natural languages are ambiguous This is especially true for non literal figurative constructs such as metaphors and indirect speech acts Even literal text suffers from problems of ambiguity as exemplified by text containing words having multiple meanings Understanding such ambiguous text is a fairly simple task for us humans it is well recognized that context is often used by people to aid in the resolution of these problems of ambiguity This dissertation presents a discussion of a computational model PIP which was designed to address the issue of ambiguity in natural language understanding By incorporating a textual context during the understanding process PIP is able to disambiguate text containing ambiguous constructs such as metaphors and indirect speech acts Furthermore the entire understanding process is uniform in the sense that the exact same mechanisms are used to process both literal and non literal ambiguous text no special processing rules are necessary to deal with the ambiguity The underlying computational model of PIP is the feed forward artificial neural network The incorporation of the textual context is accomplished by a recurrent relation between the context that is being constructed and the network input for processing the words of the text In this fashion is PIP able to use the context directly as it processes text PIP has demonstrated its effectiveness on sets of text which include ambiguous lexemes metaphors and indirect speech acts By using the context constructed from earlier sentences in these texts PIP is able to derive the intended meaning of the ambiguous sentences at the end of the texts It is shown that without use of the context PIP is unable to produce the intended meaning and in many cases cannot decide on any meaning to give to the ambiguous sentences Kamal Ali Learning Probabilistic Relational Concept Descriptions 1 6M This dissertation presents results in the area of multiple models multiple classifiers learning probabilistic relational first order rules from noisy real world data and reducing the small disjuncts problem the problem whereby learned rules that cover few training examples have high error rates on test data Several results are also presented in the arena of multiple models The multiple models approach in relevant to the problem of making accurate classifications in real world domains since it facilitates evidence combination which is needed to accurately learn on such domains It is also useful when learning from small training data samples in which many models appear to be equally good w r t the given evaluation metric Such models often have quite varying error rates on test data so in such situations the single model method has problems Increasing search only partly addresses this problem whereas the multiple models approach has the potential to be much more useful The most important result of the multiple models research is that the amount of error reduction afforded by the multiple models approach is linearly correlated with the degree to which the individual models make errors in an uncorrelated manner This work is the first to model the degree of error reduction due to the use of multiple models It is also shown that it is possible to learn models that make less correlated errors in domains in which there are many ties in the search evaluation metric during learning The third major result of the research on multiple models is the realization that models should be learned that make errors in a negatively correlated manner rather than those that make errors in an uncorrelated statistically independent manner The thesis also presents results on learning probabilistic first order rules from relational data It is shown that learning a class description for each class in the data the one per class approach and attaching probabilistic estimates to the learned rules allows accurate classifications to be made on real world data sets The thesis presents the system HYDRA which implements this approach It is shown that the resulting classifications are often more accurate than those made by three existing methods for learning from noisy relational data Furthermore the learned rules are relational and so are more expressive than the attribute value rules learned by most induction systems Finally results are presented on the small disjuncts problem in which rules that apply to rare subclasses have high error rates The thesis presents the first approach that is simultaneously successful at reducing the error rates of small disjucnts while also reducing the overall error rate by a statistically significant margin The previous approach which aimed to reduce small disjunct error rates only did so at the expense of increasing the error rates of large disjuncts It is shown that the one per class approach reduces error rates for such rare rules while not sacrificing the error rates of the other rules Cliff Brunk An Investigation of Knowledge Intensive Approaches to Concept Learning and Theory Refinement 1 6M Concept learning algorithms have been used to solve difficult problems in fields ranging from medical diagnosis to astronomy In spite of their successful application most concept learners are only able to utilize knowledge that is expressed in the form of a set of training examples Relevant knowledge from other sources can not be utilized even when it is available Evidence is presented that using knowledge from other sources leads to more accurate learned models This dissertation is an investigation of techniques for utilizing knowledge in the form of an approximate theory to facilitate concept learning The techniques explored are divided into two classes theory guided learning algorithms which use the approximate theory to constrain the search for a new concept description and theory revision algorithms which attempt to repair the approximate theory While both techniques are more accurate than approaches that ignore the information contained in the approximate theory experimental evidence indicates that theory revision algorithms produce more accurate models Furthermore these models are structurally more similar to both the approximate theory and the ideal theory than those produced by theory guided learning algorithms The main contributions of this dissertation include a new approach to theory guided learning a conceptual framework for comparing and evaluating theory revision algorithms enhanced techniques for identifying and repairing errors within a theory and a lexically enhanced approach to evaluating repairs Lexically enhanced theory revision is a novel technique for utilizing a previously unused form of knowledge contained in the approximate theory The technique uses lexical information contained in the term names of the approximate theory to prefer repairs that are lexically more coherent Evidence indicates that this further reduces the structural difference between the revised theory and the ideal theory Christopher Merz Classification and Regression by Combining Models Two novel methods for combining predictors are introduced in this thesis one for the task of regression and the other for the task of classification The goal of combining the predictions of a set of models is to form an improved predictor This dissertation demonstrates how a combining scheme can rely on the stability of the consensus opinion and at the same time capitalize on the unique contributions of each model An empirical evaluation reveals that the new methods consistently perform as well or better than existing combining schemes for a variety of prediction problems The success of these algorithms is explained empirically and analytically by demonstrating how they adhere to a set of theoretical and heuristic guidelines A byproduct of the empirical investigation is the evidence that existing combining methods fail to satisfy one or more of the guidelines defined The new combining approaches satisfy these criteria by relying upon Singular Value Decomposition as a tool for filtering out the redundancy and noise in the predictions of the learn models and for characterizing the areas of the example space where each model is superior The SVD based representation used in the new combining methods aids in avoiding sensitivity to correlated predictions without discarding any learned models Therefore the unique contributions of each model can still be discovered and exploited An added advantage of the combining algorithms derived in this dissertation is that they are not limited to models generated by a single algorithm they may be applied to model sets generated by a diverse collection of machine learning and statistical modeling methods The three main contributions of this dissertation are The introduction of two new combining methods capable of robustly combining classification and regression estimates and applicable to a broad range of model sets An in depth analysis revealing how the new methods address the specific problems encountered in combining multiple learned models A detailed account of existing combining methods and an assessment of where they fall short in the criteria for combining approaches Michael J Pazzani Department of Information and Computer Science University of California Irvine Irvine CA 92697 3425 pazzani ics uci edu", "_id": "http://www.ics.uci.edu/~pazzani/Students.html", "title": " michael j. pazzani: students and affiliated researchers ", "html": "<HTML>\n<head>\n<title> Michael J. Pazzani: Students and Affiliated Researchers </title>\n</head>\n\n<body BGCOLOR=\"#FFFFFF\">\n<h1>  Michael J. Pazzani: Research Group </h1>\n\n\n<h2>Graduate Students</h2>\n<ul>\n\n\n</ul>\n\n<h2>Ph.D. Graduates</h2>\n<li><a href=http://www.ics.uci.edu/~sbay>Stephen Bay</a>\n<Li><a href =http://www.ics.uci.edu/~dbillsus>Daniel Billsus</a>\n<li><a href=http://www.ics.uci.edu/~eamonn>Eamonn Keogh</a>\n\n<ul> \n<li><a href =\"http://www.ics.uci.edu/~wogulis\"><b>James Wogulis</b></a> <a\nhref=\"wogulis.ps\">  An Approach to Repairing and Evaluating\nFirst-Order Theories Containing Multiple Concepts and Negation.</a> (1.2M)\nThis dissertation addresses the problem of theory revision in machine\nlearning.  The task requires the learner to minimally revise an\ninitial incorrect theory such that the revised theory explains a given\nset of training data. A learning system, A3, is presented that solves\nthis task.  The main contributions of this dissertation include the\nlearning system A3 that can revise theories containing multiple\nconcepts expressed as function-free first-order Horn clauses, an\napproach to repairing theories containing negation, and the\nintroduction of a distance metric between theories to evaluate the\ndegree of revision performed.  Experimental evidence is presented that\ndemonstrates A3's ability to solve the theory revision task.\nAssumptions commonly made by other approaches to theory revision such\nas whether a theory needs to be generalized or specialized with\nrespect to misclassified examples are shown to be incorrect for\ntheories containing negation.  A3 is able to repair theories\ncontaining negation and demonstrates a simple, general approach to\nidentifying types of errors in a theory using a single mechanism for\nhandling positive and negative examples as well as examples of\nmultiple concepts.  The syntactic distance between two theories is\nproposed as an evaluation metric for theory revision systems. This\ndistance is defined in terms of the minimum number of edit operations\nrequired to transform one theory into another. This allows for a\nprecise measurement of how much a theory has been revised and allows\nfor comparison of different systems' abilities to perform minimal\nrevisions.  This distance metric is also used by A3 in order to bias\nit towards finding minimal revisions that accurately explain the data.\nThe distance metric also leads to insights about the theory revision\ntask. In particular, it is shown that the theory revision task is\nunderconstrained if the additional goal of learning a particular\ncorrect theory is to be met. Without additional constraints, there are\npotentially many accurate revisions that are far apart syntactically.\nIt is shown that providing examples of multiple concepts in the theory\ncan provide some of these constraints.\n\n<p><li> <a href= \"http://www.ics.uci.edu/~schulenb\"><b>David\nSchulenburg</b></a> <a href=\"schulenb.ps\">Learning and Using Context\nin a connectionist Model of Language Understanding</a> (2.5M) Natural\nlanguages are ambiguous.  This is especially true for non-literal\nfigurative constructs such as metaphors and indirect speech acts.\nEven literal text suffers from problems of ambiguity as exemplified by\ntext containing words having multiple meanings.  Understanding such\nambiguous text is a fairly simple task for us humans; it is well\nrecognized that context is often used by people to aid in the\nresolution of these problems of ambiguity.  This dissertation presents\na discussion of a computational model, PIP, which was designed to\naddress the issue of ambiguity in natural language understanding.  By\nincorporating a textual context during the understanding process, PIP\nis able to disambiguate text containing ambiguous constructs such as\nmetaphors and indirect speech acts.  Furthermore, the entire\nunderstanding process is uniform in the sense that the exact same\nmechanisms are used to process both literal and non-literal\n(ambiguous) text; no special processing ``rules'' are necessary to\ndeal with the ambiguity.  The underlying computational model of PIP is\nthe feed-forward artificial neural network.  The incorporation of the\ntextual context is accomplished by a recurrent relation between the\ncontext that is being constructed and the network input for processing\nthe words of the text.  In this fashion is PIP able to use the context\ndirectly as it processes text.  PIP has demonstrated its effectiveness\non sets of text which include ambiguous lexemes, metaphors, and\nindirect speech acts.  By using the context constructed from earlier\nsentences in these texts, PIP is able to derive the intended meaning\nof the ambiguous sentences at the end of the texts.  It is shown that\nwithout use of the context, PIP is unable to produce the intended\nmeaning and in many cases, cannot decide on any meaning to give to the\nambiguous sentences.\n\n<p><li> <a href= \"http://www.isle.org/~ali\"><b> Kamal Ali</b></a> <a href=\"ali.ps\">Learning Probabilistic Relational Concept Descriptions</a> (1.6M) \nThis dissertation presents results in the area of multiple models\n(multiple classifiers), learning probabilistic relational (first order)\nrules from noisy, \"real-world\" data and reducing  the small disjuncts\nproblem - the problem whereby learned rules that cover few training examples\nhave high error rates on test data.\n<p>\nSeveral results are also presented in the arena of multiple models.  The\nmultiple models approach in relevant to the problem of making accurate\nclassifications in ``real-world'' domains since it facilitates evidence\ncombination which is needed to accurately learn on such domains.\nIt is also useful when learning from small training data samples in which \nmany models appear to be equally \"good\" w.r.t. the given evaluation metric.\nSuch models often have quite varying error rates on test data so in such\nsituations, the single model method has problems. Increasing search only\npartly addresses this problem whereas the multiple models approach has the\npotential to be much more useful.\n\nThe most important result of the multiple models research is that the\n*amount* of error reduction afforded by the multiple models approach is\nlinearly correlated with the degree to which the individual models make\nerrors in an uncorrelated manner. This work is the first to model the degree\nof error reduction due to the use of multiple models.  It is also shown that\nit is possible to learn models that make less correlated errors in domains\nin which there are many ties in the search evaluation metric during\nlearning.  The third major result of the research \non multiple models is the realization that models should be learned that\nmake errors in a negatively-correlated manner rather than those that make\nerrors in an uncorrelated (statistically independent) manner.\n\nThe thesis also presents results on learning probabilistic first-order rules\nfrom relational data.  It is shown that learning a class description for\neach class in the data - the one-per-class approach - and attaching\nprobabilistic estimates to the learned rules allows accurate classifications\nto be made on real-world data sets.  The thesis presents the system HYDRA\nwhich implements this approach.  It is shown that the resulting\nclassifications are often more accurate than those made by three existing\nmethods for learning from noisy, relational data.  Furthermore, the learned\nrules are relational and so are more expressive than the attribute-value\nrules learned by most induction systems.\n<p>\nFinally, results are presented on the small-disjuncts problem in which rules\nthat apply to rare subclasses have high error rates\nThe thesis presents the first approach that is simultaneously successful\nat reducing the error rates of small disjucnts while also reducing the\noverall error rate by a statistically significant margin. The previous\napproach which aimed to reduce small disjunct error rates only did so at the\nexpense of increasing the error rates of large disjuncts.\nIt is shown that the one-per-class approach reduces error rates for such\nrare rules while not sacrificing the error rates of the other rules.\n\n<p><li> <a href= \"http://www.ics.uci.edu/~brunk\"><b>Cliff Brunk</b></a>\n<a href=\"brunk.ps\">An Investigation of Knowledge Intensive Approaches to \nConcept Learning and Theory Refinement</a> (1.6M) Concept learning algorithms have been used to solve difficult problems\nin fields ranging from medical diagnosis to astronomy.  In spite of\ntheir successful application, most concept learners are only able to\nutilize knowledge that is expressed in the form of a set of training\nexamples.  Relevant knowledge from other sources can not be utilized\neven when it is available.  Evidence is presented that using knowledge\nfrom other sources leads to more accurate learned models.\nThis dissertation is an investigation of techniques for utilizing\nknowledge in the form of an approximate theory to facilitate concept\nlearning.  The techniques explored are divided into two classes:\ntheory-guided learning algorithms which use the approximate theory to\nconstrain the search for a new concept description, and theory\nrevision algorithms which attempt to repair the approximate theory.\nWhile both techniques are more accurate than approaches that ignore\nthe information contained in the approximate theory, experimental\nevidence indicates that theory revision algorithms produce more\naccurate models.  Furthermore, these models are structurally more\nsimilar to both the approximate theory and the \"ideal\" theory, than\nthose produced by theory-guided learning algorithms.\nThe main contributions of this dissertation include: a new approach to\ntheory-guided learning, a conceptual framework for comparing and\nevaluating theory revision algorithms, enhanced techniques for\nidentifying and repairing errors within a theory, and a lexically\nenhanced approach to evaluating repairs. Lexically enhanced theory\nrevision is a novel technique for utilizing a previously unused form\nof knowledge contained in the approximate theory.  The technique uses\nlexical information contained in the term names of the approximate\ntheory to prefer repairs that are lexically more coherent.  Evidence\nindicates that this further reduces the structural difference between\nthe revised theory and the \"ideal\" theory.\n\n<p><li> <a href= \"http://www.ics.uci.edu/~cmerz\"><b>Christopher Merz</b></a>\n<a href=\"merz.ps\">Classification and Regression by Combining Models</a> \nTwo novel methods for combining predictors are introduced in this \nthesis; one for the task of regression, and the other for the task of \nclassification.  The goal of combining the predictions of a set of \nmodels is to form an improved predictor.  This dissertation \ndemonstrates how a combining scheme can rely on the stability of the \nconsensus opinion and, at the same time, capitalize on the unique \ncontributions of each model.\n<p>\nAn empirical evaluation reveals that the new methods \nconsistently perform as well or better than existing combining schemes \nfor a variety of prediction problems.  The success of these algorithms is \nexplained empirically and analytically by demonstrating how they \nadhere to a set of theoretical and heuristic guidelines.\n<p>\nA byproduct of the empirical investigation is the evidence that \nexisting combining methods fail to satisfy one or more of the \nguidelines defined.  The new combining approaches satisfy these \ncriteria by relying upon Singular Value Decomposition as a tool for \nfiltering out the redundancy and noise in the predictions of the learn \nmodels, and for characterizing the areas of the example space where \neach model is superior.  The SVD-based representation used in the new \ncombining methods aids in avoiding sensitivity to correlated \npredictions without discarding any learned models.  Therefore, the \nunique contributions of each model can still be discovered and \nexploited.  An added advantage of the combining algorithms derived in \nthis dissertation is that they are not limited to models generated by \na single algorithm; they may be applied to model sets generated by a \ndiverse collection of machine learning and statistical \nmodeling methods.\n<p>\nThe three main contributions of this dissertation are:\n<ol><li>The introduction of two new combining methods capable of \nrobustly combining classification and regression estimates, and \napplicable to a broad range of model sets.\n<li> An in-depth analysis revealing how the new methods address the \nspecific problems encountered in combining multiple learned models.\n<li> A detailed account of existing combining methods and an \nassessment of where they fall short in the criteria for combining \napproaches.\n</ol>\n\n</ul>\n\n\n\n<HR>\n\n<P>\n\n<ADDRESS>\n<A HREF=\"http://www.ics.uci.edu/~pazzani\">Michael J. Pazzani</A><br>\n<A HREF=\"http://www.ics.uci.edu/\">Department of Information and Computer Science,</A><br>\n<A HREF=\"http://www.uci.edu/\">University of California, Irvine</A><br>\nIrvine, CA 92697-3425 <br>\n<A href=\"mailto:pazzani@ics.uci.edu\">pazzani@ics.uci.edu </A>\n</ADDRESS>\n</BODY></HTML>\n", "id": 31279.0}