{"text": "Machine Learning Fall 2 6 ICS 273A Instructor Max Welling Prerequisites ICS 27 A Intro AI or with consent of instructor Goals The goal of this class is to familiarize you with various stat of the art machine learning techniques for classification regression clustering and dimensionality reduction Besides this an important aspect this class is to provide a modern statistical view of machine learning Through projects you will learn to do independent research on some real world datasets Homework Please see green boxes on the slides Projects Submit a one page description with detailed info about what you like to do or default to one of these CRAWDAD data Netflix site Syllabus 1 introduction overview examples goals algorithm evaluation statistics kNN logistic regression slides lec1 slides lec2 2 classification I decision trees random forests bagging boosting slides lec3 4 3 clustering dimensionality reduction k means expectation maximization PCA slides lec5 6 4 neural networks perceptron multi layer networks back propagation slides lec7 8 5 reinforcement learning MDPs TD and Q learning value iteration slides lec9 1 6 Bayesian methods conditional independence generative models naive Bayes classifier slides lec11 12 7 classification II kernel methods support vector machines slides lec13 14 required reading on SVM classnotes SVM Additional background reading classnotes convex optimization In the last week we will do do class presentations of your projects Please prepare 1 mins talks Final ansers Syllabus The course will primarily be lecture based with homework and exams Most homework will revolve around the implementation of various classification algorithms on the SciTech dataset provided above It is required that you use MATLAB for this coding work Grading Criteria Grading will be based on a combination of weekly homework 1 projects 35 some midterm 2 and a final exam 35 Textbook The textbook that will be used for this course is 1 Tom Mitchell Machine Learning http www cs cmu edu tom mlbook html Optional side readings are 2 D MacKay Information Theory Inference and Learning Algorithms 3 R O Duda P E Hart D Stork Pattern Classification 4 C M Bishop Neural Networks for Pattern Recognition 5 T Hastie R Tibshirani J H Friedman The Elements of Statistical Learning 6 B D Ripley Pattern Recognition and Neural Networks", "_id": "http://www.ics.uci.edu/~welling/teaching/ICS273AFall06/ICS273AFall06.html", "title": "untitled document", "html": "<html xmlns:v=\"urn:schemas-microsoft-com:vml\"\nxmlns:o=\"urn:schemas-microsoft-com:office:office\"\nxmlns:w=\"urn:schemas-microsoft-com:office:word\"\nxmlns:st1=\"urn:schemas-microsoft-com:office:smarttags\"\nxmlns=\"http://www.w3.org/TR/REC-html40\">\n\n<head>\n<meta http-equiv=Content-Type content=\"text/html; charset=us-ascii\">\n<meta name=ProgId content=Word.Document>\n<meta name=Generator content=\"Microsoft Word 11\">\n<meta name=Originator content=\"Microsoft Word 11\">\n<link rel=File-List href=\"ICS273AFall06_files/filelist.xml\">\n<link rel=Edit-Time-Data href=\"ICS273AFall06_files/editdata.mso\">\n<!--[if !mso]>\n<style>\nv\\:* {behavior:url(#default#VML);}\no\\:* {behavior:url(#default#VML);}\nw\\:* {behavior:url(#default#VML);}\n.shape {behavior:url(#default#VML);}\n</style>\n<![endif]-->\n<title>Untitled Document</title>\n<o:SmartTagType namespaceuri=\"urn:schemas-microsoft-com:office:smarttags\"\n name=\"place\"/>\n<o:SmartTagType namespaceuri=\"urn:schemas-microsoft-com:office:smarttags\"\n name=\"City\"/>\n<!--[if gte mso 9]><xml>\n <o:DocumentProperties>\n  <o:Author>Welling</o:Author>\n  <o:Template>Normal</o:Template>\n  <o:LastAuthor>Welling</o:LastAuthor>\n  <o:Revision>9</o:Revision>\n  <o:TotalTime>131</o:TotalTime>\n  <o:Created>2006-07-03T18:58:00Z</o:Created>\n  <o:LastSaved>2006-08-30T21:40:00Z</o:LastSaved>\n  <o:Pages>1</o:Pages>\n  <o:Words>358</o:Words>\n  <o:Characters>2044</o:Characters>\n  <o:Company> UCI</o:Company>\n  <o:Lines>17</o:Lines>\n  <o:Paragraphs>4</o:Paragraphs>\n  <o:CharactersWithSpaces>2398</o:CharactersWithSpaces>\n  <o:Version>11.5606</o:Version>\n </o:DocumentProperties>\n</xml><![endif]--><!--[if gte mso 9]><xml>\n <w:WordDocument>\n  <w:Zoom>125</w:Zoom>\n  <w:SpellingState>Clean</w:SpellingState>\n  <w:GrammarState>Clean</w:GrammarState>\n  <w:ValidateAgainstSchemas/>\n  <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid>\n  <w:IgnoreMixedContent>false</w:IgnoreMixedContent>\n  <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText>\n  <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel>\n </w:WordDocument>\n</xml><![endif]--><!--[if gte mso 9]><xml>\n <w:LatentStyles DefLockedState=\"false\" LatentStyleCount=\"156\">\n </w:LatentStyles>\n</xml><![endif]--><!--[if !mso]><object\n classid=\"clsid:38481807-CA0E-42D2-BF39-B33AF135CC4D\" id=ieooui></object>\n<style>\nst1\\:*{behavior:url(#ieooui) }\n</style>\n<![endif]-->\n<style>\n<!--\n /* Font Definitions */\n @font-face\n\t{font-family:Tahoma;\n\tpanose-1:2 11 6 4 3 5 4 4 2 4;\n\tmso-font-charset:0;\n\tmso-generic-font-family:swiss;\n\tmso-font-pitch:variable;\n\tmso-font-signature:1627421319 -2147483648 8 0 66047 0;}\n /* Style Definitions */\n p.MsoNormal, li.MsoNormal, div.MsoNormal\n\t{mso-style-parent:\"\";\n\tmargin:0in;\n\tmargin-bottom:.0001pt;\n\tmso-pagination:widow-orphan;\n\tfont-size:12.0pt;\n\tfont-family:\"Times New Roman\";\n\tmso-fareast-font-family:\"Times New Roman\";}\nh2\n\t{mso-margin-top-alt:auto;\n\tmargin-right:0in;\n\tmso-margin-bottom-alt:auto;\n\tmargin-left:0in;\n\tmso-pagination:widow-orphan;\n\tmso-outline-level:2;\n\tfont-size:18.0pt;\n\tfont-family:\"Times New Roman\";}\na:link, span.MsoHyperlink\n\t{color:blue;\n\ttext-decoration:underline;\n\ttext-underline:single;}\na:visited, span.MsoHyperlinkFollowed\n\t{color:blue;\n\ttext-decoration:underline;\n\ttext-underline:single;}\nspan.SpellE\n\t{mso-style-name:\"\";\n\tmso-spl-e:yes;}\nspan.GramE\n\t{mso-style-name:\"\";\n\tmso-gram-e:yes;}\n@page Section1\n\t{size:8.5in 11.0in;\n\tmargin:1.0in 1.25in 1.0in 1.25in;\n\tmso-header-margin:.5in;\n\tmso-footer-margin:.5in;\n\tmso-paper-source:0;}\ndiv.Section1\n\t{page:Section1;}\n-->\n</style>\n<!--[if gte mso 10]>\n<style>\n /* Style Definitions */\n table.MsoNormalTable\n\t{mso-style-name:\"Table Normal\";\n\tmso-tstyle-rowband-size:0;\n\tmso-tstyle-colband-size:0;\n\tmso-style-noshow:yes;\n\tmso-style-parent:\"\";\n\tmso-padding-alt:0in 5.4pt 0in 5.4pt;\n\tmso-para-margin:0in;\n\tmso-para-margin-bottom:.0001pt;\n\tmso-pagination:widow-orphan;\n\tfont-size:10.0pt;\n\tfont-family:\"Times New Roman\";\n\tmso-ansi-language:#0400;\n\tmso-fareast-language:#0400;\n\tmso-bidi-language:#0400;}\n</style>\n<![endif]--><!--[if gte mso 9]><xml>\n <o:shapedefaults v:ext=\"edit\" spidmax=\"12290\"/>\n</xml><![endif]--><!--[if gte mso 9]><xml>\n <o:shapelayout v:ext=\"edit\">\n  <o:idmap v:ext=\"edit\" data=\"1\"/>\n </o:shapelayout></xml><![endif]-->\n</head>\n\n<body bgcolor=\"#CCCCCC\" background=\"../../background.gif\" lang=EN-US link=blue\nvlink=blue style='tab-interval:.5in'>\n\n<div class=Section1>\n\n<h2><span style='font-size:10.0pt;font-family:Tahoma;color:red'>Machine\nLearning &#8211; <span class=GramE>Fall</span> 2006</span></h2>\n\n<h2><span style='font-size:10.0pt'>ICS: 273A<br>\nInstructor: Max Welling</span><span style='color:black'>&nbsp;</span></h2>\n\n<h2><span style='font-size:10.0pt'>\n\n<hr size=2 width=\"100%\" align=left>\n\n</span></h2>\n\n<h2><span style='font-size:10.0pt;font-family:Arial;color:red'>Prerequisites</span></h2>\n\n<h2><span style='font-size:10.0pt'>ICS 270A Intro AI, or with consent of\ninstructor.</span></h2>\n\n<h2><span style='font-size:10.0pt'>\n\n<hr size=2 width=\"100%\" align=left>\n\n</span></h2>\n\n<h2><span style='font-size:10.0pt;font-family:Arial;color:red'>Goals:</span></h2>\n\n<h2><span style='font-size:10.0pt;color:black'>The goal of this class is to\nfamiliarize you with various stat-of-the-art machine learning techniques for<br>\nclassification, regression, <span class=GramE>clustering</span> and\ndimensionality reduction. Besides this, an important aspect<br>\n\t\t\t\t\t\nthis class is to provide a modern statistical view of machine learning. </span></h2>\n\n<h2><span style='font-size:10.0pt;color:black'>Through projects you will learn\nto do independent research on some real world datasets.</span></h2>\n\n<h2><span style='font-size:10.0pt;font-family:Arial;color:black'>\n\n<hr size=2 width=\"100%\" align=left>\n\n</span></h2>\n\n<h2><span class=GramE><span style='font-size:10.0pt;font-family:Arial;\ncolor:red'>Homework :</span></span><span style='font-size:10.0pt;font-family:\nArial;color:red'>  <font color=\"black\">Please see green boxes on the slides.</font></span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt;font-family: Arial;color:red\"><font color=\"red\">Projects:  </font><font color=\"black\">Submit a one page description with detailed info about what you like to do,<br>\n\t\t\t\t\t\t                 or default to</font><font color=\"red\"> <a href=\"projects.pdf\">one of these</a>.<br>\n\t\t\t\t\t</font></span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt;font-family: Arial;color:red\"><font color=\"black\"><a href=\"http://www.ics.uci.edu/%7erex/ics273a/\">CRAWDAD data</a></font></span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt;font-family: Arial;color:red\"><font color=\"black\"><a href=\"http://www.netflixprize.com/index\">Netflix site</a></font></span></h2>\n\t\t\t<h2><span style='font-size:10.0pt;font-family:Arial;color:black'>\n\n<hr size=2 width=\"100%\" align=left>\n\n</span></h2>\n\t\t\t<h2><span style='font-size:10.0pt;font-family:Arial;color:red'>Syllabus:</span></h2>\n\n<h2><span class=GramE><span style=\"font-size:10.0pt;color:blue\">1</span></span><span\nstyle='font-size:10.0pt;color:blue'>:</span><span style='font-size:10.0pt;\ncolor:red'> </span><span style='font-size:10.0pt'>introduction: overview, examples, goals, algorithm evaluation, statistics, kNN,  logistic regression. [<a href=\"Intro273AFall06.ppt\"> slides lec1</a>] [<a href=\"Evaluation273AFall06.ppt\">slides lec2</a>]</span></h2>\n\n<h2><span class=GramE><span style=\"font-size:10.0pt;color:blue\">2</span></span><span style=\"font-size:10.0pt;color:blue\">:</span><span style='font-size:10.0pt;\ncolor:red'> </span><span style='font-size:10.0pt'>classification I: decision trees, random forests, bagging, boosting,. [<a href=\"ClassificationI273AFall06.ppt\">slides lec3,4</a>]</span></h2>\n\t\t\t<h2><span class=GramE><span style=\"font-size:10.0pt;color:blue\">3</span></span><span style=\"font-size:10.0pt;color:blue\">: </span><span style='font-size:10.0pt'>clustering\n&amp; dimensionality reduction:<span style='color:black'> k-means, expectation-maximization, PCA. [<a href=\"UnsupLearn273AFall06.ppt\">slides lec5,6</a>]</span></span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt\"><span class=GramE><span style=\"color:blue\">4</span></span><span style=\"color:blue\">: </span>neural networks: <span class=SpellE>perceptron</span>, multi-layer networks, back-propagation. [<a href=\"NeuralNets273AFall06.ppt\">slides lec7,8</a>]</span></h2>\n\t\t\t<h2><span class=GramE><span style=\"font-size:10.0pt;color:blue\">5</span></span><span style=\"font-size:10.0pt;color:blue\">: </span><span style='font-size:10.0pt'>reinforcement\nlearning: <span class=SpellE>MDPs</span>, TD- and Q-learning, value iteration.  [<a href=\"ReinfLearn273AFall06.ppt\">slides lec9,10</a>]</span></h2>\n\t\t\t<h2><span class=GramE><span style=\"font-size:10.0pt;color:blue\">6:</span></span><span style=\"font-size:10.0pt;color:blue\"> </span><span style='font-size:10.0pt'>Bayesian\nmethods: conditional independence, generative models, naive <span class=SpellE>Bayes</span> classifier. [<a href=\"BayesLearn273AFall06.ppt\">slides lec11,12 </a>] </span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt\"><span class=GramE><span style=\"color:blue\">7</span></span><span\nstyle='color:blue'>: </span>classification II: kernel methods &amp; support vector machines. [<a href=\"SVM273AFall06.ppt\">slides lec13,14</a>] <br>\n\t\t\t\t\t     required reading on SVM [<a href=\"SVM.pdf\">classnotes SVM</a>]. <br>\n\t\t\t\t\t    Additional background reading [<a href=\"Convex-Opt.pdf\">classnotes convex optimization</a>] <br>\n\t\t\t\t</span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt;color:blue\"><font color=\"#c00079\">In the last week we will do do class presentations of your projects. Please prepare 10 mins talks</font></span></h2>\n\t\t\t<h2><span style='font-size:10.0pt'><a href=\"Final-273Afall06-answers.pdf\">Final + ansers</a></span></h2>\n\t\t\t<h2></h2>\n\t\t\t<h2><span style='font-size:10.0pt;font-family:Arial;color:black'>\n\n<hr size=2 width=\"100%\" align=left>\n\n</span></h2>\n\t\t\t<h2><span style='font-size:10.0pt;font-family:Arial;color:red'>Syllabus:</span></h2>\n\n<h2><span style='font-size:10.0pt'>The course will primarily be lecture-based\nwith homework and<br>\nexams. Most homework will revolve around the implementation of various<br>\nclassification algorithms on the SciTech dataset provided above.<br>\n\t\t\t\t\t\nIt is required that you use MATLAB for this coding work. </span></h2>\n\n<h2><span style='font-size:10.0pt'>\n\n<hr size=2 width=\"100%\" align=left>\n\n</span></h2>\n\n<h2><span style='font-size:10.0pt;font-family:Arial;color:red'>Grading Criteria</span></h2>\n\n<h2><span style='font-size:10.0pt;color:black'><br>\n\t\t\t\t\tGrading will be based on a combination of weekly homework (10%) ,<span class=GramE><span\nstyle='mso-spacerun:yes'>&nbsp; </span>projects</span><span\nstyle='mso-spacerun:yes'>&nbsp; </span>(35%), some midterm (20% ) and a final exam (35%) .</span></h2>\n\n<h2><span style='font-size:10.0pt;color:black'>\n\n<hr size=2 width=\"100%\" align=left>\n\n</span></h2>\n\n<h2><span style='font-size:10.0pt;font-family:Arial;color:red'>Textbook</span></h2>\n\n<h2><span style='font-size:10.0pt;color:#000099'><br>\n\t\t\t\t\t\nThe textbook that will be used for this course is:</span></h2>\n\n<h2><span style='font-size:10.0pt;color:black'>1. Tom Mitchell: Machine\nLearning. <i style='mso-bidi-font-style:normal'>(http://www.cs.cmu.edu/~tom/mlbook.html)</i></span></h2>\n\n<h2><span style='font-size:10.0pt;color:#000099'><br>\nOptional side readings are:</span></h2>\n\n<h2><span style='font-size:10.0pt;color:black'>2. D. MacKay: Information\nTheory, Inference and Learning Algorithms<br>\n3. R.O. <span class=SpellE>Duda</span>, P.E. Hart, D. Stork: Pattern\nClassification<br>\n4. C.M. Bishop: Neural Networks for Pattern Recognition<br>\n5. T. <span class=SpellE>Hastie</span>, R. <span class=SpellE>Tibshirani</span>,\nJ.H, Friedman: The Elements of Statistical Learning <br>\n6. B.D. Ripley: Pattern Recognition and Neural Networks</span></h2>\n\n</div>\n\n</body>\n\n</html>\n", "id": 2813.0}