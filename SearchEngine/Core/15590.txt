{"text": "Expected Values in R Finite Sample Spaces The definition of the expected value for a finite sample space with outcomes x1 x2 x3 xn with probabilities p1 p2 p3 pn respectively is x1 p1 x2 p2 x3 p3 xn pn For example a Bernoulli trial with probability p of generating a 1 and probability 1 p of generating a has expected value or mean 1 p 1 p p Example 4 23 of the Moore McCabe text Introduction to the Practice of Statistics reports that the distribution of sizes of U S families is size 2 3 4 5 6 7 prob 413 236 211 9 32 18 To use R to compute the expected i e average size of a family from this table first create two data vectors then multiply and sum size 2 7 p c 413 236 211 9 32 18 sum p size The multiplication of two datasets in R is element by element so R multiplies 2 413 3 236 etc The sum function then adds the products together and returns 3 146 Note that the Moore McCabe text reports it has ignored families of 8 or more members so this is not the true mean family size which is about 3 17 We can also compute the expected value of a probability distribution for which we have a density function in R such as the Binomial Suppose we would like to numerically compute the expected value of a random variable that has the Binomial 1 2 distribution generated by counting the number of successes in 1 independent Bernoulli trials each with probability 2 of success k 1 p dbinom k 1 2 sum k p We could also verify this result by making use of the fact that a Binomial 1 2 random variable is just the sum of 1 Bernoulli random variables each with probability 2 of returning a 1 and the fact that the expected value of a sum of random variables is the sum of their expected values Thus the mean of the Binomial 1 2 distribution must be 1 2 2 Infinite Sample Spaces Several discrete random variables take values in infinite sample spaces including random variables that have a geometric distribution a negative binomial distribution or a Poisson distribution This is a slightly harder problem as we have to add up infinitely many terms While it is possible to compute the expected values analytically for these particular cases we can also get accurate values numerically You may want to review some of the standard probability distributions before proceeding Suppose that we are tossing a fair coin repeatedly and counting the number of tails we observe before we get the first heads Let X be the number of tails we observe then here is a little table showing the first few terms k 1 2 3 4 5 6 7 8 P X k 1 2 1 4 1 8 1 16 1 32 1 64 1 128 1 256 1 512 products 1 4 1 4 3 16 1 8 5 64 3 64 7 256 1 64 We have to sum the terms in the products One way to proceed is to simply pick some large number of terms such as 1 or 1 compute the geometric probabilities and sum the products k 1 prob dgeom k 5 sum k prob This works well here because the terms are decreasing rapidly in magnitude so we get an essentially exact sum for a fairly small number of terms What is the smallest number of terms for which R returns the correct answer 1 Try a couple of other geometric distributions i e different values of p such as 1 3 1 4 etc and see if you can guess the pattern i e what is the expected value of a geometric random variable with probability p It should be clear after a few examples if not try deriving it analytically Here is a function that computes the expected value in a different way summing terms until the total converges to a fixed value It reports the number of terms it used See if you can understand how it works Egeom function p usage Egeom p computes the expected value of a geometric random variable with probability p of success on each trial e e 1 i while e e e e i i 1 e e i dgeom i p cat i terms n cat expected value e n e Try it for one of the cases you have already computed such as p 1 3 Egeom 1 3 For further amusement try computing the expected values of Poisson or negative binomial random variables Variances The variance of a random variable is just the expected value of the squared deviation from the mean E X E X 2 Returning to Example 4 23 of the Moore McCabe text size 2 3 4 5 6 7 prob 413 236 211 9 32 18 To use R to compute the variance of the size of a family from this table repeat the mean calculation then compute the weighted mean of the squared deviations from the mean size 2 7 p c 413 236 211 9 32 18 M sum p size V sum p size M 2 If the family sizes were all equally likely i e each of the values had probability 1 6 would the variance be larger or smaller Try computing the variance of a Binomial 1 2 random variable You should get 1 6 Material taken from Albyn Jones Math 141 Lab Webpage Nov 2 13 Stats 12 A Index", "_id": "http://www.ics.uci.edu/~staceyah/120A/Expectation.html", "title": "expected value in r", "html": "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\">\n<html>\n  <head>\n    <meta http-equiv=\"content-type\" content=\"text/html; charset=UTF-8\">\n    <title>Expected Value in R</title>\n    <meta content=\"Stacey Hancock\" name=\"author\">\n  </head>\n  <body>\n    <table width=\"500\" align=\"center\" border=\"1\" cellpadding=\"2\"\n      cellspacing=\"2\">\n      <tbody>\n        <tr>\n          <th valign=\"middle\" align=\"center\" bgcolor=\"#ffffcc\">\n            <h1>Expected Values in R<br>\n            </h1>\n          </th>\n        </tr>\n      </tbody>\n    </table>\n    <meta http-equiv=\"content-type\" content=\"text/html; charset=UTF-8\">\n    <h2> Finite Sample Spaces </h2>\n    <p> The definition of the expected value for a finite sample space\n      with outcomes x1, x2, x3, ..., xn, with probabilities p1, p2, p3,\n      ..., pn, respectively, is: </p>\n    <pre>\tx1*p1 + x2*p2 + x3*p3 + ... + xn*pn\n</pre>\n    For example, a Bernoulli trial with probability p of generating a 1,\n    and probability (1-p) of generating a 0, has expected value (or\n    mean)\n    <pre>\t0*(1-p) + 1*p = p\n</pre>\n    <p> Example 4.23 of the Moore &amp; McCabe text \"Introduction to the\n      Practice of Statistics\" reports that the distribution of sizes of\n      U.S. families is: </p>\n    <pre>\tsize:  2    3    4    5    6    7\n\tprob: .413 .236 .211 .090 .032 .018\n</pre>\n    To use R to compute the expected (i.e., average) size of a family\n    from this table, first create two data vectors, then multiply and\n    sum:\n    <pre>\tsize &lt;- 2:7\n\tp &lt;- c(.413, .236, .211, .090, .032, .018)\n\tsum(p*size)\n</pre>\n    The multiplication of two datasets in R is element by element, so R\n    multiplies 2*.413, 3*.236, etc. The sum function then adds the\n    products together, and returns 3.146. Note that the Moore &amp;\n    McCabe text reports it has ignored families of 8 or more members, so\n    this is not the true mean family size (which is about 3.17).\n    <p> We can also compute the expected value of a probability\n      distribution for which we have a density function in R, such as\n      the Binomial. Suppose we would like to numerically compute the\n      expected value of a random variable that has the Binomial(10, .2)\n      distribution generated by counting the number of successes in 10\n      independent Bernoulli trials each with probability .2 of success:\n    </p>\n    <pre>\tk &lt;- 0:10\n\tp &lt;- dbinom(k,10,.2)\n\tsum(k*p)\n</pre>\n    <p> We could also verify this result by making use of the fact that\n      a Binomial(10, .2) random variable is just the sum of 10 Bernoulli\n      random variables each with probability .2 of returning a 1, and\n      the fact that the expected value of a sum of random variables is\n      the sum of their expected values. Thus the mean of the\n      Binomial(10, .2) distribution must be 10*(.2) = 2. </p>\n    <h2> Infinite Sample Spaces </h2>\n    Several discrete random variables take values in infinite sample\n    spaces, including random variables that have a geometric\n    distribution, a negative binomial distribution, or a Poisson\n    distribution. This is a slightly harder problem, as we have to add\n    up infinitely many terms. While it is possible to compute the\n    expected values analytically for these particular cases, we can also\n    get accurate values numerically. (You may want to review some of the\n    <a href=\"Prob_Distns.html\">standard probability distributions</a>\n    before proceeding.)\n    <p> Suppose that we are tossing a fair coin repeatedly, and counting\n      the number of tails we observe before we get the first heads. Let\n      X be the number of tails we observe, then here is a little table\n      showing the first few terms: </p>\n    <pre>     k:  0    1    2    3    4    5    6     7     8...\nP(X=k): 1/2  1/4  1/8  1/16 1/32 1/64 1/128 1/256 1/512...\n\nproducts: 0  1/4  1/4  3/16 1/8  5/64  3/64 7/256 1/64...\n</pre>\n    We have to sum the terms in the products. One way to proceed is to\n    simply pick some large number of terms, such as 100 or 1000, compute\n    the geometric probabilities and sum the products:\n    <pre>\tk &lt;- 0:100\n\tprob &lt;- dgeom(k,.5)\n\tsum(k*prob)\n</pre>\n    This works well here because the terms are decreasing rapidly in\n    magnitude, so we get an essentially exact sum for a fairly small\n    number of terms. What is the smallest number of terms for which R\n    returns the correct answer 1.0? Try a couple of other geometric\n    distributions, i.e., different values of p, such as 1/3, 1/4, etc.\n    and see if you can guess the pattern, i.e., what is the expected\n    value of a geometric random variable with probability p? It should\n    be clear after a few examples; if not, try deriving it analytically.\n    <p> Here is a function that computes the expected value in a\n      different way, summing terms until the total converges to a fixed\n      value. It reports the number of terms it used. See if you can\n      understand how it works! <br>\n    </p>\n    <hr>\n    <pre>Egeom &lt;- function(p){\n#\n# usage:  Egeom(p) computes the expected value of a geometric\n# random variable with probability p of success on each trial\n#\ne &lt;- 0\ne0 &lt;- -1\ni &lt;- 0\nwhile(e != e0)\n{\n\te0&lt;-e  \n\ti &lt;- i+1\n\te &lt;- e + i*dgeom(i,p)\n}\ncat(i,\"terms \\n\")\ncat(\"expected value =\",e,\"\\n\")\ne\n}\n</pre>\n    <hr> Try it for one of the cases you have already computed, such as\n    p = 1/3:\n    <pre>       Egeom(1/3)\n</pre>\n    <p> For further amusement, try computing the expected values of\n      Poisson or negative binomial random variables. </p>\n    <p> </p>\n    <h2> Variances</h2>\n    <p> The variance of a random variable is just the expected value of\n      the squared deviation from the mean: <b>E</b>((X-<b>E</b>(X))^2).\n      Returning to Example 4.23 of the Moore &amp; McCabe text: </p>\n    <pre>\tsize:  2    3    4    5    6    7\n\tprob: .413 .236 .211 .090 .032 .018\n</pre>\n    To use <b>R</b> to compute the variance of the size of a family\n    from this table, repeat the mean calculation, then compute the\n    weighted mean of the squared deviations from the mean:\n    <pre>\tsize &lt;- 2:7\n\tp &lt;- c(.413, .236, .211, .090, .032, .018)\n\tM &lt;- sum(p*size)\n\tV &lt;- sum(p*(size-M)^2)\n</pre>\n    If the family sizes were all equally likely (i.e., each of the\n    values had probability 1/6) would the variance be larger or smaller?\n    <p> Try computing the variance of a Binomial(10, .2) random\n      variable. You should get 1.6.<br>\n    </p>\n    <hr size=\"2\" width=\"100%\"><i>Material taken from Albyn Jones' Math\n      141 Lab Webpage, Nov. 2013</i><br>\n    <br>\n    <hr style=\"width: 100%; height: 2px;\"><big></big><br>\n    <div align=\"center\"> <big> <a href=\"index.html\">Stats 120A Index</a></big><br>\n      <br>\n    </div>\n    <hr style=\"width: 100%; height: 2px;\">\n    <p><br>\n    </p>\n  </body>\n</html>\n", "id": 15590.0}