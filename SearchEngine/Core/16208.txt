{"text": "Introduction to Machine Learning and Data Mining Winter 2 7 ICS 178 Instructor Max Welling Lecture TuTh 3 3 4 5 pm PSCB22 Discussion W 3 3 5 pm ICS 243 Prerequisites ICS 6A Mathematics 6A Mathematics 6B Mathematics 6C or 3A Mathematics 2A B Statistics 67 Mathematics 67 Goals The goal of this class is to familiarize you with various state of the art machine learning techniques for classification regression clustering and dimensionality reduction You will implement a number of algorithms on the netflix problem participate in group discussions and give presentations At the end of the class you will be able to apply these techniques to novel problems in academia and industry Homework Please see the slides Projects Netflix problem Netflix site Code Here is code to download the netflix data into memory about 1Gig RAM required Some note by Jeff Taggert Syllabus subject to change 1 Introduction overview examples goals probability conditional independence matrices eigenvalue decompositions slides Lec 1 slides Lec 2 2 Optimization and Data Visualization Stochastic gradient descent coordinate descent centering sphering histograms scatter plots slides Lec 3 slides Lec 4 3 Least Squares Regression Logistic Regression Least Squares Matrix Factorization slides Lec 5 slides Lec 6 4 Clustering k means and soft clustering EM MDL penalty slides Lec7 5 Decision Trees Boosting slides Lec8 Classification I emprirical Risk Minimization k nearest neighbors decision stumps decision tree Classification II random forests boosting Neural networks perceptron logistic regression multi layer networks back propagation Regression Least squares regression Dimesionality reduction principal components analysis Fisher linear discriminant analysis Reinforcement learning MDPs TD and Q learning value iteration Bayesian methods Bayes rule generative models naive Bayes classifier Matlab Demos LSRegression testLSR LSRegression demo LogRegression demo plotGauss1D ginput2 Grading Criteria Grading will be based on a combination of weekly homework projects midterm and a final exam Textbook The textbook that will be used for this course is 1 R O Duda P E Hart D Stork Pattern Classification Optional side readings are 2 Tom Mitchell Machine Learning http www cs cmu edu tom mlbook html 3 D MacKay Information Theory Inference and Learning Algorithms 4 C M Bishop Neural Networks for Pattern Recognition 5 T Hastie R Tibshirani J H Friedman The Elements of Statistical Learning 6 B D Ripley Pattern Recognition and Neural Networks", "_id": "http://www.ics.uci.edu/%7ewelling/teaching/ICS178Winter07/ICS178Winter07.html", "title": "untitled document", "html": "<html xmlns:v=\"urn:schemas-microsoft-com:vml\"\nxmlns:o=\"urn:schemas-microsoft-com:office:office\"\nxmlns:w=\"urn:schemas-microsoft-com:office:word\"\nxmlns:st1=\"urn:schemas-microsoft-com:office:smarttags\"\nxmlns=\"http://www.w3.org/TR/REC-html40\">\n\n<head>\n<meta http-equiv=Content-Type content=\"text/html; charset=us-ascii\">\n<meta name=ProgId content=Word.Document>\n<meta name=Generator content=\"Microsoft Word 11\">\n<meta name=Originator content=\"Microsoft Word 11\">\n<link rel=File-List href=\"ICS273AFall06_files/filelist.xml\">\n<link rel=Edit-Time-Data href=\"ICS273AFall06_files/editdata.mso\">\n<!--[if !mso]>\n<style>\nv\\:* {behavior:url(#default#VML);}\no\\:* {behavior:url(#default#VML);}\nw\\:* {behavior:url(#default#VML);}\n.shape {behavior:url(#default#VML);}\n</style>\n<![endif]-->\n<title>Untitled Document</title>\n<o:SmartTagType namespaceuri=\"urn:schemas-microsoft-com:office:smarttags\"\n name=\"place\"/>\n<o:SmartTagType namespaceuri=\"urn:schemas-microsoft-com:office:smarttags\"\n name=\"City\"/>\n<!--[if gte mso 9]><xml>\n <o:DocumentProperties>\n  <o:Author>Welling</o:Author>\n  <o:Template>Normal</o:Template>\n  <o:LastAuthor>Welling</o:LastAuthor>\n  <o:Revision>9</o:Revision>\n  <o:TotalTime>131</o:TotalTime>\n  <o:Created>2006-07-03T18:58:00Z</o:Created>\n  <o:LastSaved>2006-08-30T21:40:00Z</o:LastSaved>\n  <o:Pages>1</o:Pages>\n  <o:Words>358</o:Words>\n  <o:Characters>2044</o:Characters>\n  <o:Company> UCI</o:Company>\n  <o:Lines>17</o:Lines>\n  <o:Paragraphs>4</o:Paragraphs>\n  <o:CharactersWithSpaces>2398</o:CharactersWithSpaces>\n  <o:Version>11.5606</o:Version>\n </o:DocumentProperties>\n</xml><![endif]--><!--[if gte mso 9]><xml>\n <w:WordDocument>\n  <w:Zoom>125</w:Zoom>\n  <w:SpellingState>Clean</w:SpellingState>\n  <w:GrammarState>Clean</w:GrammarState>\n  <w:ValidateAgainstSchemas/>\n  <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid>\n  <w:IgnoreMixedContent>false</w:IgnoreMixedContent>\n  <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText>\n  <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel>\n </w:WordDocument>\n</xml><![endif]--><!--[if gte mso 9]><xml>\n <w:LatentStyles DefLockedState=\"false\" LatentStyleCount=\"156\">\n </w:LatentStyles>\n</xml><![endif]--><!--[if !mso]><object\n classid=\"clsid:38481807-CA0E-42D2-BF39-B33AF135CC4D\" id=ieooui></object>\n<style>\nst1\\:*{behavior:url(#ieooui) }\n</style>\n<![endif]-->\n<style>\n<!--\n /* Font Definitions */\n @font-face\n\t{font-family:Tahoma;\n\tpanose-1:2 11 6 4 3 5 4 4 2 4;\n\tmso-font-charset:0;\n\tmso-generic-font-family:swiss;\n\tmso-font-pitch:variable;\n\tmso-font-signature:1627421319 -2147483648 8 0 66047 0;}\n /* Style Definitions */\n p.MsoNormal, li.MsoNormal, div.MsoNormal\n\t{mso-style-parent:\"\";\n\tmargin:0in;\n\tmargin-bottom:.0001pt;\n\tmso-pagination:widow-orphan;\n\tfont-size:12.0pt;\n\tfont-family:\"Times New Roman\";\n\tmso-fareast-font-family:\"Times New Roman\";}\nh2\n\t{mso-margin-top-alt:auto;\n\tmargin-right:0in;\n\tmso-margin-bottom-alt:auto;\n\tmargin-left:0in;\n\tmso-pagination:widow-orphan;\n\tmso-outline-level:2;\n\tfont-size:18.0pt;\n\tfont-family:\"Times New Roman\";}\na:link, span.MsoHyperlink\n\t{color:blue;\n\ttext-decoration:underline;\n\ttext-underline:single;}\na:visited, span.MsoHyperlinkFollowed\n\t{color:blue;\n\ttext-decoration:underline;\n\ttext-underline:single;}\nspan.SpellE\n\t{mso-style-name:\"\";\n\tmso-spl-e:yes;}\nspan.GramE\n\t{mso-style-name:\"\";\n\tmso-gram-e:yes;}\n@page Section1\n\t{size:8.5in 11.0in;\n\tmargin:1.0in 1.25in 1.0in 1.25in;\n\tmso-header-margin:.5in;\n\tmso-footer-margin:.5in;\n\tmso-paper-source:0;}\ndiv.Section1\n\t{page:Section1;}\n-->\n</style>\n<!--[if gte mso 10]>\n<style>\n /* Style Definitions */\n table.MsoNormalTable\n\t{mso-style-name:\"Table Normal\";\n\tmso-tstyle-rowband-size:0;\n\tmso-tstyle-colband-size:0;\n\tmso-style-noshow:yes;\n\tmso-style-parent:\"\";\n\tmso-padding-alt:0in 5.4pt 0in 5.4pt;\n\tmso-para-margin:0in;\n\tmso-para-margin-bottom:.0001pt;\n\tmso-pagination:widow-orphan;\n\tfont-size:10.0pt;\n\tfont-family:\"Times New Roman\";\n\tmso-ansi-language:#0400;\n\tmso-fareast-language:#0400;\n\tmso-bidi-language:#0400;}\n</style>\n<![endif]--><!--[if gte mso 9]><xml>\n <o:shapedefaults v:ext=\"edit\" spidmax=\"12290\"/>\n</xml><![endif]--><!--[if gte mso 9]><xml>\n <o:shapelayout v:ext=\"edit\">\n  <o:idmap v:ext=\"edit\" data=\"1\"/>\n </o:shapelayout></xml><![endif]-->\n</head>\n\n<body bgcolor=\"#CCCCCC\" background=\"../../background.gif\" lang=EN-US link=blue\nvlink=blue style='tab-interval:.5in'>\n\n<div class=Section1>\n\n<h2><span style='font-size:10.0pt;font-family:Tahoma;color:red'>Introduction to Machine Learning and Data Mining, <span class=GramE>Winter 2007</span></span></h2>\n\t\t\t<h2><span style='font-size:10.0pt'>ICS: 178</span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt\">Instructor: Max Welling</span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt\">Lecture TuTh 3.30-4.50pm PSCB220</span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt\">Discussion: W 3.00-3.50pm ICS 243</span></h2>\n\t\t\t<h2><span style='font-size:10.0pt'>\n\n<hr size=2 width=\"100%\" align=left>\n\n</span></h2>\n\t\t\t<h2><span style='font-size:10.0pt;font-family:Arial;color:red'>Prerequisites</span></h2>\n\n<h2><span style=\"font-size:10.0pt\">ICS 6A/Mathematics 6A, Mathematics 6B, Mathematics 6C or 3A, Mathematics 2A-B, Statistics 67/Mathematics 67.</span></h2>\n\n<h2><span style='font-size:10.0pt'>\n\n<hr size=2 width=\"100%\" align=left>\n\n</span></h2>\n\n<h2><span style='font-size:10.0pt;font-family:Arial;color:red'>Goals:</span></h2>\n\t\t\t<h2><span style='font-size:10.0pt;color:black'>The goal of this class is to familiarize you with various state-of-the-art machine learning techniques for classification, regression, <span class=GramE>clustering</span> and dimensionality reduction. <br>\n\t\t\t\t\t You will implement a number of algorithms on the netflix problem, participate in group discussions and give presentations. <br>\n\t\t\t\t\t At the end of the class you will be able to apply these techniques to novel problems in academia and industry.<br>\n\t\t\t\t</span></h2>\n\t\t\t<h2><span style='font-size:10.0pt;font-family:Arial;color:black'>\n\n<hr size=2 width=\"100%\" align=left>\n\n</span></h2>\n\t\t\t<h2><span class=GramE><span style='font-size:10.0pt;font-family:Arial;\ncolor:red'>Homework :</span></span><span style='font-size:10.0pt;font-family:\nArial;color:red'>  <font color=\"black\">Please see the slides.</font></span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt;font-family: Arial;color:red\"><font color=\"red\">Projects:  </font><font color=\"black\"> Netflix problem. </font></span><span style=\"font-size:10.0pt;font-family: Arial;color:red\"><font color=\"black\"><a href=\"http://www.netflixprize.com/index\">Netflix site</a></font></span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt;font-family: Arial;color:red\"><font color=\"red\">Code:</font><font color=\"black\">  Here is  <a href=\"netflixcpp.tgz\">code</a> to download the netflix data into memory (about 1Gig RAM required). Some <a href=\"netflix_on_solaris.txt\">note</a> by Jeff Taggert</font></span></h2>\n\t\t\t<h2><span style='font-size:10.0pt;font-family:Arial;color:black'>\n\n<hr size=2 width=\"100%\" align=left>\n\n</span></h2>\n\t\t\t<h2><span style='font-size:10.0pt;font-family:Arial;color:red'>Syllabus: (subject to change)</span></h2>\n\n<h2><span class=GramE><span style=\"font-size:10.0pt;color:blue\">1</span></span><span\nstyle='font-size:10.0pt;color:blue'>:</span><span style='font-size:10.0pt;\ncolor:red'> </span><span style=\"font-size:10.0pt\">Introduction: overview, examples, goals, probability, conditional independence, matrices, eigenvalue decompositions [<a href=\"Intro178winter07.ppt\">slides Lec 1</a>] [<a href=\"VisSupLearn178winter07.ppt\">slides Lec 2</a>]</span></h2>\n\n<h2><span class=GramE><span style=\"font-size:10.0pt;color:blue\">2</span></span><span style=\"font-size:10.0pt;color:blue\">:</span><span style='font-size:10.0pt;\ncolor:red'> <font color=\"black\">Optimization and Data Visualization: Stochastic gradient descent, coordinate descent, centering, sphering, histograms, scatter-plots. [<a href=\"SupLearn178winter07.ppt\">slides Lec 3</a>] [<a href=\"LSRegr178winter07.ppt\">slides Lec 4</a>]</font></span></h2>\n\t\t\t<h2><span class=GramE><span style=\"font-size:10.0pt;color:blue\">3</span></span><span style=\"font-size:10.0pt;color:blue\">:</span><span style='font-size:10.0pt;\ncolor:red'>  <font color=\"black\">Least Squares Regression, Logistic Regression, Least Squares Matrix Factorization  [<a href=\"SVD178winter07.ppt\">slides Lec 5</a>] [<a href=\"LogReg178winter07.ppt\">slides Lec 6</a>]</font></span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt;color:blue\"><font color=\"#270092\">4:</font><font color=\"black\"> Clustering: k-means and soft clustering (EM), MDL penalty. <a href=\"Clustering178winter07.ppt\">[slides Lec7]</a></font></span></h2>\n\t\t\t<h2><span class=GramE><span style=\"font-size:10.0pt;color:blue\">5</span></span><span style=\"font-size:10.0pt;color:blue\">:</span><span style='font-size:10.0pt;\ncolor:red'>  <font color=\"black\">Decision Trees &amp; Boosting [<a href=\"DTBoosting178winter07.ppt\">slides Lec8</a>]</font></span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt\">Classification I: emprirical Risk Minimization, k-ne</span><span style=\"font-size:10.0pt\">arest neighbors, decision stumps, decision tree, </span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt\"><span style=\"color:blue\"> </span>Classification II: random forests, boosting.</span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt\"><span style=\"color:blue\"> </span>Neural networks: <span class=SpellE>perceptron</span>, logistic regression, multi-layer networks, back-propagation.</span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt;color:blue\"> <font color=\"black\">Regression: Least squares regression.</font></span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt;color:blue\"> </span><span style=\"font-size:10.0pt\">Dimesionality reduction: principal components analysis, Fisher linear discriminant analysis.</span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt;color:blue\"> </span><span style=\"font-size:10.0pt\">Reinforcement learning: <span class=SpellE>MDPs</span>, TD- and Q-learning, value iteration.</span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt;color:blue\"> </span><span style='font-size:10.0pt'>Bayesian methods: Bayes rule, generative models, naive <span class=SpellE>Bayes</span> classifier. </span></h2>\n\t\t\t<hr size=2 width=\"100%\" align=left>\n\t\t\t<p><strong><font color=\"red\">Matlab Demos </font></strong></p>\n\t\t\t<p><span style=\"font-size:10.0pt;font-family: Arial;color:red\"><font color=\"black\"><a href=\"LSRegression.m\">LSRegression</a>, <a href=\"testLSRegression.m\">testLSR</a>, <a href=\"demo_LinReg.m\"> LSRegression_demo</a>,  <a href=\"demo_LogReg.m\">LogRegression_demo</a>,  <a href=\"plotGauss1D.m\">plotGauss1D</a>,  <a href=\"ginput2.m\">ginput2</a></font></span></p>\n\t\t\t<p></p>\n\t\t\t<hr size=2 width=\"100%\" align=left>\n\t\t\t<h2></h2>\n\t\t\t<h2><span style='font-size:10.0pt;font-family:Arial;color:red'>Grading Criteria</span></h2>\n\n<h2><span style='font-size:10.0pt;color:black'>Grading will be based on a combination of weekly homework,<span class=GramE><span\nstyle='mso-spacerun:yes'>&nbsp; </span>projects,</span><span\nstyle='mso-spacerun:yes'>\u00a0</span> midterm  and a final exam.</span></h2>\n\n<h2><span style='font-size:10.0pt;color:black'>\n\n<hr size=2 width=\"100%\" align=left>\n\n</span></h2>\n\n<h2><span style='font-size:10.0pt;font-family:Arial;color:red'>Textbook</span></h2>\n\n<h2><span style='font-size:10.0pt;color:#000099'>\n\t\t\t\t\t\nThe textbook that will be used for this course is:</span></h2>\n\n<h2><span style='font-size:10.0pt;color:black'>1.  R.O. <span class=SpellE>Duda</span>, P.E. Hart, D. Stork: Pattern\nClassification<br>\n\t\t\t\t</span></h2>\n\n<h2><span style='font-size:10.0pt;color:#000099'>\nOptional side readings are:</span></h2>\n\t\t\t<h2><span style='font-size:10.0pt;color:black'>2. Tom Mitchell: Machine Learning. <i style='mso-bidi-font-style:normal'>(http://www.cs.cmu.edu/~tom/mlbook.html)<br>\n\t\t\t\t\t</i>3. D. MacKay: Information Theory, Inference and Learning Algorithms<br>\n\t\t\t\t\t4. C.M. Bishop: Neural Networks for Pattern Recognition<br>\n\t\t\t\t\t\n5. T. <span class=SpellE>Hastie</span>, R. <span class=SpellE>Tibshirani</span>,\nJ.H, Friedman: The Elements of Statistical Learning <br>\n\t\t\t\t\t\n6. B.D. Ripley: Pattern Recognition and Neural Networks</span></h2>\n\t\t\t<h2></h2>\n\t\t\t<h2></h2>\n\t\t</div>\n\n</body>\n\n</html>\n", "id": 16208.0}