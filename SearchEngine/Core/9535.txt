{"text": "CompSci 267 Homework set 1 Sayood p 38 1 Suppose X is a random variable that takes on values from an M letter alphabet Show that H X lg M Sayood p 38 3 Given an alphabet a b c d find the first order entropy in the following cases a P a P b P c P d 1 4 b P a 1 2 P b 1 4 P c P d 1 8 c P a 5 5 P b 1 4 P c 1 8 P d 12 Sayood p 39 6abcd Conduct an experiment to see how well a model can describe a source a Write a program that randomly selects letters from the 26 letter alphabet and forms four letter words Form 1 such words and see how many of these words make sense Do this several times and determine the approximate expected number of sensible words b File http www ics uci edu dan class 267 datasets text 4letter words contains a list of four letter words Using this file and remembering to fold upper and lower case letters obtain a probability model for the alphabet Repeat part a generating words using the probability model You may use the random number generator located in file http www ics uci edu dan class 267 programs random c Compare your results with part a c Repeat part b using a single letter context d Repeat part b using a two letter context a Find the entropy of a source with 6 symbols having probabilities 5 2 1 1 5 and 5 b An information source has 128 equally probable symbols How long is a message from the source whose entropy is 56 bits c What is the entropy of a message of 32 symbols from a source with three symbols if each symbol is equally likely the symbol probabilities are 6 3 and 1 ", "_id": "http://www.ics.uci.edu/~dan/class/267/hw1.htm", "title": "", "html": "<HTML>\n<center>\n<H3>CompSci 267 Homework set #1</H3>\n</center>\n<OL>\n<LI> [Sayood p.38#1]\n    Suppose X is a random variable that takes on values\n    from an M-letter alphabet.\n    Show that 0 <u>&lt;</u> H(X) <u>&lt;</u> lg M.\n<BR> &nbsp;\n\n<LI> [Sayood p.38#3]\n    Given an alphabet {<I>a,b,c,d</I>}, find the first-order entropy\n    in the following cases:\n   <BR> (<I>a</I>) P(<I>a</I>)=P(<I>b</I>)=P(<I>c</I>)=P(<I>d</I>) = 1/4\n   <BR> (<I>b</I>) P(<I>a</I>)= 1/2, P(<I>b</I>)= 1/4, P(<I>c</I>)=P(<I>d</I>)= 1/8\n   <BR> (<I>c</I>) P(<I>a</I>)=0.505, P(<I>b</I>)=1/4, P(<I>c</I>)=1/8, P(<I>d</I>)=.12\n<BR> &nbsp;\n\n<LI> [Sayood p.39#6abcd]\nConduct an experiment to see how well a model can describe a source.\n<BR>\n(a) Write a program that randomly selects letters from the 26-letter\nalphabet and forms four-letter words.  Form 100 such words and see\nhow many of these words make sense.  Do this several times and\ndetermine the approximate expected number of sensible words.\n<BR>\n<BR>\n(b) File\n<a href=\"/~dan/class/267/datasets/text/4letter.words\">http://www.ics.uci.edu/~dan/class/267/datasets/text/4letter.words</a>&nbsp; &nbsp;\ncontains a list of four-letter words. Using this file, and remembering\nto fold upper- and lower-case letters, obtain a probability model for\nthe alphabet.\n<BR>\nRepeat part (a) generating words using the probability model.\n(You may use the random number generator located in file\n<a href=\"/~dan/class/267/programs/random.c\">http://www.ics.uci.edu/~dan/class/267/programs/random.c</a>&nbsp; &nbsp;)&nbsp;\nCompare your results with part (a).\n<BR>\n<BR>\n(c) Repeat part (b) using a single-letter context.\n<BR>\n<BR>\n(d) Repeat part (b) using a two-letter context.\n<BR> &nbsp;\n\n<LI> (a) Find the entropy of a source with 6 symbols\n     having probabilities .5, .2, .1, .1, .05, and .05.\n<BR> &nbsp;\n<BR> (b) An information source has 128 equally probable symbols.\n     How long is a message from the source whose entropy is 56 bits?\n<BR> &nbsp;\n<BR> (c) What is the entropy of a message of 32 symbols\n     from a source with three symbols if\n     <UL>\n     <LI> each symbol is equally likely\n     <LI> the symbol probabilities are .6, .3, and .1.\n     </UL>\n</OL>\n</HTML>\n", "id": 9535.0}