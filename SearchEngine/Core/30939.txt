{"text": "HW5 Face recognition Due Dec 9 in EEE DropBox at 11 59 pm HW5 Face recognition In this assignment you ll implement the Eigenface algorithm for face recognition from this paper I have typed up an additional writeup on PCA here A common approach to recognition is utilizing a nearest neighbor framework Here images are represented as points in a high dimensional vector space One recognizes a query face by finding the closest labelled face from a training set of images Eigenfaces extends this approach by using principle component analysis PCA to reduce the dimensionality of images before matching The following directory contains extensive skeleton code as well as the training testing images for the assignment Downloads Programming 2 points The main scriptfile hw5 run m is written completely for you You will only need to make modifications to print images for the writeup There are 4 functions which you will need to code classifyNN m 5 pts function yguess classifyNN Xtest X Y yguess classifyNN Xtest X Y X d by n design matrix Y n by 1 vector of class labels Xtest d by m matrix of test points to classify yguess m by 1 vector of output labels for test points This function implements the nearest neighbor classification framework It requires a set of n training points X and n training labels Y Given m test points with unknown labels for each test point it will return the label of the closest training point pca m 5 pts function U mu pca X k U mu pca X k X is a d by n matrix of n d dimensional data points 1 mu is a d by 1 vector average data point 2 U is a d by k matrix of k column vectors spanning the low dimensional space in which the points live 3 k is the number of principle component vectors to compute This function performs principle component analysis given the design matrix X Each column of X is a vector representing a data point in high dimensional space in our case the image of a face Because d is quite large it will be difficult to compute the eigenvectors of the entire covariance matrix which will be d by d Rather you should directly apply the SVD function on the centered design matrix U S V svd Xcen and only retain the k columns of U associated with the largest singular vectors project pca m 5 pts function res project pca X U mu res project pca X U mu X d by n matrix of n d dimensional data points U d by k matrix of basis vectors mu d by 1 vector of mean point res k by n matrix of points from X projected into space spanned by U This function projects the high dimensional points from X into the k dimensional space spanned by the vectors in U reconstruct pca m 5 pts function res reconstruct pca x U mu k res reconstruct pca x U mu k x d by 1 vector U d by k matrix returned from pca mu d by 1 vector returned from pca k the number of vectors used to reconstruct x res d by 1 vector This function reconstructs a point x using k basis vectors from U Writeup 1 points Show the error rate of the baseline nearest neighbor classifier and the eigenface algorithm Use k 5 principle components for the eigenface algoritm The error rate is the percentage of test images that are incorrectly classified For this dataset you should see values around 11 for both Pick a random image from the test set Show its closest matching training image found by the eigenface algorithm For the image from above show the difference image between it and average face from the training set mu For the image from above show its reconstruction with 1 1 and 5 principle components Matlab Tips The function bsxfun is a handy tool that lets you apply operations to each vector of a matrix For example here is quick way to center all the points in X Xcen bsxfun minus X mu Extra credit As detailed in the guidelines any project handed by 11 59 pm on the previous day Dec 8th will recieve 1 3 points extra credit ", "_id": "http://www.ics.uci.edu/~dramanan/teaching/cs116_winter15/hw/Project/Faces/", "title": "hw5: ", "html": "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n<html>\n  <head>\n    <title>HW5: </title>\n  </head>\n  <body>\n    <h1>HW5: Face recognition </h1>\n    <h3> Due Dec 9 in EEE DropBox at 11:59 pm </h3>\n    <hr>\n\n<h2>HW5: Face recognition</h2>\n\n\n<img width=200 src=\"meanface.jpg\">\n<br>\n\n<p>\nIn this assignment, you'll implement the Eigenface algorithm for face recognition from this <a href=\"http://www.cs.ucsb.edu/~mturk/Papers/jcn.pdf\"> paper </a>. I have typed up an additional writeup on PCA <a href=\"../../../lec/PCA_notes.pdf\"> here </a>. A common approach to recognition is utilizing a nearest neighbor framework. Here, images are represented as points in a high-dimensional vector space. One recognizes a query face by finding the closest labelled face from a training set of images. Eigenfaces extends this approach by using principle component analysis (PCA) to reduce the dimensionality of images before matching.\n\nThe following directory contains extensive skeleton code, as well as the training & testing images for the assignment\n\n<ul>\n<li><a href=\"downloads\">Downloads</a>\n</ul>\n\n<h4>Programming: [20 points]</h4>\n\n<p>\nThe main scriptfile, <tt>hw5_run.m</tt>, is written completely for you. You will only need to make modifications to print images for the writeup. There are 4 functions which you will need to code\n\n    <ol>\n      <li> classifyNN.m (5 pts)\n\t<blockquote><tt><pre>\nfunction yguess = classifyNN(Xtest,X,Y)\n% yguess = classifyNN(Xtest,X,Y)\n% X:     \"d by n\" design matrix\n% Y:     \"n by 1\" vector of class labels\n% Xtest: \"d by m\" matrix of test points to classify\n% yguess: \"m by 1\" vector of output labels for test points\n\t  </pre></tt></blockquote>\n\tThis function implements the nearest-neighbor classification framework. It requires a set of \"n\" training points (X) and \"n\" training labels (Y). Given \"m\" test points with unknown labels, for each test point, it will return the label of the closest training point.\n\t<p>\n      <li> pca.m (5 pts)\n\t<blockquote><tt><pre>\nfunction [U,mu] = pca(X,k)\n% [U,mu] = pca(X,k)\n% X is a \"d by n\" matrix of n d-dimensional data points\n% 1) mu is a \"d by 1\" vector = average data point\n% 2) U is a \"d by k\" matrix of k column vectors spanning the low-dimensional space\n%   in which the points live  \n% 3) k is the number of principle component vectors to compute\n\t  </pre></tt></blockquote>\n\tThis function performs principle component analysis given the design matrix X. Each column of X is a vector representing a data point in high-dimensional space; in our case, the image of a face. Because d is quite large, it will be difficult to compute the eigenvectors of the entire covariance matrix, which will be \"d by d\". Rather, you should directly apply the SVD function on the centered design matrix <tt> [U,S,V] = svd(Xcen) </tt>, and only retain the \"k\" columns of U associated with the largest singular vectors.\n\t<p>\n      <li> project_pca.m (5 pts)\n\t<blockquote><tt><pre>\nfunction res = project_pca(X,U,mu)\n% res = project_pca(X,U,mu)\n% X: \"d by n\" matrix of n d-dimensional data points\n% U: \"d by k\" matrix of basis vectors\n% mu: \"d by 1\" vector of mean point\n% res: \"k by n\" matrix of points from X projected into space spanned by U\n\t  </pre></tt></blockquote>\n\tThis function projects the high-dimensional points from X into the k dimensional-space spanned by the vectors in U.\n\t<p>\n\t<li> reconstruct_pca.m (5 pts)\n\t<blockquote><tt><pre>\nfunction res = reconstruct_pca(x,U,mu,k)\n% res = reconstruct_pca(x,U,mu,k)\n% x: \"d by 1\" vector\n% U: \"d by k\" matrix returned from pca\n% mu: \"d by 1\" vector returned from pca\n% k:   the number of vectors used to reconstruct x\n% res: \"d by 1\" vector\n\t  </pre></tt></blockquote>\n\tThis function reconstructs a point x using k basis vectors from U.\n    </ol>\n\n<h4>Writeup: [10 points]</h4>\n\n<ol>\n\n<p><li> Show the error rate of the baseline nearest-neighbor classifier, and the eigenface algorithm. Use k = 50 principle components for the eigenface algoritm. The error rate is the percentage of test images that are incorrectly classified. For this dataset, you should see values around 11% for both.\n\n\n<p><li> Pick a random image from the test set. Show its closest matching training image found by the eigenface algorithm.\n\n<p><li> For the image from above, show the difference image between it and average face from the training set (mu).\n\n<p><li> For the image from above, show its reconstruction with 1, 10, and 50 principle components.\n\n<h4>Matlab Tips</h4>\n\n<ol>\n\n<p><li> The function <tt> bsxfun</tt> is a handy tool that lets you apply operations to each vector of a matrix. For example, here is quick way to center all the points in X: <tt> Xcen = bsxfun(@minus,X,mu); </tt>\n\n</ol>\n\n<h4>Extra-credit</h4>\n    As detailed in the <a href=\"../../hw_guide.html\"> guidelines</a>, any project handed by 11:59 pm on the previous day (Dec 8th), will recieve 10% (3 points) extra credit.\n\n</p></div></center>\n</body></html>", "id": 30939.0}