{"text": "Learning in Graphical Models spring 2 6 ICS 274B Instructor Max Welling Code Typ Sec Unt Instructor Time Place 36745 Lec A 4 WELLING M TuTh 2 3 2 p CS 213 Prerequisites ICS 274A Probabilistic Learning Theory and Algorithms or with consent of instructor Goals Many modern approaches to probabilistic modeling of real world data sets can be formulated in the unifying framework of graphical models Graphical models provide a common language to think and communicate about probabilistic models and makes explicit the underlying assumptions Moreover it provides the appropriate structure for computations necessary for inference and learning in these models It is the primary goal of this course to familiarize the student with the concepts of graphical models and in particular with learning these models from data A student who has successfully completed the course should be able to understand a wide variety of well known models in terms of this unifying framework and feel comfortable using it to design new models The course will contain 1 formal mathematical sections necessary for the development of the theory 2 examples of probabilistic models re formulated in the language of graphical models and 3 examples of successful applications to real data This secondary goal of this class is to give the students hands on experience in solving real world problems For that purpose I have negotiated a deal with SciTech a San Diego based company if we improve their naive Bayes classifier on a particular classification problem prediction of activity levels of chemical compounds for data see below then they will provide a 3 bonus for the student who will come down and present this work In addition provided their goals are met one student can implement this algorithm into their software package as a summer intern Homework slides serve to give you an impression what was done last time but I expect that we will significantly deviate from that Also homework will be updated as we go Book Book chapters can be found in this password protected directory week 1 ROC read sections 2 1 2 2 2 3 from chapter 2 of David MacKay s book read chapter 2 5 until plates 13 from Mike Jordan s book read classnotes Excercises HW1 week 2 read chapter 6 7 from Mike Jordan s book Excercises HW2 only the relevant ones on topics we have treated in class Project 1 due May 4 week 3 read chapter 9 19 2 from Mike Jordan s book Excercises HW3 Excercises HW4 only the relevant ones on topics we have treated in class At this point homework is optional but instructive week 4 5 6 read chapters 1 11 14 from Jordan s book read the following classnotes classnotes EM classnotes PPCA FA ICA start on Project 2 due Tu June 6 stuff below this line is not updated week 7 belief propagation junction trees week 8 classnote Sampling week 9 classnotes HMM classnotes KF week 1 presentation projects week 11 final exam MATLAB Demos week 1 demo Bayes demo MAP demo ML plotGauss1D plotGauss2D ginput2 week 2 demo LinReg demo LogReg week5 demo EM week6 MoG demo plotGauss color randMean randCovariance kmeans dist2 randvec gaussian week7 demo pca FA week8 demo gibbs demo mcmc week9 demo HMM demo KF demo KF2 SciTech Dataset training data mat This files includes Training labels inactive compound 1 medium active compound 2 active compound sparse format Continuous attributes 2 continuous attributes AlogP and Molecular Weight Discrete attributes 3 discrete attributes Num H Acceptors Num H Donors Num RotatableBonds Binary finger print very sparse binary matrix where 1 s code for the presence of certain substructures sparse format Using all the available attributes we wish to predict the activity level of the compound Background Reading paper1 paper2 SciTech powerpoint slides Syllabus The course will primarily be lecture based with homework and exams Most homework will revolve around the implementation of various classification algorithms on the SciTech dataset provided above It is required that you use MATLAB for this coding work The following is a rough syllabus subject to change 1 Review of Statistical Concepts Random variables probability distributions and probability densities The multivariate Gaussian distribution Marginal and conditional independence Bayes rule Estimation maximum likelihood MAP estimates Bayesian inference bias variance tradeoff Model selection and averaging over fitting 2 Graphical Models Markov random fields and undirected graphical models Bayesian networks and directed acyclic graphical models Semantics of graphical models independence assumptions Markov properties Markov blanket separability Factor graphs chain graphs Plates 3 Hidden Variables and Exact Inference Observed and hidden random variables Bayes ball algorithm Exact inference junction tree propagation and cut set conditioning 4 Learning in Graphical Models The expectation maximization algorithm and free energy minimization Iterative conditional modes Iterative scaling 5 Unsupervised Learning Directed Graphical Models Mixture of Gaussians K means principal components analysis probabilistic principal components analysis factor analysis independent components analysis latent Dirichlet allocation 6 Unsupervised Learning Undirected Graphical Models Boltzmann machines products of experts additive random field models Examples in vision and text 7 Supervised Learning Directed and Undirected Graphical Models Naive Bayes as a graphical model logistic regression linear regression Conditional mixture models mixtures of experts Conditional random fields 8 Graphical Models of Time Series State space models autoregressive models Hidden Markov Models The Baum Welch and Viterbi algorithm The Kalman filter and smoother Dynamic Bayes nets Examples in speech and biological sequence data 9 Approximate Inference Mean field methods and structured variational inference Loopy belief propagation Region graphs and generalized belief propagation Sampling rejection sampling importance sampling particle filters Markov chain Monte Carlo sampling Gibbs sampling Hybrid Monte Carlo sampling 1 Bayesian Learning and Structure Learning in Graphical Models Conjugate priors Fully observed Bayes nets Variational Bayes algorithm Sampling from the posterior Laplace approximation Chow Liu s algorithm for trees Structure learning in fully observed Bayes nets Structure learning in the presence of hidden variables structural EM Grading Criteria Grading will be based on a combination of weekly homework and a project 4 of the grade a midterm exam 3 and a final exam 3 Textbook The textbook that will be used for this course has not been published yet but copies will distributed during class 1 M I Jordan An Introduction to Graphical Models Optional side readings are 2 D MacKay Information Theory Inference and Learning Algorithms 3 M I Jordan Learning in Graphical Models 4 B Frey Graphical Models for Machine Learning and Digital Communication 5 J Pearl Probabilistic Reasoning in Intelligent Systems 6 R O Duda P E Hart D Stork Pattern Classification 7 C M Bishop Neural Networks for Pattern Recognition 8 T Hastie R Tibshirani J H Friedman The Elements of Statistical Learning 9 B D Ripley Pattern Recognition and Neural Networks", "_id": "http://www.ics.uci.edu/~welling/teaching/ICS274Bspring06/GraphicalModels.html", "title": "untitled document", "html": "<html xmlns:v=\"urn:schemas-microsoft-com:vml\"\nxmlns:o=\"urn:schemas-microsoft-com:office:office\"\nxmlns:w=\"urn:schemas-microsoft-com:office:word\"\nxmlns:st1=\"urn:schemas-microsoft-com:office:smarttags\"\nxmlns=\"http://www.w3.org/TR/REC-html40\">\n\n<head>\n<meta http-equiv=Content-Type content=\"text/html; charset=us-ascii\">\n<meta name=ProgId content=Word.Document>\n<meta name=Generator content=\"Microsoft Word 11\">\n<meta name=Originator content=\"Microsoft Word 11\">\n<link rel=File-List href=\"GraphicalModels_files/filelist.xml\">\n<link rel=Edit-Time-Data href=\"GraphicalModels_files/editdata.mso\">\n<!--[if !mso]>\n<style>\nv\\:* {behavior:url(#default#VML);}\no\\:* {behavior:url(#default#VML);}\nw\\:* {behavior:url(#default#VML);}\n.shape {behavior:url(#default#VML);}\n</style>\n<![endif]-->\n<title>Untitled Document</title>\n<o:SmartTagType namespaceuri=\"urn:schemas-microsoft-com:office:smarttags\"\n name=\"City\"/>\n<o:SmartTagType namespaceuri=\"urn:schemas-microsoft-com:office:smarttags\"\n name=\"country-region\"/>\n<o:SmartTagType namespaceuri=\"urn:schemas-microsoft-com:office:smarttags\"\n name=\"place\"/>\n<!--[if gte mso 9]><xml>\n <o:DocumentProperties>\n  <o:Author>Welling</o:Author>\n  <o:Template>Normal</o:Template>\n  <o:LastAuthor>Welling</o:LastAuthor>\n  <o:Revision>12</o:Revision>\n  <o:TotalTime>94</o:TotalTime>\n  <o:Created>2006-03-31T23:49:00Z</o:Created>\n  <o:LastSaved>2006-05-17T23:31:00Z</o:LastSaved>\n  <o:Pages>1</o:Pages>\n  <o:Words>1348</o:Words>\n  <o:Characters>7687</o:Characters>\n  <o:Company> UCI</o:Company>\n  <o:Lines>64</o:Lines>\n  <o:Paragraphs>18</o:Paragraphs>\n  <o:CharactersWithSpaces>9017</o:CharactersWithSpaces>\n  <o:Version>11.5606</o:Version>\n </o:DocumentProperties>\n</xml><![endif]--><!--[if gte mso 9]><xml>\n <w:WordDocument>\n  <w:Zoom>125</w:Zoom>\n  <w:ValidateAgainstSchemas/>\n  <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid>\n  <w:IgnoreMixedContent>false</w:IgnoreMixedContent>\n  <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText>\n  <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel>\n </w:WordDocument>\n</xml><![endif]--><!--[if gte mso 9]><xml>\n <w:LatentStyles DefLockedState=\"false\" LatentStyleCount=\"156\">\n </w:LatentStyles>\n</xml><![endif]--><!--[if !mso]><object\n classid=\"clsid:38481807-CA0E-42D2-BF39-B33AF135CC4D\" id=ieooui></object>\n<style>\nst1\\:*{behavior:url(#ieooui) }\n</style>\n<![endif]-->\n<style>\n<!--\n /* Font Definitions */\n @font-face\n\t{font-family:Tahoma;\n\tpanose-1:2 11 6 4 3 5 4 4 2 4;\n\tmso-font-charset:0;\n\tmso-generic-font-family:swiss;\n\tmso-font-pitch:variable;\n\tmso-font-signature:1627421319 -2147483648 8 0 66047 0;}\n /* Style Definitions */\n p.MsoNormal, li.MsoNormal, div.MsoNormal\n\t{mso-style-parent:\"\";\n\tmargin:0in;\n\tmargin-bottom:.0001pt;\n\tmso-pagination:widow-orphan;\n\tfont-size:12.0pt;\n\tfont-family:\"Times New Roman\";\n\tmso-fareast-font-family:\"Times New Roman\";}\nh2\n\t{mso-margin-top-alt:auto;\n\tmargin-right:0in;\n\tmso-margin-bottom-alt:auto;\n\tmargin-left:0in;\n\tmso-pagination:widow-orphan;\n\tmso-outline-level:2;\n\tfont-size:18.0pt;\n\tfont-family:\"Times New Roman\";}\na:link, span.MsoHyperlink\n\t{color:blue;\n\ttext-decoration:underline;\n\ttext-underline:single;}\na:visited, span.MsoHyperlinkFollowed\n\t{color:blue;\n\ttext-decoration:underline;\n\ttext-underline:single;}\n@page Section1\n\t{size:8.5in 11.0in;\n\tmargin:1.0in 1.25in 1.0in 1.25in;\n\tmso-header-margin:.5in;\n\tmso-footer-margin:.5in;\n\tmso-paper-source:0;}\ndiv.Section1\n\t{page:Section1;}\n-->\n</style>\n<!--[if gte mso 10]>\n<style>\n /* Style Definitions */\n table.MsoNormalTable\n\t{mso-style-name:\"Table Normal\";\n\tmso-tstyle-rowband-size:0;\n\tmso-tstyle-colband-size:0;\n\tmso-style-noshow:yes;\n\tmso-style-parent:\"\";\n\tmso-padding-alt:0in 5.4pt 0in 5.4pt;\n\tmso-para-margin:0in;\n\tmso-para-margin-bottom:.0001pt;\n\tmso-pagination:widow-orphan;\n\tfont-size:10.0pt;\n\tfont-family:\"Times New Roman\";\n\tmso-ansi-language:#0400;\n\tmso-fareast-language:#0400;\n\tmso-bidi-language:#0400;}\n</style>\n<![endif]--><!--[if gte mso 9]><xml>\n <o:shapedefaults v:ext=\"edit\" spidmax=\"9218\"/>\n</xml><![endif]--><!--[if gte mso 9]><xml>\n <o:shapelayout v:ext=\"edit\">\n  <o:idmap v:ext=\"edit\" data=\"1\"/>\n </o:shapelayout></xml><![endif]-->\n</head>\n\n<body bgcolor=\"#CCCCCC\" background=\"../../background.gif\" lang=EN-US link=blue\nvlink=blue style='tab-interval:.5in'>\n\n<div class=Section1>\n\n<h2><span style='font-size:10.0pt;font-family:Tahoma;color:red'>Learning in\nGraphical Models &#8211; spring 2006</span><span style='font-size:10.0pt'><o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt'>ICS: 274B<br>\nInstructor: Max Welling</span><span style='color:black'>&nbsp;</span><span\nstyle='font-size:10.0pt'><o:p></o:p></span></h2>\n\n<table class=MsoNormalTable border=0 cellspacing=0 cellpadding=0 width=311\n style='width:233.1pt;mso-cellspacing:0in;mso-padding-alt:2.25pt 2.25pt 2.25pt 2.25pt'>\n <tr style='mso-yfti-irow:0;mso-yfti-firstrow:yes'>\n  <td style='background:#E7E7E7;padding:2.25pt 2.25pt 2.25pt 2.25pt'>\n  <p class=MsoNormal align=center style='text-align:center'><b><span\n  style='font-size:8.5pt;color:black'>Code<o:p></o:p></span></b></p>\n  </td>\n  <td style='background:#E7E7E7;padding:2.25pt 2.25pt 2.25pt 2.25pt'>\n  <p class=MsoNormal align=center style='text-align:center'><b><span\n  style='font-size:8.5pt;color:black'>Typ<o:p></o:p></span></b></p>\n  </td>\n  <td style='background:#E7E7E7;padding:2.25pt 2.25pt 2.25pt 2.25pt'>\n  <p class=MsoNormal align=center style='text-align:center'><b><span\n  style='font-size:8.5pt;color:black'>Sec<o:p></o:p></span></b></p>\n  </td>\n  <td style='background:#E7E7E7;padding:2.25pt 2.25pt 2.25pt 2.25pt'>\n  <p class=MsoNormal align=center style='text-align:center'><b><span\n  style='font-size:8.5pt;color:black'>Unt<o:p></o:p></span></b></p>\n  </td>\n  <td style='background:#E7E7E7;padding:2.25pt 2.25pt 2.25pt 2.25pt'>\n  <p class=MsoNormal align=center style='text-align:center'><b><span\n  style='font-size:8.5pt;color:black'>Instructor<o:p></o:p></span></b></p>\n  </td>\n  <td style='background:#E7E7E7;padding:2.25pt 2.25pt 2.25pt 2.25pt'>\n  <p class=MsoNormal align=center style='text-align:center'><b><span\n  style='font-size:8.5pt;color:black'>Time<o:p></o:p></span></b></p>\n  </td>\n  <td style='background:#E7E7E7;padding:2.25pt 2.25pt 2.25pt 2.25pt'>\n  <p class=MsoNormal align=center style='text-align:center'><b><span\n  style='font-size:8.5pt;color:black'>Place<o:p></o:p></span></b></p>\n  </td>\n </tr>\n <tr style='mso-yfti-irow:1;mso-yfti-lastrow:yes'>\n  <td nowrap valign=top style='background:#D5E5FF;padding:2.25pt 2.25pt 2.25pt 2.25pt'>\n  <p class=MsoNormal><span style='font-size:8.5pt;color:black'>36745<o:p></o:p></span></p>\n  </td>\n  <td nowrap valign=top style='background:#FFFFCC;padding:2.25pt 2.25pt 2.25pt 2.25pt'>\n  <p class=MsoNormal><span style='font-size:8.5pt;color:black'>Lec<o:p></o:p></span></p>\n  </td>\n  <td nowrap valign=top style='background:#D5E5FF;padding:2.25pt 2.25pt 2.25pt 2.25pt'>\n  <p class=MsoNormal><span style='font-size:8.5pt;color:black'>A<o:p></o:p></span></p>\n  </td>\n  <td nowrap valign=top style='background:#FFFFCC;padding:2.25pt 2.25pt 2.25pt 2.25pt'>\n  <p class=MsoNormal><span style='font-size:8.5pt;color:black'>4<o:p></o:p></span></p>\n  </td>\n  <td nowrap valign=top style='background:#D5E5FF;padding:2.25pt 2.25pt 2.25pt 2.25pt'>\n  <p class=MsoNormal><span style='font-size:8.5pt;color:black'>WELLING, M.<o:p></o:p></span></p>\n  </td>\n  <td nowrap valign=top style='background:#FFFFCC;padding:2.25pt 2.25pt 2.25pt 2.25pt'>\n  <p class=MsoNormal><span style='font-size:8.5pt;color:black'>TuTh &nbsp;\n  2:00- 3:20p<o:p></o:p></span></p>\n  </td>\n  <td nowrap valign=top style='background:#D5E5FF;padding:2.25pt 2.25pt 2.25pt 2.25pt'>\n  <p class=MsoNormal><span style='font-size:8.5pt;color:black'>CS 213<o:p></o:p></span></p>\n  </td>\n </tr>\n</table>\n\n<h2><span style='font-size:10.0pt'>\n\n<hr size=2 width=\"100%\" align=left>\n\n</span></h2>\n\n<h2><span style='font-size:10.0pt;font-family:Arial;color:red'>Prerequisites</span><span\nstyle='font-size:10.0pt'><o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt'>ICS 274A<span\nstyle='mso-spacerun:yes'>&nbsp; </span>Probabilistic Learning: Theory<br>\nand Algorithms, or with consent of instructor.<o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt'>\n\n<hr size=2 width=\"100%\" align=left>\n\n</span></h2>\n\n<h2><span style='font-size:10.0pt;font-family:Arial;color:red'>Goals</span><span\nstyle='font-size:10.0pt'><o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt;color:black'>Many modern approaches to\nprobabilistic modeling of real world<br>\ndata sets can be formulated in the unifying framework of graphical<br>\nmodels. Graphical models provide a common language to think and<br>\ncommunicate about probabilistic models and makes explicit the<br>\nunderlying assumptions. Moreover, it provides the appropriate<br>\nstructure for computations necessary for inference and learning in<br>\nthese models.</span><span style='font-size:10.0pt'><o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt;color:black'>It is the primary goal of this\ncourse to familiarize the student<br>\nwith the concepts of graphical models, and in particular with<br>\nlearning these models from data. A student who has successfully<br>\ncompleted the course should be able to understand a wide variety<br>\nof well known models in terms of this unifying framework and feel<br>\ncomfortable using it to design new models. The course will contain<br>\n1) formal mathematical sections necessary for the development of<br>\nthe theory, 2) examples of probabilistic models (re)formulated in<br>\nthe language of graphical models and 3) examples of successful<br>\napplications to real data.<o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt'>This secondary goal of this class is to give\nthe <br>\nstudents hands on experience in solving real world problems.<br>\nFor that purpose I have negotiated a deal with SciTech, a San Diego based\ncompany:<br>\nif we improve their (naive Bayes) classifier on a particular classification\nproblem <br>\n(prediction of activity levels of chemical compounds - for data, see below)\nthen they <br>\nwill provide a $300 bonus for the student who will come down and present this\nwork.<br>\nIn addition, provided their goals are met one student can implement this\nalgorithm into <br>\ntheir software package as a summer intern.<o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt;font-family:Arial;color:black'>\n\n<hr size=2 width=\"100%\" align=left>\n\n</span></h2>\n\n<h2><span style='font-size:10.0pt;font-family:Arial;color:red'>Homework : </span><span\nstyle='font-size:10.0pt;font-family:Arial;color:black'>(slides serve to give\nyou an impression what was done last time, but I expect<br>\n<span\nstyle='mso-spacerun:yes'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n</span>that we will significantly deviate from that. Also, homework will be\nupdated as we go.)<o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt;font-family:Arial;color:red'>Book: </span><span\nstyle='font-size:10.0pt;font-family:Arial'>Book-chapters can be found in this <a\nhref=\"Jordan_book\">password protected directory</a></span><span\nstyle='font-size:8.0pt'><o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt;color:blue'>week 1:</span><span\nstyle='font-size:10.0pt;color:red'> </span><span style='font-size:10.0pt'><a\nhref=\"ROC.ppt\">ROC</a><br>\n<span style='color:black'>- read sections 2.1, 2.2, 2.3 from </span><a\nhref=\"22.40.pdf\">chapter 2</a><span style='color:black'> of David MacKay's\nbook.</span><br>\n<span style='color:black'>- read chapter 2, 5 (until &quot;plates&quot;) &amp;\n13 from Mike Jordan's book.</span><br>\n<span style='color:black'>- read </span><a href=\"Estimation.pdf\">classnotes</a><br>\n<span style='color:black'>- </span><a href=\"HOMEWORK/HW1.PDF\">Excercises HW1</a><span\nstyle='color:black'> </span><o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt;color:blue'>week 2:</span><span\nstyle='font-size:10.0pt;color:red'> </span><span style='font-size:10.0pt'><br>\n<span style='color:black'>- read chapter 6, 7 from Mike Jordan's book.</span><br>\n<span style='color:black'>- </span><a href=\"HOMEWORK/HW2.pdf\">Excercises HW2</a><span\nstyle='color:black'> </span></span><span style='font-size:8.0pt;color:black'>(only\nthe relevant ones on topics we have treated in class)</span><span\nstyle='font-size:10.0pt;color:black'><br>\n- <a href=\"HOMEWORK/Project%201.doc\">Project 1</a> (due May 4)<o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt'>&nbsp;<span style='color:blue'>week 3: </span><br>\n<span style='color:black'>- read chapter 9, 19, 20 from Mike Jordan's book.</span><br>\n<span style='color:black'>- </span><a href=\"HOMEWORK/HW3.pdf\">Excercises HW3</a><span\nstyle='color:black'> ,<span style='mso-spacerun:yes'>&nbsp; </span></span><a\nhref=\"HOMEWORK/HW4.pdf\">Excercises HW4</a> <span style='color:black'><span\nstyle='mso-spacerun:yes'>&nbsp;</span><br>\n</span></span><span style='font-size:8.0pt;color:black'>(only the relevant ones\non topics we have treated in class. At this point homework is optional but\ninstructive. )</span><span style='font-size:10.0pt;color:black'><br>\n<br>\n</span><span style='font-size:10.0pt;color:blue'>week 4,5,6:<br>\n</span><span style='font-size:10.0pt'>- read chapters 10,11,14 from <st1:place\nw:st=\"on\"><st1:country-region w:st=\"on\">Jordan</st1:country-region></st1:place>&#8217;s\nbook.<br>\n- read the following classnotes: <a href=\"EM.pdf\">classnotes (EM)</a> , <a\nhref=\"PCA.ps\">classnotes (PPCA, FA, ICA)</a> <br>\n- start on <a href=\"HOMEWORK/Project%202.doc\">Project 2</a> (due Tu June 6)<o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt;color:black'>--------------------------------------------------------------------------------------<br>\n</span><span style='font-size:10.0pt;color:red'>(stuff below this line is not\nupdated)<o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt;color:blue'><span\nstyle='mso-spacerun:yes'>&nbsp;</span>week 7: belief propagation, junction\ntrees.</span><span style='font-size:10.0pt;color:black'><o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt'>&nbsp;&nbsp;<span style='color:blue'>week 8:\n</span><a href=\"Approx.ps\">classnote (Sampling)</a><br style='mso-special-character:\nline-break'>\n<![if !supportLineBreakNewLine]><br style='mso-special-character:line-break'>\n<![endif]><o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt'>&nbsp;<span style='color:blue'>week 9:</span><span\nstyle='color:black'> </span><a href=\"HMM.ps\">classnotes(HMM)</a><span\nstyle='color:black'> </span><a href=\"KalmanFilter.ps\">classnotes(KF)</a><br\nstyle='mso-special-character:line-break'>\n<![if !supportLineBreakNewLine]><br style='mso-special-character:line-break'>\n<![endif]><o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt;color:blue'>week 10:</span><span\nstyle='font-size:10.0pt'><br>\n<span style='color:black'>- presentation projects:</span> <br style='mso-special-character:\nline-break'>\n<![if !supportLineBreakNewLine]><br style='mso-special-character:line-break'>\n<![endif]><o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt'>&nbsp;<span style='color:blue'>week 11:</span><span\nstyle='color:black'> </span><span style='color:red'>final exam:</span><span\nstyle='color:black'> </span><o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt'>&nbsp;<span style='color:blue'><o:p></o:p></span></span></h2>\n\n<h2><span style='font-size:10.0pt;color:blue'>\n\n<hr size=2 width=\"100%\" align=left>\n\n</span></h2>\n\n<h2><span style='font-size:10.0pt;font-family:Arial;color:red'>MATLAB Demos:</span><span\nstyle='font-size:10.0pt'><o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt;color:blue'>week 1: </span><span\nstyle='font-size:10.0pt'><a href=\"demo_Bayes.m\">demo_Bayes</a><span\nstyle='color:black'>, </span><a href=\"demo_MAP.m\">demo_MAP</a><span\nstyle='color:black'>, </span><a href=\"demo_ML.m\">demo_ML</a><span\nstyle='color:black'>, </span><a href=\"plotGauss1D.m\">plotGauss1D</a><span\nstyle='color:black'>, </span><a href=\"plotGauss.m\">plotGauss2D, </a><a\nhref=\"ginput2.m\">ginput2</a><br>\n<span style='color:blue'>week 2:</span><span style='color:black'> </span><a\nhref=\"demo_LinReg.m\">demo_LinReg</a><span style='color:black'>, </span><a\nhref=\"demo_LogReg.m\">demo_LogReg</a><br>\n<span style='color:blue'>week5:</span><a href=\"demo_EM.m\"> demo_EM</a><br>\n<span style='color:blue'>week6: </span><a href=\"MoG_demo.m\">MoG_demo</a><span\nstyle='color:blue'>, </span><a href=\"plotGauss_color.m\">plotGauss_color</a><span\nstyle='color:blue'>, </span><a href=\"randMean.m\">randMean</a><span\nstyle='color:blue'>, </span><a href=\"randCovariance.m\">randCovariance</a><span\nstyle='color:blue'>, </span><a href=\"kmeans.m\">kmeans</a><span\nstyle='color:blue'>, </span><a href=\"dist2.m\">dist2</a><span style='color:blue'>,\n</span><a href=\"randvec.m\">randvec</a><span style='color:blue'>, </span><a\nhref=\"gaussian.m\">gaussian</a><br>\n<span style='color:blue'>week7:</span> <a href=\"demo_pca.m\">demo_pca</a>, <a\nhref=\"FA.m\">FA</a><br>\n<span style='color:blue'>week8: </span><a href=\"demo_gibbs.m\">demo_gibbs</a>, <a\nhref=\"demo_mcmc.m\">demo_mcmc</a><br>\n<span style='color:blue'>week9: </span><a href=\"HMM_demo4.m\">demo_HMM </a><a\nhref=\"demo_KF.m\">demo_KF</a>, <a href=\"demo_KF2.m\">demo_KF2</a><o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt;font-family:Arial;color:black'>\n\n<hr size=2 width=\"100%\" align=left>\n\n</span></h2>\n\n<h2><span style='font-size:10.0pt;font-family:Arial;color:red'>SciTech Dataset:\n<o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt;color:#000099'><a href=\"training_data.mat\">training_data.mat</a><span\nstyle='mso-spacerun:yes'>&nbsp; </span></span><span style='font-size:10.0pt'>This\nfiles includes:<span style='color:#000099'><o:p></o:p></span></span></h2>\n\n<h2><span style='font-size:10.0pt;color:#000099'>Training labels: </span><span\nstyle='font-size:10.0pt'>0: inactive compound, 1: medium active compound, 2:\nactive compound, sparse format.<o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt;color:#000099'>Continuous attributes: </span><span\nstyle='font-size:10.0pt'>2 continuous attributes: AlogP and Molecular_Weight<o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt;color:#000099'>Discrete attributes: </span><span\nstyle='font-size:10.0pt'>3 discrete attributes: Num_H_Acceptors,<span\nstyle='mso-spacerun:yes'>&nbsp; </span>Num_H_Donors, Num_RotatableBonds.<o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt;color:#000099'>Binary finger print: </span><span\nstyle='font-size:10.0pt'>very sparse binary matrix where <span\nstyle='color:#000099'><span style='mso-spacerun:yes'>&nbsp;</span></span>1&#8217;s\ncode for the presence of certain substructures, sparse format.<o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt'>Using all the available attributes we wish\nto predict the activity level of the compound.<br>\nBackground <st1:place w:st=\"on\"><st1:City w:st=\"on\">Reading</st1:City></st1:place>:\n<a href=\"David%20Rogers%20-%20ECFP%20Manuscript.doc\">paper1</a>,<span\nstyle='mso-spacerun:yes'>&nbsp; </span><a href=\"Amgen-Bayesian.pdf\">paper2</a>,<span\nstyle='mso-spacerun:yes'>&nbsp; </span><a href=\"NaiveBayesian.ppt\">SciTech\npowerpoint slides</a>.<o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt;font-family:Arial;color:black'>\n\n<hr size=2 width=\"100%\" align=left>\n\n</span></h2>\n\n<h2><span style='font-size:10.0pt;font-family:Arial;color:red'>Syllabus:</span><span\nstyle='font-size:10.0pt'><o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt'>The course will primarily be lecture-based\nwith homework and<br>\nexams. Most homework will revolve around the implementation of various<br>\nclassification algorithms on the SciTech dataset provided above.<br>\nIt is required that you use MATLAB for this coding work. <o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt'>The following is a rough syllabus subject to\nchange.<o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt;color:#000099'>1. Review of Statistical\nConcepts</span><span style='font-size:10.0pt'><o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt'>Random variables, probability distributions\nand<br>\nprobability densities. The multivariate Gaussian distribution.<br>\nMarginal and conditional independence. Bayes' rule. Estimation:<br>\nmaximum likelihood, MAP-estimates, Bayesian inference,<br>\nbias-variance tradeoff. Model selection and averaging,<br>\nover-fitting.<o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt;color:#000099'>2. Graphical Models.</span><span\nstyle='font-size:10.0pt'><o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt'>Markov random fields and undirected<br>\ngraphical models. Bayesian networks and directed acyclic graphical<br>\nmodels. Semantics of graphical models: independence assumptions,<br>\nMarkov properties, Markov blanket, separability. Factor graphs,<br>\nchain graphs. Plates.<o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt;color:#000099'>3. Hidden Variables and Exact\nInference.</span><span style='font-size:10.0pt'><o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt'>Observed and hidden random variables. Bayes'\nball algorithm. Exact inference:<br>\njunction tree propagation and cut-set conditioning.<o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt;color:#000099'>4. Learning in Graphical\nModels.</span><span style='font-size:10.0pt'><o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt'>The expectation<br>\nmaximization algorithm and free energy minimization. Iterative<br>\nconditional modes. Iterative scaling.<o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt;color:#000099'>5. Unsupervised Learning -\nDirected Graphical Models.</span><span style='font-size:10.0pt'><o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt'>Mixture of Gaussians, K-means, principal\ncomponents analysis,<br>\nprobabilistic principal components analysis, factor analysis,<br>\nindependent components analysis, latent Dirichlet allocation.<o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt;color:#000099'>6. Unsupervised Learning -\nUndirected Graphical Models.</span><span style='font-size:10.0pt'><o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt'>Boltzmann machines, products of experts,\nadditive random field<br>\nmodels. Examples in vision and text.<o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt;color:#000099'>7. Supervised Learning -\nDirected and Undirected Graphical Models.</span><span style='font-size:10.0pt'><o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt'>Naive Bayes as a graphical model,<br>\nlogistic regression, linear regression.<br>\nConditional mixture models, mixtures of experts. Conditional<br>\nrandom fields.<o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt;color:#000099'>8. Graphical Models of Time\nSeries.</span><span style='font-size:10.0pt'><o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt'>State space models, autoregressive models.<br>\nHidden Markov Models. The Baum-Welch and Viterbi algorithm. The<br>\nKalman filter and smoother. Dynamic Bayes nets. Examples in speech<br>\nand biological sequence data.<o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt;color:#000099'>9. Approximate Inference.</span><span\nstyle='font-size:10.0pt'><o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt'>Mean field methods and<br>\nstructured variational inference. Loopy belief propagation. Region<br>\ngraphs and generalized belief propagation. Sampling: rejection<br>\nsampling, importance sampling, particle filters, Markov chain<br>\nMonte Carlo sampling, Gibbs sampling, Hybrid <st1:place w:st=\"on\">Monte Carlo</st1:place>\nsampling.<o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt;color:#000099'>10. Bayesian Learning and\nStructure Learning in Graphical Models.</span><span style='font-size:10.0pt'><o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt'>Conjugate priors.<br>\nFully observed Bayes' nets. Variational Bayes algorithm. Sampling<br>\nfrom the posterior. <st1:place w:st=\"on\">Laplace</st1:place> approximation.\nChow-Liu's algorithm<br>\nfor trees. Structure learning in fully observed Bayes' nets.<br>\nStructure learning in the presence of hidden variables: structural<br>\nEM.<o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt'>\n\n<hr size=2 width=\"100%\" align=left>\n\n</span></h2>\n\n<h2><span style='font-size:10.0pt;font-family:Arial;color:red'>Grading Criteria</span><span\nstyle='font-size:10.0pt'><o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt;color:black'><br>\nGrading will be based on a combination of weekly homework and a project (40%<br>\nof the grade), a midterm exam (30%) and a final exam (30%) .</span><span\nstyle='font-size:10.0pt'><o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt;color:black'>\n\n<hr size=2 width=\"100%\" align=left>\n\n</span></h2>\n\n<h2><span style='font-size:10.0pt;font-family:Arial;color:red'>Textbook</span><span\nstyle='font-size:10.0pt'><o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt;color:#000099'><br>\nThe textbook that will be used for this course has not been<br>\npublished yet, but copies will distributed during class.</span><span\nstyle='font-size:10.0pt'><o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt;color:black'>1. M.I. Jordan: An Introduction\nto Graphical Models.</span><span style='font-size:10.0pt'><o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt;color:#000099'><br>\nOptional side readings are:</span><span style='font-size:10.0pt'><o:p></o:p></span></h2>\n\n<h2><span style='font-size:10.0pt;color:black'>2. D. MacKay: Information\nTheory, Inference and Learning<br>\nAlgorithms<br>\n3. M.I. Jordan: Learning in Graphical Models<br>\n4. B. Frey: Graphical Models for Machine Learning and Digital<br>\nCommunication<br>\n5. J. Pearl: Probabilistic Reasoning in Intelligent<br>\nSystems<br>\n6. R.O. Duda, P.E. Hart, D. Stork: Pattern<br>\nClassification<br>\n7. C.M. Bishop: Neural Networks for Pattern Recognition<br>\n8. T. Hastie, R. Tibshirani, J.H, Friedman: The Elements of<br>\nStatistical Learning<br>\n9. B.D. Ripley: Pattern Recognition and Neural Networks</span></h2>\n\n</div>\n\n</body>\n\n</html>\n", "id": 2734.0}