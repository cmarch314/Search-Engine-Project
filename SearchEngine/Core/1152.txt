{"text": "Data Compression 2 SEMANTIC DEPENDENT METHODS Semantic dependent data compression techniques are designed to respond to specific types of local redundancy occurring in certain applications One area in which data compression is of great importance is image representation and processing There are two major reasons for this The first is that digitized images contain a large amount of local redundancy An image is usually captured in the form of an array of pixels whereas methods which exploit the tendency for pixels of like color or intensity to cluster together may be more efficient The second reason for the abundance of research in this area is volume Digital images usually require a very large number of bits and many uses of digital images involve large collections of images One technique used for compression of image data is run length encoding In a common version of run length encoding the sequence of image elements along a scan line row is mapped into a sequence of pairs c l where c represents an intensity or color and l the length of the run sequence of pixels of equal intensity For pictures such as weather maps run length encoding can save a significant number of bits over the image element sequence Gonzalez and Wintz 1977 Another data compression technique specific to the area of image data is difference mapping in which the image is represented as an array of differences in brightness or color between adjacent pixels rather than the brightness values themselves Difference mapping was used to encode the pictures of Uranus transmitted by Voyager 2 The 8 bits per pixel needed to represent 256 brightness levels was reduced to an average of 3 bits per pixel when difference values were transmitted Laeser et al 1986 In spacecraft applications image fidelity is a major concern due to the effect of the distance from the spacecraft to earth on transmission reliability Difference mapping was combined with error correcting codes to provide both compression and data integrity in the Voyager project Another method which takes advantage of the tendency for images to contain large areas of constant intensity is the use of the quadtree data structure Samet 1984 Additional examples of coding techniques used in image processing can be found in Wilkins and Wintz and in Cappellini Wilkins and Wintz 1971 Cappellini 1985 Data compression is of interest in business data processing both because of the cost savings it offers and because of the large volume of data manipulated in many business applications The types of local redundancy present in business data files include runs of zeros in numeric fields sequences of blanks in alphanumeric fields and fields which are present in some records and null in others Run length encoding can be used to compress sequences of zeros or blanks Null suppression may be accomplished through the use of presence bits Ruth and Kreutzer 1972 Another class of methods exploits cases in which only a limited set of attribute values exist Dictionary substitution entails replacing alphanumeric representations of information such as bank account type insurance policy type sex month etc by the few bits necessary to represent the limited number of possible attribute values Reghbati 1981 Cormack describes a data compression system which is designed for use with database files Cormack 1985 The method which is part of IBM s Information Management System IMS compresses individual records and is invoked each time a record is stored in the database file expansion is performed each time a record is retrieved Since records may be retrieved in any order context information used by the compression routine is limited to a single record In order for the routine to be applicable to any database it must be able to adapt to the format of the record The fact that database records are usually heterogeneous collections of small fields indicates that the local properties of the data are more important than its global characteristics The compression routine in IMS is a hybrid method which attacks this local redundancy by using different coding schemes for different types of fields The identified field types in IMS are letters of the alphabet numeric digits packed decimal digit pairs blank and other When compression begins a default code is used to encode the first character of the record For each subsequent character the type of the previous character determines the code to be used For example if the record 187 ABCD LMN were encoded with the letter code as default the leading zero would be coded using the letter code the 1 8 7 and the first blank would be coded by the numeric code The A would be coded by the blank code B C D and the next blank by the letter code the next blank and the L by the blank code and the M and N by the letter code Clearly each code must define a codeword for every character the letter code would assign the shortest codewords to letters the numeric code would favor the digits etc In the system Cormack describes the types of the characters are stored in the encode decode data structures When a character c is received the decoder checks type c to detect which code table will be used in transmitting the next character The compression algorithm might be more efficient if a special bit string were used to alert the receiver to a change in code table Particularly if fields were reasonably long decoding would be more rapid and the extra bits in the transmission would not be excessive Cormack reports that the performance of the IMS compression routines is very good at least fifty sites are currently using the system He cites a case of a database containing student records whose size was reduced by 42 1 and as a side effect the number of disk operations required to load the database was reduced by 32 7 Cormack 1985 A variety of approaches to data compression designed with text files in mind include use of a dictionary either representing all of the words in the file so that the file itself is coded as a list of pointers to the dictionary Hahn 1974 or representing common words and word endings so that the file consists of pointers to the dictionary and encodings of the less common words Tropper 1982 Hand selection of common phrases Wagner 1973 programmed selection of prefixes and suffixes Fraenkel et al 1983 and programmed selection of common character pairs Snyderman and Hunt 197 Cortesi 1982 have also been investigated This discussion of semantic dependent data compression techniques represents a limited sample of a very large body of research These methods and others of a like nature are interesting and of great value in their intended domains Their obvious drawback lies in their limited utility It should be noted however that much of the efficiency gained through the use of semantic dependent techniques can be achieved through more general methods albeit to a lesser degree For example the dictionary approaches can be implemented through either Huffman coding Section 3 2 Section 4 or Lempel Ziv codes Section 5 1 Cormack s database scheme is a special case of the codebook approach Section 3 2 and run length encoding is one of the effects of Lempel Ziv codes ", "_id": "http://www.ics.uci.edu/~dan/pubs/DC-Sec2.html", "title": " data compression -- section 2 ", "html": "<HTML>\n<HEAD>\n<TITLE> Data Compression -- Section 2 </TITLE>\n</HEAD><BODY>\n\n<H1> Data Compression </H1>\n\n<a name=\"Sec_2\">\n<H2> 2.  SEMANTIC DEPENDENT METHODS</H2> </a>\n\n<A HREF=\"DC-Sec1.html\"><IMG SRC=\"prev.gif\" ALT=\"PREV section\"></A>\n<A HREF=\"DC-Sec3.html\"><IMG SRC=\"next.gif\" ALT=\"NEXT section\"></A>\n<P>\n\n\tSemantic dependent data compression techniques are \ndesigned to respond to specific types of local redundancy\noccurring in certain \napplications.  One area in which data compression is \nof great importance is image representation and processing.\nThere are two major reasons for this.  The first is that digitized\nimages contain a large amount of local redundancy.  An image is usually\ncaptured in the form of an array of pixels whereas methods which  \nexploit the tendency for pixels of like color or intensity to cluster\ntogether may be more efficient.  The second reason for the \nabundance of research in this area is volume.  Digital images \nusually require a very large number of bits, and many uses of digital\nimages involve large collections of images.\n<P>\n\tOne technique used for compression of image data is \n<EM>run length encoding</EM>.  In a common version of run length\nencoding, the sequence of image elements along a scan line (row)\nis mapped into a sequence of pairs (<VAR>c</VAR>,<VAR>l</VAR>)\nwhere <VAR>c</VAR> represents an intensity or color and <VAR>l</VAR>\nthe length of the run (sequence of pixels of equal intensity).\nFor pictures such as weather maps, run length encoding can save a\nsignificant number of bits over the image element sequence\n[Gonzalez and Wintz 1977].  Another data compression technique\nspecific to the area of image data is <EM>difference mapping</EM>, in which\nthe image is represented as an array of differences in\nbrightness (or color) between adjacent pixels rather than the\nbrightness values themselves.  Difference mapping was used\nto encode the pictures of Uranus transmitted by <EM>Voyager</EM> 2.  \nThe 8 bits per pixel needed to represent 256 brightness levels\nwas reduced to an average of 3 bits per pixel when difference\nvalues were transmitted [Laeser et al. 1986].  In spacecraft \napplications, image fidelity is a major concern due to the effect\nof the distance from the spacecraft to earth on transmission\nreliability.  Difference mapping was combined with error-correcting \ncodes to provide both compression and data integrity in the \n<EM>Voyager</EM> project.  Another method which takes advantage of the\ntendency for images to contain large areas of constant intensity\nis the use of the quadtree data structure [Samet 1984].\nAdditional examples of coding techniques used in image processing\ncan be found in Wilkins and Wintz and in Cappellini [Wilkins and Wintz 1971; Cappellini 1985].\n<P>\n\tData compression is of interest in business data processing,\nboth because of the cost savings it offers and because of the\nlarge volume of data manipulated in many business applications.\nThe types of local redundancy present in business data files include\nruns of zeros in numeric fields, sequences of blanks in alphanumeric\nfields, and fields which are present in some records and null in\nothers.  Run length encoding can be used to compress sequences of \nzeros or blanks.  Null suppression may be accomplished\nthrough the use of presence bits [Ruth and Kreutzer 1972].  \nAnother class of methods exploits cases\nin which only a limited set of attribute values exist.  <EM>Dictionary\nsubstitution</EM>  entails replacing alphanumeric representations of information\nsuch as bank account type, insurance policy type, sex, month, etc.  \nby the few bits necessary to represent the limited number of possible\nattribute values [Reghbati 1981].\n<P>\n\tCormack describes a data compression system which is\ndesigned for use with database files [Cormack 1985].  \nThe method, which is part of \nIBM's \"Information Management System\" (IMS), compresses\nindividual records and is invoked each time a record\nis stored in the database file; expansion is performed each\ntime a record is retrieved.  Since records may be retrieved\nin any order, context information used by the compression\nroutine is limited to a single record.  In order for\nthe routine to be applicable to any database, it must be able to\nadapt to the format of the record.  The fact that database records\nare usually heterogeneous collections of small fields indicates\nthat the local properties of the data are more important than\nits global characteristics.  The compression routine\nin IMS is a hybrid method which attacks this local redundancy \nby using different coding schemes for different types of fields.\nThe identified field types in IMS are <EM>letters of the alphabet, \nnumeric digits, packed decimal digit pairs, blank,</EM> and <EM>other</EM>.\nWhen compression begins, a default code is used to encode the \nfirst character of the record.  For each subsequent character,\nthe type of the previous character determines the code to be used.\nFor example, if the record <kbd>01870_ABCD__LMN</kbd> were encoded \nwith the <EM>letter</EM> code as default, the leading zero would be\ncoded using the <EM>letter</EM> code; the 1, 8, 7, 0 and the first\nblank (_) would be coded by the <EM>numeric</EM> code.  The <VAR>A</VAR> would be\ncoded by the <EM>blank</EM> code; <VAR>B</VAR>, <VAR>C</VAR>, <VAR>D</VAR>, and the next blank by the\n<EM>letter</EM> code; the next blank and the <VAR>L</VAR> by the <EM>blank</EM>\ncode; and the <VAR>M</VAR> and <VAR>N</VAR> by the <EM>letter</EM> code. \nClearly, each code must define a codeword for every\ncharacter; the <EM>letter</EM> code would assign the shortest codewords\nto letters, the numeric code would favor the digits, etc.  In the system\nCormack describes, the types of the characters are stored in the encode/decode\ndata structures.  When a character <VAR>c</VAR> is received, the decoder checks\n<VAR>type</VAR>(<VAR>c</VAR>) to detect which code table will be used in transmitting the\nnext character.   The compression algorithm might be more efficient if a special\nbit string were used to alert the receiver to a change in code table.\nParticularly if fields were reasonably long,  decoding would be more rapid\nand the extra bits in the transmission would not be excessive.\nCormack reports that the performance of the IMS compression routines \nis very good; at least fifty sites are currently using the system.\nHe cites a case of a database containing student records whose size\nwas reduced by 42.1%, and as a side effect the number of disk operations\nrequired to load the database was reduced by 32.7% [Cormack 1985]. \n<P>\n\tA variety of approaches to data compression designed with\ntext files in mind include use of a dictionary\neither representing  all of the words in the file so that the\nfile itself is coded as a list of pointers to the dictionary\n[Hahn 1974], or representing common words and word endings so that the\nfile consists of pointers to the dictionary and encodings\nof the less common words [Tropper 1982].  Hand-selection of\ncommon phrases [Wagner 1973], programmed selection of prefixes \nand suffixes [Fraenkel et al. 1983] and programmed selection of\ncommon character pairs [Snyderman and Hunt 1970; Cortesi 1982] \nhave also been investigated.\n<P>\n\tThis discussion of semantic dependent data compression\ntechniques represents a limited sample of a very large body of\nresearch.  These methods and others of a like nature are interesting\nand of great value in their intended domains.  Their obvious\ndrawback lies in their limited utility.  It should be noted, however,\nthat much of the efficiency gained through the use of semantic dependent\ntechniques can be achieved through more general methods, albeit to\na lesser degree.  For example, the dictionary approaches can be implemented\nthrough either Huffman coding (\n<a href=\"DC-Sec3.html#Sec_3.2\">Section 3.2</a>, <a href=\"DC-Sec4.html#Sec_4\">Section 4</a>) or Lempel-Ziv\ncodes (<a href=\"DC-Sec5.html#Sec_5.1\">Section 5.1</a>).  Cormack's database scheme is a special case of\nthe codebook approach (<a href=\"DC-Sec3.html#Sec_3.2\">Section 3.2</a>), and run length encoding is one\nof the effects of Lempel-Ziv codes. \n\n<P>\n<A HREF=\"DC-Sec1.html\"><IMG SRC=\"prev.gif\" ALT=\"PREV section\"></A>\n<A HREF=\"DC-Sec3.html\"><IMG SRC=\"next.gif\" ALT=\"NEXT section\"></A>\n<P>\n</BODY></HTML>\n", "id": 1152.0}