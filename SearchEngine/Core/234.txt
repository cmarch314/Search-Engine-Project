{"text": " Classes Group Research Publications Code login Classes CS178 Notes Matlab Classes Matlab Object Oriented Programming and Classes Matlab object oriented programming is a bit awkward and unlike most other OO languages but still has some of the elements that make OO programming useful Here I will describe some aspects of Matlab objects using a class to construct a k nearest neighbor classifier as a working example Getting started First download the course code into a directory say ihler Code cs178 You should see a number of files and directories created there for the purposes of this handout the relevant directory to have is the knnClassify directory which contains the files for the classifier You should make sure you are either in the directory you downloaded the files to or add it to your Matlab path For example use addpath ihler Code cs178 to add that directory to Matlab s path Note that you should not be in the knnClassify directory nor should you add it to the path only its parent directory We will use old style Matlab objects which consist of a directory something which uses one file per member function private functions go in a further subdirectory called private While there are some drawbacks to this type of object and Matlab has since updated their object representations to be more flexible and give them more capabilities this is the type that is also compatible with Octave which is useful also Using the class To create an object of the class type you can simply call the constructor To find out its usage use help help knnClassify knnClassify X Y create k nearest neighbors classifier takes no arguments or training data to be used in constructing the classifier alpha weighted average coefficient Gaussian weighting alpha simple average and we can then use it Xtr rand 5 3 Create feature matrix of 5 data points with 3 features each Xtr just in case you want to use the same numbers I do 4898 276 4984 4456 6797 9597 6463 6551 34 4 7 94 1626 5853 7547 119 2238 Ytr 1 1 and a corresponding Ytrain of target classes knn knnClassify Xtr Ytr 3 Create 3 nearest nbr classifier with those data KNN Classifier 2 classes K 3 Now to find out what methods are available for a given class we can use the methods command to list them methods knnClassify Methods for class knnClassify auc err predict setClasses confusion getClasses predictSoft setK display knnClassify roc train Our most typical operations will be train and predict which train a model on training data and predict the current model on new data respectively Typically when constructor functions accept training data they simply call the train function To see what parameters train takes we need to differentiate which train we mean help knnClassify train knn train knn Xtrain Ytrain Batch training for knn just memorize data Using this calling pattern we can re train the model with e g knn train knn Xtr Ytr and predict with Xte rand 2 3 Make two test data points same of features Xte 7513 5 6 89 9 2551 6991 9593 Yte 1 and some test target values for goot measure Yhat predict knn Xte Make prediction for those points Yhat Two points are worth noting First member functions are typically called by passing the object as the first argument of the function They can also be called in a more typical format knn train Xtr Ytr but both are implemented in exactly the same way Second functions that modify the class in some way such as train should actually return and set the object variable Matlab cannot generally update variables by reference recent object changes relax this point and so the object must be returned in order to modify it We can also use accessor functions to get or set object properties such as knn setK knn 1 Change to a 1 nearest nbr classifier KNN Classifier 2 classes K 1 Again we actually return the modified object variable and set kdd to be equal to the returned value The object constructor Let s take a closer look at how the constructor function knnClassify m works First here is the file header function obj knnClassify Xtr Ytr K knnClassify X Y K create k nearest neighbors classifier takes no arguments or training data to be used in constructing the classifier The first line declares the function itself and any returned variables The first set of comments in the file are output for the help command help knnClassify and should contain basic usage information Default values are a bit awkward typically you can check how many variables were passed in and fill in any missing ones positional defaults if nargin 3 K 1 end Another typical approach is to require that the caller pass an empty matrix which can be tested for and filled in with a default value The basic form of an object is simply a Matlab structure with a bit of extra gloss on top we declare the member variables as if it were a structure and then call a function to define it as a class obj K K obj Xtrain obj Ytrain obj classes obj class obj knnClassify I usually do this immediately with empty values since the variable fields must always be declared in the same order Note also that return values are specified by name so if obj is listed as the return variable whatever variable is called obj when the function ends is returned I typically also allow train to be called automatically by just passing the data into the constructor if nargin obj train obj Xtr Ytr obj setK obj K end Or these can be called manually after As an aside Matlab objects can be converted into structures allowing their internal data to be accessed by anyone s struct knn s K 1 Xtrain 5x3 double Ytrain 5x1 double classes 2x1 double This can be useful if you re trying to access something in an object while debugging but is usually non reversible i e it is difficult to modify s and transform it back to an object afterwards The train function Training for a k nearest neighbor classifier is trivial we simply memorize the data function obj train obj Xtr Ytr knn train knn Xtrain Ytrain Batch training for knn just memorize data obj Xtrain Xtr obj Ytrain Ytr obj classes unique Ytr One minor point I always keep a column vector obj classes in each classifier The internal implementation of the classifier predicts a positive integer and then returns obj classes c This way the class values can be non consecutive non standard and even of different Matlab types characters or whatever without any difficulty Some classifiers are implemented specifically for binary classification problems in which case we can simply check that the number of classes is only two The predict function In order to predict with a k nearest neighbor classifier we simply search the stored training data for the nearest points in terms of their sum of squared differences The file header function Yte predict obj Xte Yhat predict knn Xtest make a nearest neighbor prediction on test data Ntr Mtr size obj Xtrain get size of training test data Nte Mte size Xte K min obj K Ntr can t have more than Ntrain neighbors Yte repmat obj Ytrain 1 Nte 1 make Ytest the same data type as Ytrain gets the number of training and test data and their dimensions M which should be the same for both and makes sure K is valid We pre initialize Yte by copying repmat repeat matrix one of the training data to the correct size pre allocating the correct vector size helps Matlab avoid constantly re allocating memory for Yte and using repmat ensures it has the right variable type Next for each test data point we compute the distance from all training data for example for i 1 Nte For each test example dist zeros Ntr 1 pre allocate a distance vector for j 1 Ntr and compute distance from all Ntr training data dist j sum obj Xtrain j Xte i 2 end end However this turns out to be awfully slow Matlab is interpreted and often has trouble performing for loops quickly For better performance you may learn to vectorize your calculations performing them all in one step dist sum obj Xtrain repmat Xte i Ntr 1 2 2 This copies the Xte data point to be the same size as Xtrain then subtracts the two matrices squares the entries element wise and sums them over their 2nd dimension the features leaving a column vector of distances exactly like the for loop above Even harder to read but slightly faster still is to use the bsxfun function which performs operators on differently sized matrices so that you don t need to use repmat to copy the data point dist sum bsxfun minus obj Xtrain Xte i 2 2 All this is useful if you are finding Matlab very slow but takes a while to get used to Finally we find the K nearest data examples and find the majority vote among them dst idx sort dist find nearest neighbors over Xtrain idx idx 1 K keep nearest K data points nClasses length obj classes count zeros 1 nClasses count up how many in each class for c 1 nClasses count c sum obj Ytrain idx obj classes c end nil cMax max count find the position of the largest Yte i obj classes cMax and save the prediction A useful trick here both sort and max can return both the sorted maximum value first return value and the position of those values as the second return value So idx is a list of the training data points in order from nearest to farthest and cMax is the index of the class with the largest count value Also obj Ytrain idx obj classes c is a binary vector with 1 when the equality condition is satisified and if not Then sum counts up the number of 1 entries Measuring errors A few functions are common to almost all predictors for example frequently we want to evalute the error rate measuring how often our model makes incorrect predictions on a data set e g the training or validation error Functions like err do this easily J err knn Xte Yte evalute the empirical error rate on these data J 5 To get more information we may want to look at the confusion matrix C confusion knn Xte Yte evalute the confusion matrix C 1 one true class zero column predicted row 1 one true class zero column predicted 1 row Similar functions mse etc are found in regression classes Many classifiers also support soft predictions which express some level of confidence in the possible outcomes For example in kNN we might return the fraction of the K neighbors in each class rather than just the decision predictSoft returns a length nClasses vector of such confidences for each data point knn setK knn 3 poinless for k 1 ySoft predictSoft knn Xte make soft predictions ySoft 6667 3333 test point 1 has 66 confidence in class 1 test point 2 has 1 confidence in class These soft scores are useful in computing for example ROC curves note that this only works for binary classifications fpr tpr tnr roc knn Xte Yte find info for ROC curve plot fpr tpr not very interesting for these data though and the area under the curve can be computed with auc m Last modified January 5 2 15 at 12 6 PM Bren School of Information and Computer Science University of California Irvine", "_id": "http://sli.ics.uci.edu/Classes-CS178-Notes/Matlab-Classes", "title": "sli | classes-cs178-notes / matlab-classes ", "html": "<!DOCTYPE html \n    PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \n    \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html>\n<head>\n  <title>SLI | Classes-CS178-Notes / Matlab-Classes </title>\n  <meta http-equiv='Content-Style-Type' content='text/css' />\n  <link rel='stylesheet' href='http://sli.ics.uci.edu/pmwiki/pub/skins/custom/pmwiki.css' type='text/css' />\n  <!--HTMLHeader--><style type='text/css'><!--\n  ul, ol, pre, dl, p { margin-top:0px; margin-bottom:0px; }\n  code.escaped { white-space: nowrap; }\n  .vspace { margin-top:1.33em; }\n  .indent { margin-left:40px; }\n  .outdent { margin-left:40px; text-indent:-40px; }\n  a.createlinktext { text-decoration:none; border-bottom:1px dotted gray; }\n  a.createlink { text-decoration:none; position:relative; top:-0.5em;\n    font-weight:bold; font-size:smaller; border-bottom:none; }\n  img { border:0px; }\n  .editconflict { color:green; \n  font-style:italic; margin-top:1.33em; margin-bottom:1.33em; }\n\n  table.markup { border:2px dotted #ccf; width:90%; }\n  td.markup1, td.markup2 { padding-left:10px; padding-right:10px; }\n  table.vert td.markup1 { border-bottom:1px solid #ccf; }\n  table.horiz td.markup1 { width:23em; border-right:1px solid #ccf; }\n  table.markup caption { text-align:left; }\n  div.faq p, div.faq pre { margin-left:2em; }\n  div.faq p.question { margin:1em 0 0.75em 0; font-weight:bold; }\n  div.faqtoc div.faq * { display:none; }\n  div.faqtoc div.faq p.question \n    { display:block; font-weight:normal; margin:0.5em 0 0.5em 20px; line-height:normal; }\n  div.faqtoc div.faq p.question * { display:inline; }\n   \n    .frame \n      { border:1px solid #cccccc; padding:4px; background-color:#f9f9f9; }\n    .lfloat { float:left; margin-right:0.5em; }\n    .rfloat { float:right; margin-left:0.5em; }\na.varlink { text-decoration:none; }\n\n--></style>\r\n\t<link href='http://sli.ics.uci.edu/pmwiki/pub/commentboxplus/commentboxplus.css' rel='stylesheet' type='text/css' />  <meta name='robots' content='index,follow' />\n\n</head>\n<body>\n<!--PageHeaderFmt-->\n  <div id='wikilogo'><a href='http://sli.ics.uci.edu'><img src='/pmwiki/pub/skins/custom/SLI_white.png'\n    alt='SLI' border='0' /></a></div>\n  <div id='wikihead'>\n  <form action='http://sli.ics.uci.edu'>\n    <!-- <span class='headnav'><a href='http://sli.ics.uci.edu/Classes-CS178-Notes/RecentChanges'\n      accesskey='c'>Recent Changes</a> -</span> --> \n    <input type='hidden' name='n' value='Classes-CS178-Notes.Matlab-Classes' />\n    <input type='hidden' name='action' value='search' />\n    <!-- <a href='http://sli.ics.uci.edu/Site/Search'>Search</a>: -->\n    <input type='text' name='q' value='' class='inputbox searchbox' />\n    <input type='submit' class='inputbutton searchbutton'\n      value='Search' />\n    <a href='http://sli.ics.uci.edu/Site/Search'>(?)</a>\n  </form></div>\n<!--/PageHeaderFmt-->\n  <table id='wikimid' width='100%' cellspacing='0' cellpadding='0'><tr>\n<!--PageLeftFmt-->\n      <td id='wikileft' valign='top'>\n        <ul><li><a class='wikilink' href='http://sli.ics.uci.edu/Classes/Classes'>Classes</a>\n</li><li><a class='wikilink' href='http://sli.ics.uci.edu/Group/Group'>Group</a>\n</li><li><a class='wikilink' href='http://sli.ics.uci.edu/Projects/Projects'>Research</a>\n</li><li><a class='urllink' href='http://www.ics.uci.edu/~ihler/pubs.html' rel='nofollow'>Publications</a>\n</li><li><a class='wikilink' href='http://sli.ics.uci.edu/Code/Code'>Code</a>\n</li></ul><div class='vspace'></div><hr />\n<div class='vspace'></div>\n</td>\n<!--/PageLeftFmt-->\n      <td id='wikibody' valign='top'>\n<!--PageActionFmt-->\n        <div id='wikicmds'><ul><li class='browse'><a class='wikilink' href='http://sli.ics.uci.edu/Classes-CS178-Notes/Matlab-Classes?action=login'>login</a>\n</li></ul>\n</div>\n<!--PageTitleFmt-->\n        <div id='wikititle'>\n          <div class='pagegroup'><a href='http://sli.ics.uci.edu/Classes-CS178-Notes'>Classes-CS178-Notes</a> /</div>\n          <h1 class='pagetitle'>Matlab-Classes</h1></div>\n<!--PageText-->\n<div id='wikitext'>\n<h2>Matlab Object-Oriented Programming and Classes</h2>\n<p>Matlab object-oriented programming is a bit awkward and unlike most other\nOO languages, but still has some of the elements that make OO programming\nuseful.  Here I will describe some aspects of Matlab objects, using a class\nto construct a k-nearest neighbor classifier as a working example.\n</p>\n<div class='vspace'></div><h3>Getting started</h3>\n<p>First, download the course code into a directory, say,\n<code>~ihler/Code/cs178/</code>.\nYou should see a number of files and directories created there;\nfor the purposes of this handout, the relevant directory to have is the\n<code>@knnClassify</code> directory, which contains the files for the\nclassifier.\n</p>\n<p class='vspace'>You should make sure you are either in the directory you downloaded the\nfiles to, or add it to your Matlab path.  For example, use\n</p><div class='Matlab' style='padding:5px; border:1px solid black; background-color:#dddddd' >\n<pre> &gt;&gt; addpath ~ihler/Code/cs178\n</pre></div>\n<p>to add that directory to Matlab's path.  Note that you should <em>not</em>\nbe <em>in</em> the <code>@knnClassify</code> directory, nor should you add\nit to the path -- only its parent directory.\n</p>\n<p class='vspace'>We will use \"old style\" Matlab objects, which consist of a directory\n(@-something), which uses one file per member function (private functions\ngo in a further subdirectory called <code>private</code>).  While there are\nsome drawbacks to this type of object (and Matlab has since updated\ntheir object representations to be more flexible and give them\nmore capabilities), this is the type\nthat is also compatible with Octave, which is useful also.\n</p>\n<div class='vspace'></div><h3>Using the class</h3>\n<p>To create an object of the class type, you can simply call the constructor.\nTo find out its usage, use <code>help</code>:\n</p><div class='Matlab' style='padding:5px; border:1px solid black; background-color:#dddddd' >\n<pre> &gt;&gt; help knnClassify\n  knnClassify(X,Y,...) : create k-nearest-neighbors classifier\n  takes no arguments, or training data to be used in constructing the classifier\n  alpha: weighted average coefficient (Gaussian weighting); alpha=0 =&gt; simple average\n</pre></div>\n<p>and we can then use it:\n</p><div class='Matlab' style='padding:5px; border:1px solid black; background-color:#dddddd' >\n<pre> &gt;&gt; Xtr = rand(5,3),     <span  style='color: green;'> % Create feature matrix of 5 data points with 3 features each </span>\n Xtr =                   <span  style='color: green;'> %  (just in case you want to use the same numbers I do...) </span>\n    0.4898    0.2760    0.4984 \n    0.4456    0.6797    0.9597 \n    0.6463    0.6551    0.3404\n    0.7094    0.1626    0.5853\n    0.7547    0.1190    0.2238\n &gt;&gt; Ytr = [0 0 0 1 1]';  <span  style='color: green;'> % and a corresponding Ytrain of target classes </span>\n &gt;&gt; knn = knnClassify(Xtr,Ytr,3),   <span  style='color: green;'> % Create 3-nearest-nbr classifier with those data</span>\n KNN Classifier, 2 classes, K=3\n</pre></div>\n<p class='vspace'>Now, to find out what methods are available for a given class,\nwe can use the <code>methods</code> command to list them:\n</p><div class='Matlab' style='padding:5px; border:1px solid black; background-color:#dddddd' >\n<pre> &gt;&gt; methods knnClassify\n Methods for class knnClassify:\n\n auc          err          predict      setClasses   \n confusion    getClasses   predictSoft  setK         \n display      knnClassify  roc          train        \n</pre></div>\n<p>Our most typical operations will be <code>train</code> and <code>predict</code>, which\ntrain a model on training data, and predict the current model on new data, respectively.\nTypically, when constructor functions accept training data, they simply call the\n<code>train</code> function.\nTo see what parameters <code>train</code> takes, we need to differentiate <em>which</em>\ntrain we mean:\n</p><div class='Matlab' style='padding:5px; border:1px solid black; background-color:#dddddd' >\n<pre> &gt;&gt; help knnClassify/train\n   knn=train(knn,Xtrain,Ytrain) : Batch training for knn; just memorize data\n</pre></div>\n<p class='vspace'>Using this calling pattern, we can re-train the model with e.g.,\n</p><div class='Matlab' style='padding:5px; border:1px solid black; background-color:#dddddd' >\n<pre> &gt;&gt; knn = train(knn, Xtr,Ytr);\n</pre></div>\n<p>and predict with\n</p><div class='Matlab' style='padding:5px; border:1px solid black; background-color:#dddddd' >\n<pre> &gt;&gt; Xte = rand(2,3),               <span  style='color: green;'> % Make two test data points (same # of features!)</span>\n Xte =\n    0.7513    0.5060    0.8909\n    0.2551    0.6991    0.9593\n &gt;&gt; Yte = [0 1]';                  <span  style='color: green;'> %   and some test target values for goot measure</span>\n &gt;&gt; Yhat= predict(knn,Xte),        <span  style='color: green;'> % Make prediction for those points:</span>\n Yhat =\n     0\n     0\n</pre></div>\n<p>Two points are worth noting: First, member functions are typically called by\npassing the object as the first argument of the function.  They can also be\ncalled in a more typical format, <code>knn.train(Xtr,Ytr)</code>, but both are\nimplemented in exactly the same way.\n</p>\n<p class='vspace'>Second, functions that modify the class in some way (such as <code>train</code>)\nshould actually return (and set) the object variable.  Matlab cannot generally\nupdate variables by reference (recent object changes relax this point), and\nso the object must be returned in order to modify it.\n</p>\n<p class='vspace'>We can also use accessor functions to get or set object properties, such as:\n</p><div class='Matlab' style='padding:5px; border:1px solid black; background-color:#dddddd' >\n<pre> &gt;&gt; knn = setK(knn, 1),            <span  style='color: green;'> % Change to a 1-nearest nbr classifier</span>\n KNN Classifier, 2 classes, K=1\n</pre></div>\n<p>Again, we actually return the modified object variable, and set <code>kdd</code>\nto be equal to the returned value.\n</p>\n<div class='vspace'></div><h3>The object constructor</h3>\n<p>Let's take a closer look at how the constructor function\n<code>knnClassify.m</code> works.  First, here is the file header:\n</p><div class='Matlab' style='padding:5px; border:1px solid black; background-color:#dddddd' >\n<pre> function obj = knnClassify(Xtr,Ytr,K)\n <span  style='color: green;'> % knnClassify([X,Y,K]) : create k-nearest-neighbors classifier</span>\n <span  style='color: green;'> % takes no arguments, or training data to be used in constructing the classifier</span>\n</pre></div>\n<p>The first line declares the function itself, and any returned variables.\nThe first set of comments in the file are output for the help\ncommand (<code>help knnClassify</code>), and should contain basic usage information.\n</p>\n<p class='vspace'>Default values are a bit awkward; typically, you can check how many variables\nwere passed in and fill in any missing ones (positional defaults):\n</p><div class='Matlab' style='padding:5px; border:1px solid black; background-color:#dddddd' >\n<pre>   if (nargin &lt; 3) K = 1; end;\n</pre></div>\n<p>Another typical approach is to require that the caller pass an empty matrix,\nwhich can be tested for and filled in with a default value.\n</p>\n<p class='vspace'>The basic form of an object is simply a Matlab structure with a bit of extra\ngloss on top; we declare the member variables as if it were a structure, and\nthen call a function to define it as a class:\n</p><div class='Matlab' style='padding:5px; border:1px solid black; background-color:#dddddd' >\n<pre>   obj.K=K; obj.Xtrain=[]; obj.Ytrain=[]; obj.classes=[];\n   obj=class(obj,'knnClassify');\n</pre></div>\n<p>I usually do this immediately with empty values, since the variable fields\nmust always be declared in the same order.  Note also that return values are\nspecified by name, so if <code>obj</code> is listed as the return variable, whatever\nvariable is called <code>obj</code> when the function ends is returned.\n</p>\n<p class='vspace'>I typically also allow <code>train</code> to be called automatically by just passing\nthe data into the constructor:\n</p><div class='Matlab' style='padding:5px; border:1px solid black; background-color:#dddddd' >\n<pre>   if (nargin &gt; 0)\n     obj = train(obj,Xtr,Ytr);\n     obj = setK(obj,K);\n   end;\n</pre></div>\n<p>Or, these can be called manually after.\n</p>\n<p class='vspace'>As an aside, Matlab objects can be converted into structures, allowing their\ninternal data to be accessed by anyone:\n</p><div class='Matlab' style='padding:5px; border:1px solid black; background-color:#dddddd' >\n<pre> &gt;&gt; s = struct(knn),\n s = \n          K: 1\n     Xtrain: [5x3 double]\n     Ytrain: [5x1 double]\n    classes: [2x1 double]\n</pre></div>\n<p>This can be useful if you're trying to access something in an object while debugging,\nbut is usually non-reversible, i.e., it is difficult to modify <code>s</code> and transform it\nback to an object afterwards.\n</p>\n<div class='vspace'></div><h3>The <code>train</code> function</h3>\n<p>Training for a k-nearest neighbor classifier is trivial; we simply memorize the data:\n</p><div class='Matlab' style='padding:5px; border:1px solid black; background-color:#dddddd' >\n<pre> function obj=train(obj, Xtr,Ytr)\n <span  style='color: green;'> % knn=train(knn,Xtrain,Ytrain) : Batch training for knn; just memorize data</span>\n   obj.Xtrain = Xtr;\n   obj.Ytrain = Ytr;\n   obj.classes= unique(Ytr);\n</pre></div>\n<p>One minor point -- I always keep a column vector <code>obj.classes</code> in each classifier.\nThe internal implementation of the classifier predicts a positive integer <img class='TrueLatexImage' style='vertical-align:middle;' border=0 src=\"http://sli.ics.uci.edu/pmwiki/pub/latexcache/3fa649761d69091d3b08de5566039e5e.png\" />,\nand then returns <code>obj.classes(c)</code>.  This way, the class values can be\nnon-consecutive, non-standard, and even of different Matlab types (characters or whatever)\nwithout any difficulty.  Some classifiers are implemented specifically for binary\nclassification problems, in which case we can simply check that the number of classes is only\ntwo.\n</p>\n<div class='vspace'></div><h3>The <code>predict</code> function</h3>\n<p>In order to predict with a k-nearest neighbor classifier, we simply search the stored\ntraining data for the nearest points, in terms of their sum of squared differences.\nThe file header,\n</p><div class='Matlab' style='padding:5px; border:1px solid black; background-color:#dddddd' >\n<pre> function Yte = predict(obj,Xte)\n <span  style='color: green;'> % Yhat = predict(knn, Xtest) : make a nearest-neighbor prediction on test data  </span>\n\n   [Ntr,Mtr] = size(obj.Xtrain);          <span  style='color: green;'> % get size of training, test data</span>\n   [Nte,Mte] = size(Xte);\n   K = min(obj.K, Ntr);                   <span  style='color: green;'> % can't have more than Ntrain neighbors</span>\n   Yte = repmat(obj.Ytrain(1), [Nte,1]);  <span  style='color: green;'> % make Ytest the same data type as Ytrain</span>\n</pre></div>\n<p>gets the number of training and test data, and their dimensions ($M$, which should be the\nsame for both), and makes sure $K$ is valid.  We pre-initialize <code>Yte</code> by\ncopying (<code>repmat</code> = repeat matrix) one of the training data to the correct size;\npre-allocating the correct vector size helps Matlab avoid constantly re-allocating\nmemory for <code>Yte</code>,\nand using <code>repmat</code> ensures it has the right variable type.\n</p>\n<p class='vspace'>Next, for each test data point, we compute the distance from all training data,\nfor example:\n</p><div class='Matlab' style='padding:5px; border:1px solid black; background-color:#dddddd' >\n<pre>   for i=1:Nte,                <span  style='color: green;'> % For each test example:</span>\n     dist=zeros(Ntr,1);        <span  style='color: green;'> % pre-allocate a distance vector</span>\n     for j=1:Ntr,              <span  style='color: green;'> % and compute distance from all Ntr training data</span>\n       dist(j)=sum( (obj.Xtrain(j,:)-Xte(i,:)).^2 ); \n     end;\n   end;\n</pre></div>\n<p>However, this turns out to be awfully slow; Matlab is interpreted, and often has\ntrouble performing for-loops quickly.  For better performance, you may\nlearn to ``vectorize'' your calculations, performing them all in one step:\n</p><div class='Matlab' style='padding:5px; border:1px solid black; background-color:#dddddd' >\n<pre>   dist = sum( (obj.Xtrain - repmat(Xte(i,:),[Ntr,1]) ).^2 , 2);\n</pre></div>\n<p>This copies the <code>Xte</code> data point to be the same size as <code>Xtrain</code>,\nthen subtracts the two matrices, squares the entries (element-wise), and sums\nthem over their 2nd dimension (the features), leaving a column-vector of distances\nexactly like the for-loop above.  Even harder to read but slightly faster still\nis to use the <code>bsxfun</code> function, which performs operators on differently-sized\nmatrices (so that you don't need to use <code>repmat</code> to copy the data point):\n</p><div class='Matlab' style='padding:5px; border:1px solid black; background-color:#dddddd' >\n<pre>   dist = sum( bsxfun( @minus, obj.Xtrain, Xte(i,:) ).^2 , 2);\n</pre></div>\n<p>All this is useful if you are finding Matlab very slow, but takes a while to get used to.\n</p>\n<p class='vspace'>Finally, we find the $K$ nearest data examples, and find the majority vote among them:\n</p><div class='Matlab' style='padding:5px; border:1px solid black; background-color:#dddddd' >\n<pre>   [dst,idx] = sort(dist);       <span  style='color: green;'> % find nearest neighbors over Xtrain</span>\n   idx=idx(1:K);                 <span  style='color: green;'> % keep nearest K data points</span>\n   nClasses=length(obj.classes);\n   count = zeros(1,nClasses);    <span  style='color: green;'> % count up how many in each class</span>\n   for c=1:nClasses, count(c)=sum( obj.Ytrain(idx)==obj.classes(c) ); end;\n   [nil cMax] = max(count);      <span  style='color: green;'> % find the (position of the) largest #</span>\n   Yte(i) = obj.classes(cMax);   <span  style='color: green;'> % and save the prediction</span>\n</pre></div>\n<p>A useful trick here -- both <code>sort</code> and <code>max</code> can return both the\nsorted / maximum value (first return value) <em>and</em> the position of those values\n(as the second return value).  So <code>idx</code> is a list of the training data points\nin order from nearest to farthest, and <code>cMax</code> is the index of the class with\nthe largest <code>count</code> value.\n</p>\n<p class='vspace'>Also, <code>obj.Ytrain(idx)==obj.classes(c)</code> is a binary vector, with \"1\"\nwhen the equality condition is satisified and \"0\" if not.  Then, <code>sum</code>\ncounts up the number of \"1\" entries.\n</p>\n<div class='vspace'></div><h3>Measuring errors</h3>\n<p>A few functions are common to almost all predictors; for example,\nfrequently, we want to evalute the error rate, measuring how often our model makes\nincorrect predictions on a data set (e.g., the training or validation error).\nFunctions like <code>err</code> do this easily:\n</p><div class='Matlab' style='padding:5px; border:1px solid black; background-color:#dddddd' >\n<pre> &gt;&gt; J = err(knn, Xte, Yte),       <span  style='color: green;'> % evalute the empirical error rate on these data</span>\n J = \n     0.5000\n</pre></div>\n<p>To get more information, we may want to look at the confusion matrix:\n</p><div class='Matlab' style='padding:5px; border:1px solid black; background-color:#dddddd' >\n<pre> &gt;&gt; C = confusion(knn, Xte, Yte), <span  style='color: green;'> % evalute the confusion matrix:</span>\n C =                             \n      1     0                    <span  style='color: green;'> % one true class zero (column), predicted 0 (row)</span>\n      1     0                    <span  style='color: green;'> % one true class zero (column), predicted 1 (row)</span>\n</pre></div>\n<p>Similar functions (<code>mse</code>, etc.) are found in regression classes.\n</p>\n<p class='vspace'>Many classifiers also support <em>soft</em> predictions, which express some level\nof confidence in the possible outcomes.  For example, in kNN, we might return\nthe fraction of the $K$ neighbors in each class (rather than just the decision);\n<code>predictSoft</code> returns a length-<code>nClasses</code> vector of such confidences\nfor each data point:\n</p><div class='Matlab' style='padding:5px; border:1px solid black; background-color:#dddddd' >\n<pre> &gt;&gt; knn = setK(knn, 3);           <span  style='color: green;'> % poinless for k=1...</span>\n &gt;&gt; ySoft = predictSoft(knn,Xte), <span  style='color: green;'> % make soft predictions:</span>\n ySoft =\n     0.6667    0.3333            <span  style='color: green;'> % test point 1 has 66% confidence in class 0</span>\n     1.0000         0            <span  style='color: green;'> % test point 2 has 100% confidence in class 0</span>\n</pre></div>\n<p>These soft scores are useful in computing, for example, ROC curves\n(note that this only works for binary classifications):\n</p><div class='Matlab' style='padding:5px; border:1px solid black; background-color:#dddddd' >\n<pre>  [fpr,tpr,tnr] = roc(knn,Xte,Yte);  <span  style='color: green;'> % find info for ROC curve:</span>\n  plot(fpr,tpr,'-');            <span  style='color: green;'> % (not very interesting for these data, though...)</span>\n</pre></div>\n<p>and the area under the curve can be computed with <code>auc.m</code>.\n</p>\n<div class='vspace'></div>\n</div>\n\n      </td>\n    </tr></table>\n<!--PageFooterFmt-->\n  <div id='wikifoot'>\n    <div class='footnav' style='float:left'> Last modified January 05, 2015, at 12:06 PM</div>\n    <div class='footnav' style='float:right; text-align:right'>\n    <a href=\"http://www.ics.uci.edu\">Bren School of Information and Computer Science</a><br>\n    <a href=\"http://www.uci.edu\">University of California, Irvine</a>\n    </div>\n  </div>\n<!--HTMLFooter--><script type=\"text/javascript\">\n  var _gaq = _gaq || [];\n  _gaq.push([\"_setAccount\", \"UA-24148957-2\"]);\n\t_gaq.push([\"_trackPageview\"]);\n\t(function() {\n\t  var ga = document.createElement(\"script\"); ga.type = \"text/javascript\"; ga.async = true;\n\t  ga.src = (\"https:\" == document.location.protocol ? \"https://ssl\" : \"http://www\") + \".google-analytics.com/ga.js\";\n\t  var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(ga, s);\n\t  })();\n</script>\n</body>\n</html>\n", "id": 234.0}