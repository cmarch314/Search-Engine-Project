{"text": "Proxemics Recognition in Personal Photos Yi Yang Simon Baker Anitha Kannan and Deva Ramanan Abstract Proxemics is the study of how people interact We present a computational formulation of visual proxemics by attempting to label each pair of people in an image with a subset of physically based touch codes A baseline approach would be to first perform pose estimation and then detect the touch codes based on the estimated joint locations We found that this sequential approach does not perform well because pose estimation step is too unreliable for images of interacting people due to difficulties with occlusion and limb ambiguities Instead we propose a direct approach where we build an articulated model tuned for each touch code Each such model contains two people connected in an appropriate manner for the touch code in question We fit this model to the image and then base classification on the fitting error Experiments show that this approach significantly outperforms the sequential baseline as well as other related approches Download Data Codes The latest copy of our code and dataset DOWNLOAD Please read the README file for proper installation Examples Publications If you use our software or our data sets please cite Yi Yang Simon Baker Anitha Kannan Deva Ramanan Recognizing Proxemics in Personal Photos IEEE Conference on Computer Vision and Pattern Recognition CVPR Rhode Island USA 2 12 Paper Slides Poster Talk conference yang2 12recognizing title Recognizing proxemics in personal photos author Yang Y and Baker S and Kannan A and Ramanan D booktitle 2 12 IEEE Conference on Computer Vision and Pattern Recognition year 2 12 organization IEEE Acknowledgements Funding The research described was conducted while Yi Yang was an intern at Microsoft Research Deva Ramanan was supported by NSF Grant 954 83 ONR MURI Grant N 14 1 1 933 and the Intel Science and Technology Center for Visual Computing License Copyright 2 12 Yi Yang Simon Baker Anitha Kannan and Deva Ramanan Permission is hereby granted free of charge to any person obtaining a copy of this software and associated documentation files the Software to deal in the Software without restriction including without limitation the rights to use copy modify merge publish distribute sublicense and or sell copies of the Software and to permit persons to whom the Software is furnished to do so subject to the following conditions The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software THE SOFTWARE IS PROVIDED AS IS WITHOUT WARRANTY OF ANY KIND EXPRESS OR IMPLIED INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM DAMAGES OR OTHER LIABILITY WHETHER IN AN ACTION OF CONtrACT TORT OR OTHERWISE ARISING FROM OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE ", "_id": "http://www.ics.uci.edu/~dramanan/software/proxemics/", "title": "proxemics recognition in personal photos - uc irvine", "html": "<html>\n<head>\n<title>Proxemics Recognition in Personal Photos - UC Irvine</title>\n<style>\nbody\n{\n\tfont-family : Arial;\n}\n#container\n{\n\twidth : 900px;\n\tmargin : 20px auto;\n\tbackground-color : #fff;\n\tpadding : 20px;\n}\nh1 strong\n{\n\tfont-size : 40px;\n}\nh1\n{\n\tmargin : 0;\n\tpadding : 0;\n\tfont-size : 30px;\n}\ncode\n{\n\tborder : 1px solid #ccc;\n\tdisplay : block;\n\tpadding : 5px;\n\tmargin : 10px;\n}\n</style>\n<!--<script type=\"text/javascript\">\n\n  var _gaq = _gaq || [];\n  _gaq.push(['_setAccount', 'UA-17813713-3']);\n  _gaq.push(['_setDomainName', '.mit.edu']);\n  _gaq.push(['_trackPageview']);\n\n  (function() {\n    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;\n    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';\n    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);\n  })();\n\n</script>-->\n</head>\n\n\n<body>\n<div id=\"container\">\n\n<h1 style=\"text-align:center;\"><strong>P</strong>roxemics <strong>R</strong>ecognition in <strong>P</strong>ersonal <strong>P</strong>hotos</h1>\n\n<p style=\"text-align:center;\"><a href=\"http://www.ics.uci.edu/~yyang8\">Yi Yang</a>, <a href=\"http://research.microsoft.com/en-us/people/sbaker\">Simon Baker</a>, <a href=\"http://research.microsoft.com/en-us/people/ankannan\">Anitha Kannan</a> and <a href=\"http://www.ics.uci.edu/~dramanan\">Deva Ramanan</a></p>\n\n\n\n<!--<blockquote style=\"text-align:center;\">\"These are some of the best HITS I've ever done!\" <cite>&mdash; Mechanical Turk worker</cite></blockquote>-->\n\n<div style=\"margin-top : 30px;\"></div>\n\n<div style=\"text-align : center; margin-left : 0px; margin-right: 0px; border : 2px solid black;\">\n<img src=\"models_new.jpg\" style=\"height : 256px;\">\n</div>\n\n<h2>Abstract</h2>\n<p>Proxemics is the study of how people interact. We present a computational formulation of visual proxemics by attempting to label each pair of people in an image with a subset of physically based touch codes. A baseline approach would be to first perform pose estimation and then detect the touch codes based on the estimated joint locations. We found that this sequential approach does not perform well because pose estimation step is too unreliable for images of interacting people, due to difficulties with occlusion and limb ambiguities. Instead, we propose a direct approach where we build an articulated model tuned for each touch code. Each such model contains two people, connected in an appropriate manner for the touch code in question. We fit this model to the image and then base classification on the fitting error. Experiments show that this approach significantly outperforms the sequential baseline as well as other related approches.</p>\n\n<h2>Download Data & Codes</h2>\n\n<p>The latest copy of our code and dataset: <a href=\"proxemic-v1.0.zip\">DOWNLOAD</a></p>\n\n<p>Please read the <a href=\"README\">README</a> file for proper installation.</p>\n\n<h2>Examples</h2>\n\n<center>\n<table align=\"center\" border=\"0\" cellspacing=\"0\" cellpadding=\"2\"\n\n<tr>\n<td align=\"center\" width=\"140\"><img src=\"model_new_hh1_1.jpg\" width=\"140\"></td>\n<td align=\"center\" width=\"140\"><img src=\"model_new_ss1_1.jpg\" width=\"140\"></td>\n<td align=\"center\" width=\"140\"><img src=\"model_new_hs1_1.jpg\" width=\"140\"></td>\n<td align=\"center\" width=\"140\"><img src=\"model_new_he1_1.jpg\" width=\"140\"></td>\n<td align=\"center\" width=\"140\"><img src=\"model_new_es1_1.jpg\" width=\"140\"></td>\n<td align=\"center\" width=\"140\"><img src=\"model_new_hbt1_1.jpg\" width=\"140\"></td>\n</tr>\n<tr>\n<td align=\"center\" width=\"140\"><img src=\"LHRH_0043_new.jpg\" width=\"140\"></td>\n<td align=\"center\" width=\"140\"><img src=\"LSRS_0092_new.jpg\" width=\"140\"></td>\n<td align=\"center\" width=\"140\"><img src=\"LHLS_0010_new.jpg\" width=\"140\"></td>\n<td align=\"center\" width=\"140\"><img src=\"LHRE_0013_new.jpg\" width=\"140\"></td>\n<td align=\"center\" width=\"140\"><img src=\"LERS_0016_new.jpg\" width=\"140\"></td>\n<td align=\"center\" width=\"140\"><img src=\"RHBT_0034_new.jpg\" width=\"140\"></td>\n</tr>\n\n</table>\n</center>\n\n<h2>Publications</h2>\n\n<p>If you use our software or our data sets, please cite:</p>\n\n<p>Yi Yang, Simon Baker, Anitha Kannan, Deva Ramanan. \"Recognizing Proxemics in Personal Photos\". <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Rhode Island, USA, 2012.</em> <a href=\"proxemic2012.pdf\">[Paper]</a><a href=\"proxemic2012_slides.pptx\">[Slides]</a><a href=\"proxemic2012_poster.pdf\">[Poster]</a><a href=\"video/lecture.htm\">[Talk]</a> </p>\n\n<pre><code>@conference{yang2012recognizing,\n  title={Recognizing proxemics in personal photos},\n  author={Yang, Y. and Baker, S. and Kannan, A. and Ramanan, D.},\n  booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition},\n  year={2012},\n  organization={IEEE}\n}</code></pre>\n\n<h2>Acknowledgements &amp; Funding</h2>\n<p>The research described was conducted while Yi Yang was an intern at Microsoft Research. Deva Ramanan was supported by NSF Grant 0954083, ONR-MURI Grant N00014-10-1-0933, and the Intel Science and Technology Center for Visual Computing.</p>\n\n<!--<h2>Links</h2>\n<ul>\n<li><a href=\"http://www.ics.uci.edu/~yyang8/projects/pose/\">Articulated Pose Estimation with Flexible Mixtures of Parts</a></li>\n</ul>\n!-->\n\n<!--\n<h2>Update History</h2>\n<center>\n<table style=\"align:center; border:1px solid black; border-spacing:0px; width:100%\">\n<tr>\n<th style=\"border:1px solid black; width:20%\"><strong>Date</strong></th>\n<th style=\"border:1px solid black; width:80%\"><strong>Description</strong></th>\n</tr>\n<tr>\n<td style=\"border:1px solid black; width:20%\">7/21/2012</td>\n<td style=\"border:1px solid black; width:80%\">Hello World</td>\n</tr>\n</table>\n</center>\n!-->\n\n<h2>License</h2>\n<p>Copyright &copy; 2012 Yi Yang, Simon Baker, Anitha Kannan and Deva Ramanan</p>\n<p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p>\n<ul>\n<li>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</li>\n</ul>\n<p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONtrACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>\n\n</div>\n\n</body>\n</html>\n\n", "id": 4531.0}