{"text": "Machine Learning Spring 2 1 Classnotes updated 5 13 1 Q A Blog on Machine Learning updated 5 11 1 CompSci 273A Instructor Max Welling Office hours Fridays 1 2pm Prerequisites ICS 27 A Intro AI or with consent of instructor Goals The goal of this class is to familiarize you with various stat of the art machine learning techniques for classification regression clustering and dimensionality reduction Besides this an important aspect this class is to provide a modern statistical view of machine learning Grading Homwork The course will primarily be lecture based Every week there will be homework mostly consisting of a coding assignement Coding will be done in MATLAB Finishing all your homework assignments is required for passing this class If you do not hand in your HW before the weekly deadline you will acculumate penalty points or viewed more positively if you hand in your HW in time every week you will accumulate bonus points I may ask students to demo their implementation of the HW assignment in class There will be short quizzes every week starting in week 2 but no midterms or final Quizzes will be multiple choice and you will need to buy Green scantron test forms in the UCI bookstore both variety of green form will work There will also be a project starting right from day 1 Details are given below You may work in group s of up to 5 students but every member must do their own coding work Each group will also be required to do a presentation and write a report as a group It is important that the various contributions of the groupmembers will be made transparent Your final grade will be determined as a combination of Homework about 2 Project about 4 and Quizz exams about 4 Projects There will be 3 predefined projects 1 Parameter estimation of possibly stochastic nonlinear dynamical systems We will work with a well defined set of ODE s which model the development of cell lineages from stem cells to terminal cells The model can be found in the following paper As an objectiver we will measure the deviation of the various cell populations as a function of time from some ideal curves We may also specify upper and lower margins of deviations that are still considered acceptable The difficulty in learning these models is the highly nonlinear dynamics which makes it hard to compute the gradient of the objective function w r t the parameters of the model One way to approach the problem is to first learn a mapping C A with C a cost function and A the parameter setting Note that we can easily request samples to learn this function because we can easily forward simulate the ODEs for any set of A and then compute a cost C X as a function of the resulting populations X Therefore each simulation generates a datapoint and we have control over what datapoints to request Clearly we want to optimize this process and ask for the most informative data points This is called active learning Using these data points you will learn the mapping C A You can use a neural network or a Gaussian process or whatever else you like GPs are nice because they provide you with the uncertainty in your estimates of your mapping which can be used to optimize for the most informative simulation to run Given an estimate of the mapping C A you can find the best i e lowest cost parameter setting that will produce simulations that match the constraints most closely The learning process will have to trade off exploration of new parts of parameter space to improve the mapping with exploitation of the current mapping to find the best possible simulations If successful this project could lead to a publication because it is on the cutting edge of research and highly interesting to biologists 2 The second project will also concern a relastic problem in biology It has to do with proving that two classes of cells can be distinguished from the image data alone website for downloading data http cinquin org uk static datasets4welling There are two germlines 44 unstraightened and 61 rb each with four corresponding mat files which are as follows DAPI mat The original image of the germline fullSeg mat The segmented image of the germline coordinates mat The y x z coordinates of each of the n cells in the germline A 3xn matrix individual DAPI segmentations mat Each of the segmented cells A 61x61x61xn matrix where each cell is included in a 61 3 cube The segmented part of the original image is overlaid on a black background If you want to join this team please contact Kevin Heins kaheins gmail com 3 The following data will be used for all our Homework assignments It consists of a about 7 features columns and 5 data cases You should ignore the first 4 columns as they do not represent features attributes inputs covariates and the last column is the label response variable that you need to predict The typical performance on this data is between 77 8 Area Under The ROC Curve The task is to improve performance on this dataset to be significantly better than 8 ROC on Xross validation Any strong improvements will make the company who provided this data very happy 4 This will change shortly Predicting stock prices Old Data no longer of any use New Data Your task is to build a model that will predict the highest and lowest price for stock a over the next 5 mins Note that there are 7 time series in total but you are only asked to predict series a The other series are correlated with a so you can use them to make predictions Each time series is aligned and has a number of attributes opening and closing prices at 1 minute intervals highest and lowest price in each interval number of trades and total trading volume Given all information up till time t you are asked to predict the highest and lowest price for stock a over the next 5 minutes 5 time points This means that you can compute your regression label response variable Y as MAX X t 1 X t 2 X t 3 X t 4 X t 5 where X is the attribute highest price in your data Similarly for the MIN Given your predictions the company involved will execute some trading strategy on data that you haven t seen and report how much money you would have made More about this trading strategy and how you may be able to test your own programme later For now you can start on making predictions on the training data We will compute your Sharpe Ratio This measures risk corrected access return relative to a simple buy and hold strategy The best algorithm with significant positive Sharpe ratio wins an iPad The company who is providing this price will have access to your report and will test your code Here is an interesting paper on how to use boosting for stockmarket prediction 5 You may also define your own project but it will have to be approved by me first You may even work on problems dirtectly related to your research however you have to make very clear what you did for this class and there obviosuly needs to be a strong connections with machine learning Slides Week 1 Introduction kNN Logistic Regression Overfitting Xvalidation ppt pdf Week 2 Decision Trees Random Forests Bagging Boosting ppt pdf Week 3 Neural Networks ppt pdf Week 4 Convex Optimization see appendix A of the classnotes SVMs ppt pdf chapter 8 of classnotes Week 5 Unsupervised Learning k means clustering PCA ppt pdf Week 6 PCA Kernel PCA classnotes matlab demo Week 7 Receiver Operating Characteristic ROC ppt pdf Kernel Fisher Linear Discriminant Analysis classnotes Week 8 Spectral Kernel Clustering classnotes Week 9 Comparing Classfiers ppt pdf Kernel Canoncial Correlation Analysis classnotes Week 1 Data Sets and Online Resources Homework Dataset There are about 7 features columns and 5 data cases rows You should ignore the first 4 columns as they do not represent features attributes inputs covariates and the last column is the label response variable that you need to predict The typical performance on this data is between 77 8 Area Under The ROC Curve Homework always due the next Tuesday at 12pm Week 1 Homework 1 doc pdf updated Th April 1 Week 2 Homework 2 doc pdf Week 3 Homework 3 doc pdf Week 4 Homework 4 doc pdf Week 5 Homework 5 doc pdf updated Th Apr 29 Week 6 Homework 6 doc pdf Week 7 Homework 7 doc pdf updated Th May 13 Week 8 Homework 8 doc pdf Week 9 Homework 9 doc pdf Week 1 Syllabus incomplete 1 introduction overview examples goals algorithm evaluation statistics kNN logistic regression 2 classification I decision trees random forests bagging boosting 3 clustering dimensionality reduction k means expectation maximization PCA 4 neural networks perceptron multi layer networks back propagation 5 classification II kernel methods support vector machines Textbook I am writing my own text which will be posted here and on the top of this webpage Note that I will continuously update it during the course and feedback is appreciated The textbook that will be used for this course is 1 C Bishop Pattern Recognition and Machine Learning Optional side readings are 2 D MacKay Information Theory Inference and Learning Algorithms 3 R O Duda P E Hart D Stork Pattern Classification 4 C M Bishop Neural Networks for Pattern Recognition 5 T Hastie R Tibshirani J H Friedman The Elements of Statistical Learning 6 B D Ripley Pattern Recognition and Neural Networks 7 T Mitchell Machine Learning http www cs cmu edu tom mlbook html ", "_id": "http://www.ics.uci.edu/~welling/teaching/273ASpring10/ICS273ASpring10.html", "title": "untitled document", "html": "<html xmlns:v=\"urn:schemas-microsoft-com:vml\"\nxmlns:o=\"urn:schemas-microsoft-com:office:office\"\nxmlns:w=\"urn:schemas-microsoft-com:office:word\"\nxmlns:st1=\"urn:schemas-microsoft-com:office:smarttags\"\nxmlns=\"http://www.w3.org/TR/REC-html40\">\n\n\t<head>\n<meta http-equiv=Content-Type content=\"text/html; charset=us-ascii\">\n<meta name=ProgId content=Word.Document>\n<meta name=Generator content=\"Microsoft Word 11\">\n<meta name=Originator content=\"Microsoft Word 11\">\n<link rel=File-List href=\"ICS273AFall06_files/filelist.xml\">\n<link rel=Edit-Time-Data href=\"ICS273AFall06_files/editdata.mso\">\n<!--[if !mso]>\n<style>\nv\\:* {behavior:url(#default#VML);}\no\\:* {behavior:url(#default#VML);}\nw\\:* {behavior:url(#default#VML);}\n.shape {behavior:url(#default#VML);}\n</style>\n<![endif]-->\n<title>Untitled Document</title>\n<o:SmartTagType namespaceuri=\"urn:schemas-microsoft-com:office:smarttags\"\n name=\"place\"/>\n<o:SmartTagType namespaceuri=\"urn:schemas-microsoft-com:office:smarttags\"\n name=\"City\"/>\n<!--[if gte mso 9]><xml>\n <o:DocumentProperties>\n  <o:Author>Welling</o:Author>\n  <o:Template>Normal</o:Template>\n  <o:LastAuthor>Welling</o:LastAuthor>\n  <o:Revision>9</o:Revision>\n  <o:TotalTime>131</o:TotalTime>\n  <o:Created>2006-07-03T18:58:00Z</o:Created>\n  <o:LastSaved>2006-08-30T21:40:00Z</o:LastSaved>\n  <o:Pages>1</o:Pages>\n  <o:Words>358</o:Words>\n  <o:Characters>2044</o:Characters>\n  <o:Company> UCI</o:Company>\n  <o:Lines>17</o:Lines>\n  <o:Paragraphs>4</o:Paragraphs>\n  <o:CharactersWithSpaces>2398</o:CharactersWithSpaces>\n  <o:Version>11.5606</o:Version>\n </o:DocumentProperties>\n</xml><![endif]--><!--[if gte mso 9]><xml>\n <w:WordDocument>\n  <w:Zoom>125</w:Zoom>\n  <w:SpellingState>Clean</w:SpellingState>\n  <w:GrammarState>Clean</w:GrammarState>\n  <w:ValidateAgainstSchemas/>\n  <w:SaveIfXMLInvalid>false</w:SaveIfXMLInvalid>\n  <w:IgnoreMixedContent>false</w:IgnoreMixedContent>\n  <w:AlwaysShowPlaceholderText>false</w:AlwaysShowPlaceholderText>\n  <w:BrowserLevel>MicrosoftInternetExplorer4</w:BrowserLevel>\n </w:WordDocument>\n</xml><![endif]--><!--[if gte mso 9]><xml>\n <w:LatentStyles DefLockedState=\"false\" LatentStyleCount=\"156\">\n </w:LatentStyles>\n</xml><![endif]--><!--[if !mso]><object\n classid=\"clsid:38481807-CA0E-42D2-BF39-B33AF135CC4D\" id=ieooui></object>\n<style>\nst1\\:*{behavior:url(#ieooui) }\n</style>\n<![endif]-->\n<style>\n<!--\n /* Font Definitions */\n @font-face\n\t{font-family:Tahoma;\n\tpanose-1:2 11 6 4 3 5 4 4 2 4;\n\tmso-font-charset:0;\n\tmso-generic-font-family:swiss;\n\tmso-font-pitch:variable;\n\tmso-font-signature:1627421319 -2147483648 8 0 66047 0;}\n /* Style Definitions */\n p.MsoNormal, li.MsoNormal, div.MsoNormal\n\t{mso-style-parent:\"\";\n\tmargin:0in;\n\tmargin-bottom:.0001pt;\n\tmso-pagination:widow-orphan;\n\tfont-size:12.0pt;\n\tfont-family:\"Times New Roman\";\n\tmso-fareast-font-family:\"Times New Roman\";}\nh2\n\t{mso-margin-top-alt:auto;\n\tmargin-right:0in;\n\tmso-margin-bottom-alt:auto;\n\tmargin-left:0in;\n\tmso-pagination:widow-orphan;\n\tmso-outline-level:2;\n\tfont-size:18.0pt;\n\tfont-family:\"Times New Roman\";}\na:link, span.MsoHyperlink\n\t{color:blue;\n\ttext-decoration:underline;\n\ttext-underline:single;}\na:visited, span.MsoHyperlinkFollowed\n\t{color:blue;\n\ttext-decoration:underline;\n\ttext-underline:single;}\nspan.SpellE\n\t{mso-style-name:\"\";\n\tmso-spl-e:yes;}\nspan.GramE\n\t{mso-style-name:\"\";\n\tmso-gram-e:yes;}\n@page Section1\n\t{size:8.5in 11.0in;\n\tmargin:1.0in 1.25in 1.0in 1.25in;\n\tmso-header-margin:.5in;\n\tmso-footer-margin:.5in;\n\tmso-paper-source:0;}\ndiv.Section1\n\t{page:Section1;}\n-->\n</style>\n<!--[if gte mso 10]>\n<style>\n /* Style Definitions */\n table.MsoNormalTable\n\t{mso-style-name:\"Table Normal\";\n\tmso-tstyle-rowband-size:0;\n\tmso-tstyle-colband-size:0;\n\tmso-style-noshow:yes;\n\tmso-style-parent:\"\";\n\tmso-padding-alt:0in 5.4pt 0in 5.4pt;\n\tmso-para-margin:0in;\n\tmso-para-margin-bottom:.0001pt;\n\tmso-pagination:widow-orphan;\n\tfont-size:10.0pt;\n\tfont-family:\"Times New Roman\";\n\tmso-ansi-language:#0400;\n\tmso-fareast-language:#0400;\n\tmso-bidi-language:#0400;}\n</style>\n<![endif]--><!--[if gte mso 9]><xml>\n <o:shapedefaults v:ext=\"edit\" spidmax=\"12290\"/>\n</xml><![endif]--><!--[if gte mso 9]><xml>\n <o:shapelayout v:ext=\"edit\">\n  <o:idmap v:ext=\"edit\" data=\"1\"/>\n </o:shapelayout></xml><![endif]-->\n</head>\n\n\t<body bgcolor=\"#CCCCCC\" background=\"../../background.gif\" lang=EN-US link=blue vlink=blue style=\"tab-interval:.5in\">\n\t\t<div class=Section1>\n\t\t\t<h2><span style=\"font-size:10.0pt;font-family:Tahoma;color:red\"><font size=\"+5\">Machine Learning <span class=GramE>Spring</span> 2010</font></span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt;font-family:Tahoma;color:red\"><font size=\"+3\" color=\"#0033bb\" face=\"Times New Roman\"><em><a href=\"IntroMLBook.pdf\">Classnotes</a> </em></font></span><em><font size=\"3\">(updated 5/13/10)</font></em></h2>\n\t\t</div>\n\t\t<div class=Section1 align=\"left\">\n\t\t\t<h2><span style=\"font-size:10.0pt;color:black\"><font size=\"3\"><a href=\"http://introml.blogspot.com/\">Q&amp;A Blog on Machine Learning</a></font></span><em><font size=\"3\">(updated 5/11/10)</font></em></h2>\n\t\t</div>\n\t\t<div class=Section1>\n\t\t\t<hr size=2 width=\"100%\" align=left>\n\t\t\t<h2><span style=\"font-size:10.0pt\">CompSci: 273A<br>\n\t\t\t\t\t\tInstructor: Max Welling</span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt\">Office hours: Fridays 1-2pm</span><span style=\"color:black\">\u00a0</span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt\">\n\t\t\t\t\t<hr size=2 width=\"100%\" align=left>\n\t\t\t\t</span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt;font-family:Arial;color:red\">Prerequisites</span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt\">ICS 270A Intro AI, or with consent of instructor.</span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt\">\n\t\t\t\t\t<hr size=2 width=\"100%\" align=left>\n\t\t\t\t</span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt;font-family:Arial;color:red\">Goals:</span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt;color:black\">The goal of this class is to familiarize you with various stat-of-the-art machine learning techniques for classification, regression, <span class=GramE>clustering</span> and dimensionality reduction. Besides this, an important aspect this class is to provide a modern statistical view of machine learning. </span></h2>\n\t\t\t<hr size=2 width=\"100%\" align=left>\n\t\t\t<p><span style=\"font-size:10.0pt;font-family:Arial;color:red\"><strong>Grading &amp; Homwork:</strong></span></p>\n\t\t\t<h2><span style=\"font-size:10.0pt\">The course will primarily be lecture-based.<br>\n\t\t\t\t\t<br>\n\t\t\t\t\tEvery week there will be homework mostly consisting of a coding assignement. Coding will be done in MATLAB. Finishing all your homework assignments is required for passing this class. If you do not hand in your HW before the weekly deadline, you will acculumate penalty points (or viewed more positively, if you hand in your HW in time every week you will accumulate bonus points). I\u00a0may ask students to demo their implementation of the HW assignment in class.<br>\n\t\t\t\t\t<br>\n\t\t\t\t\t\tThere will be short quizzes every week, starting in week 2, but no midterms or final. Quizzes will be multiple choice, and you will need to buy <em><font color=\"red\">Green scantron test forms</font></em> in the UCI\u00a0bookstore (both variety of green form will work).<br>\n\t\t\t\t\t<br>\n\t\t\t\t</span><span style=\"font-size:10.0pt\">There will also be a project, starting right from day 1. Details are given below. You may work in group's of up to 5 students but every member must do their own coding work. Each group will also be required to do a presentation and write a report (as a group). It is important that the various contributions of the groupmembers will be made transparent.<br>\n\t\t\t\t\t<br>\n\t\t\t\t\t\tYour final grade will be determined as a combination of Homework (about 20%), Project (about 40%) and Quizz exams (about 40%).</span></h2>\n\t\t\t<hr size=2 width=\"100%\" align=left>\n\t\t\t<p><strong><font color=\"red\">Projects</font>:</strong></p>\n\t\t\t<h5>There will be 3 predefined projects:</h5>\n\t\t\t<h5>1. Parameter estimation of (possibly stochastic) nonlinear dynamical systems. We will work with a well defined set of ODE's which model the development of cell lineages from stem cells to terminal cells. The model can be found in the following <a href=\"journal.pbio.1000015.pdf\">paper</a>. As an objectiver we will measure the deviation of the various cell populations as a function of time from some ideal curves. We may also specify upper and lower margins of deviations that are still considered acceptable. The difficulty in learning these models is the highly nonlinear dynamics which makes it hard to compute the gradient of the objective function w/r/t/ the parameters of the model. One way to approach the problem is to first learn a mapping C(A) with C a cost function and &quot;A&quot; the parameter setting. Note that we can easily request samples to learn this function because we can easily forward simulate the ODEs for any set of A and then compute a cost C(X) as a function of the resulting populations X. Therefore, each simulation generates a datapoint and we have control over what datapoints to request. Clearly, we want to optimize this process and ask for the most informative data-points. This is called &quot;active learning&quot;. Using these data-points you will learn the mapping C(A). You can use a neural network or a Gaussian process or whatever else you like (GPs are nice because they provide you with the uncertainty in your estimates of your mapping which can be used to optimize for the most informative simulation to run). Given an estimate of the mapping C(A), you can find the best, i.e. lowest cost, parameter setting that will produce simulations that match the constraints most closely. The learning process will have to trade-off exploration of new parts of parameter space to improve the mapping with exploitation of the current mapping to find the best possible simulations. If successful this project could lead to a publication. because it is on the cutting edge of research and highly interesting to biologists.</h5>\n\t\t\t<h5>2. The second project will also concern a relastic problem in biology. It has to do with proving that two classes of cells can be distinguished from the image data alone..<br>\n\t\t\t\t\twebsite for downloading data: <a href=\"http://cinquin.org.uk/static/datasets4welling/\">http://cinquin.org.uk/static/datasets4welling/</a> . <br>\n\t\t\t\tThere are two germlines, 44_unstraightened and 61_rb, each with four corresponding .mat files, which are as follows:<br>\n\t\t\t\tDAPI.mat - The original image of the germline<br>\n\t\t\t\tfullSeg.mat - The segmented image of the germline<br>\n\t\t\t\tcoordinates.mat - The [y,x,z] coordinates of each of the n cells in the germline. A 3xn matrix.<br>\n\t\t\t\tindividual_DAPI_segmentations.mat - Each of the segmented cells. A 61x61x61xn matrix, where each cell is included in a 61^3 cube. The segmented part of the original image is overlaid on a black background.<br>\n\t\t\t\tIf you want to join this team please contact Kevin Heins: &lt;kaheins@gmail.com&gt;<br>\n\t\t\t</h5>\n\t\t\t<h5>3.  The<a href=\"d1uci.zip\"> following data</a> will be used for all our Homework assignments. It consists of a about 70 features (columns) and 50,000 data-cases. You should ignore the first 4 columns as they do not represent features (attributes, inputs, covariates) and the last column is the label (response variable) that you need to predict. The typical performance on this data is between [0.77-0.8] Area Under The ROC Curve.  The task is to improve performance on this dataset to be significantly better than 80% ROC on Xross validation. Any strong improvements will make the company who provided this data very happy. </h5>\n\t\t\t<h5>4. <font color=\"red\">This will change shortly:</font><br>\n\t\t\t\tPredicting stock prices. <br>\n\t\t\t\t<a href=\"TimeSeries.zip\">Old Data </a> (no longer of any use)<br>\n\t\t\t</h5>\n\t\t\t<h5><a href=\"NewTimeSeries.zip\">New Data<br>\n\t\t\t\t</a>Your task is to build a model that will predict the highest and lowest price for  stock &quot;a&quot; over the next 5 mins. Note that there are 7 time series in total but you are only asked to predict series &quot;a&quot;. The other series are correlated with &quot;a&quot; so you can use them to make predictions. Each time series is aligned and has a number of attributes: opening and closing  prices at 1 minute intervals, highest and lowest price in each interval, number of trades and total trading volume. Given all information up till time &quot;t&quot;, you are asked to predict the highest and lowest price for stock &quot;a&quot; over the next 5 minutes (5 time points). This means that you can compute your regression label (response variable Y) as MAX[X(t+1),X(t+2),X(t+3),X(t+4),X(t+5)]  where X is the attribute &quot;highest price&quot; in your data.  Similarly for the MIN. Given your predictions the company involved will execute some trading strategy on data that you haven't seen and report how much money you would have made. More about this trading strategy and how you may be able to test your own programme later. For now you can start on making predictions on the training data.</h5>\n\t\t\t<h5>We will compute your Sharpe Ratio. This measures risk corrected access return relative to a simple buy and hold strategy. The best algorithm with significant positive Sharpe ratio wins an iPad. The company who is providing this price will have access to your report and will test your code. Here is an interesting <a href=\"AdaBoost4Stocks.pdf\">paper</a> on how to use boosting for stockmarket prediction.</h5>\n\t\t\t<h5>5. You may also define your own project, but it will have to be approved by me first. You may even work on problems dirtectly related to your research, however you have to make very clear what you did for this class and there obviosuly needs to be a strong connections with machine learning.</h5>\n\t\t\t<h2><span style=\"font-size:10.0pt;font-family:Arial;color:black\">\n\t\t\t\t\t<hr size=2 width=\"100%\" align=left>\n\t\t\t\t</span></h2>\n\t\t\t<h2><span class=GramE><span style=\"font-size:10.0pt;font-family:Arial; color:red\">Slides:<br>\n\t\t\t\t\t\t<br>\n\t\t\t\t\t\t<font size=\"-1\" color=\"#0033bb\">Week 1: Introduction, kNN, Logistic Regression, Overfitting, Xvalidation [<a href=\"Intro273ASpring10.ppt\">ppt</a>] [<a href=\"Intro273ASpring10.pdf\">pdf</a>]<br>\n\t\t\t\t\t\t\tWeek 2: Decision Trees, Random Forests, Bagging, Boosting [<a href=\"ClassificationI273Aspring10.ppt\">ppt</a>] [<a href=\"ClassificationI273Aspring10.pdf\">pdf</a>]</font><br>\n\t\t\t\t\t\t<font size=\"-1\" color=\"#0033bb\">Week 3: Neural Networks [<a href=\"NeuralNets273ASpring10.ppt\">ppt</a>] [<a href=\"NeuralNets273ASpring10.pdf\">pdf</a>]<br>\n\t\t\t\t\t\t\tWeek 4: Convex Optimization (see appendix A of the classnotes), SVMs [<a href=\"SVM273ASpring2010.ppt\">ppt</a>] [<a href=\"SVM273ASpring2010.pdf\">pdf</a>] (chapter 8 of classnotes)  <br>\n\t\t\t\t\t\t</font></span></span><span class=GramE><span style=\"font-size:10.0pt;font-family:Arial; color:red\"><font size=\"-1\" color=\"#0033bb\">Week 5:</font><font size=\"1\" color=\"black\"> </font><font size=\"-1\" color=\"#0033bb\">Unsupervised Learning: k-means clustering &amp; PCA [<a href=\"UnsupLearn273ASpring10.ppt\">ppt</a>] [<a href=\"UnsupLearn273ASpring10.pdf\">pdf</a>]</font><font size=\"2\" color=\"#0033bb\"><br>\n\t\t\t\t\t\t</font><font size=\"-1\" color=\"#0033bb\">Week 6: PCA &amp; Kernel PCA (classnotes) [<a href=\"demo_kpca.m\">matlab demo</a>], <br>\n\t\t\t\t\t\t</font><font size=\"-1\" color=\"#0033bb\">Week 7: </font><font size=\"1\" color=\"black\"> </font><font size=\"-1\" color=\"#0033bb\">Receiver Operating Characteristic (ROC) [<a href=\"ROC273ASpring10.ppt\">ppt</a>] [<a href=\"ROC273ASpring10.pdf\">pdf</a>]</font><font size=\"1\" color=\"black\"> </font><font size=\"2\" color=\"black\">, </font><font size=\"-1\" color=\"#0033bb\">Kernel Fisher Linear Discriminant Analysis (classnotes)</font><font size=\"2\" color=\"black\"><br>\n\t\t\t\t\t\t</font><font size=\"-1\" color=\"#0033bb\">Week 8: Spectral &amp; Kernel Clustering (classnotes).</font><font size=\"2\" color=\"black\"><br>\n\t\t\t\t\t\t</font><font size=\"-1\" color=\"#0033bb\">Week 9: Comparing Classfiers [<a href=\"EvauationMethods273ASpring10.ppt\">ppt</a>] [<a href=\"EvauationMethods273ASpring10.pdf\">pdf</a>] , Kernel Canoncial Correlation Analysis (classnotes)</font><font size=\"2\" color=\"black\"><br>\n\t\t\t\t\t\t</font><font size=\"1\" color=\"black\"> </font><font size=\"-1\" color=\"#0033bb\">Week 10:</font><font size=\"1\" color=\"black\"> </font><font size=\"2\" color=\"black\"><br>\n\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t</font></span></span></h2>\n\t\t\t<hr size=2 width=\"100%\" align=left>\n\t\t\t<h2><span class=GramE><span style=\"font-size:10.0pt;font-family:Arial; color:red\">Data Sets and Online Resources:<br>\n\t\t\t\t\t</span></span></h2>\n\t\t\t<h4><a href=\"d1uci.zip\">Homework Dataset<br>\n\t\t\t\t</a></h4>\n\t\t\t<p><b><font size=\"2\">  </font></b><b><font size=\"2\">  </font></b></p>\n\t\t\t<h5>There are about 70 features (columns) and 50,000 data-cases (rows). You should ignore the first 4 columns as they do not represent features (attributes, inputs, covariates) and the last column is the label (response variable) that you need to predict. The typical performance on this data is between [0.77-0.8] Area Under The ROC Curve.</h5>\n\t\t\t<hr size=2 width=\"100%\" align=left>\n\t\t\t<h2><span class=GramE><span style=\"font-size:10.0pt;font-family:Arial; color:red\">Homework : <font size=\"-1\" color=\"black\">(always due the next Tuesday at 12pm)</font></span></span></h2>\n\t\t\t<h2><span class=GramE><span style=\"font-size:10.0pt;font-family:Arial; color:red\"><font size=\"-1\" color=\"#0033bb\">Week 1: Homework 1 [<a href=\"HWCS273ASpring10-1.doc\">doc</a>] [<a href=\"HWCS273ASpring10-1.pdf\">pdf</a>] (updated Th. April 1),<br>\n\t\t\t\t\t\t</font></span></span><span class=GramE><span style=\"font-size:10.0pt;font-family:Arial; color:red\"><font size=\"-1\" color=\"#0033bb\">Week 2: Homework 2 [<a href=\"HWCS273ASpring10-2.doc\">doc</a>] [<a href=\"HWCS273ASpring10-2.pdf\">pdf</a>]  <br>\n\t\t\t\t\t\t</font></span></span><span class=GramE><span style=\"font-size:10.0pt;font-family:Arial; color:red\"><font size=\"-1\" color=\"#0033bb\">Week 3: Homework 3 [<a href=\"HWCS273ASpring10-3.doc\">doc</a>] [<a href=\"HWCS273ASpring10-3.pdf\">pdf</a>]<br>\n\t\t\t\t\t\t</font></span></span><span class=GramE><span style=\"font-size:10.0pt;font-family:Arial; color:red\"><font size=\"-1\" color=\"#0033bb\">Week 4: Homework 4 [<a href=\"HWCS273ASpring10-4.doc\">doc</a>] [<a href=\"HWCS273ASpring10-4.pdf\">pdf</a>]</font><font size=\"2\" color=\"black\" face=\"Times New Roman\"><br>\n\t\t\t\t\t\t</font><font size=\"-1\" color=\"#0033bb\">Week 5: Homework 5 [<a href=\"HWCS273ASpring10-5.doc\">doc</a>] [<a href=\"HWCS273ASpring10-5.pdf\">pdf</a>] (updated Th. Apr. 29)</font></span></span><span class=GramE><span style=\"font-size:10.0pt;font-family:Arial; color:red\"><font size=\"2\" color=\"black\" face=\"Times New Roman\"><br>\n\t\t\t\t\t\t</font></span></span><span class=GramE><span style=\"font-size:10.0pt;font-family:Arial; color:red\"><font size=\"-1\" color=\"#0033bb\">Week 6: Homework 6 [<a href=\"HWCS273ASpring10-6.doc\">doc</a>] [<a href=\"HWCS273ASpring10-6.pdf\">pdf</a>] </font></span></span><font size=\"2\"><span class=GramE><span style=\"font-size:10.0pt;font-family:Arial; color:red\"><font color=\"black\" face=\"Times New Roman\"><br>\n\t\t\t\t\t\t\t</font></span></span></font><span class=GramE><span style=\"font-size:10.0pt;font-family:Arial; color:red\"><font size=\"-1\" color=\"#0033bb\">Week 7: Homework 7 [<a href=\"HWCS273ASpring10-7.doc\">doc</a>] [<a href=\"HWCS273ASpring10-7.pdf\">pdf</a>] (updated Th. May 13)</font></span></span><font size=\"2\"><span class=GramE><span style=\"font-size:10.0pt;font-family:Arial; color:red\"><font color=\"black\" face=\"Times New Roman\"><br>\n\t\t\t\t\t\t\t</font></span></span></font><span class=GramE><span style=\"font-size:10.0pt;font-family:Arial; color:red\"><font size=\"-1\" color=\"#0033bb\">Week 8: Homework 8 [<a href=\"HWCS273ASpring10-8.doc\">doc</a>] [<a href=\"HWCS273ASpring10-8.pdf\">pdf</a>] </font></span></span><font size=\"2\"><span class=GramE><span style=\"font-size:10.0pt;font-family:Arial; color:red\"><font color=\"black\" face=\"Times New Roman\"><br>\n\t\t\t\t\t\t\t</font></span></span></font><span class=GramE><span style=\"font-size:10.0pt;font-family:Arial; color:red\"><font size=\"-1\" color=\"#0033bb\">Week 9: Homework 9 [<a href=\"HWCS273ASpring10-9.doc\">doc</a>] [<a href=\"HWCS273ASpring10-9.pdf\">pdf</a>]</font><font size=\"2\" color=\"black\"><br>\n\t\t\t\t\t\t</font><font size=\"-1\" color=\"#0033bb\">Week 10:</font><font size=\"1\" color=\"black\"> </font><font size=\"2\" color=\"black\"><br>\n\t\t\t\t\t\t</font></span></span><font size=\"2\"><span class=GramE><span style=\"font-size:10.0pt;font-family:Arial; color:red\"><font color=\"black\" face=\"Times New Roman\"><br>\n\t\t\t\t\t\t\t</font></span></span></font></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt;font-family:Arial;color:black\">\n\t\t\t\t\t<hr size=2 width=\"100%\" align=left>\n\t\t\t\t</span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt;font-family:Arial;color:red\">Syllabus: <font size=\"-1\" color=\"black\">(incomplete)</font></span></h2>\n\t\t\t<h2><span class=GramE><span style=\"font-size:10.0pt;color:blue\">1</span></span><span style=\"font-size:10.0pt;color:blue\">:</span><span style=\"font-size:10.0pt; color:red\"> </span><span style=\"font-size:10.0pt\">introduction: overview, examples, goals, algorithm evaluation, statistics, kNN, logistic regression. </span></h2>\n\t\t\t<h2><span class=GramE><span style=\"font-size:10.0pt;color:blue\">2</span></span><span style=\"font-size:10.0pt;color:blue\">:</span><span style=\"font-size:10.0pt; color:red\"> </span><span style=\"font-size:10.0pt\">classification I: decision trees, random forests, bagging, boosting. </span></h2>\n\t\t\t<h2><span class=GramE><span style=\"font-size:10.0pt;color:blue\">3</span></span><span style=\"font-size:10.0pt;color:blue\">: </span><span style=\"font-size:10.0pt\">clustering &amp; dimensionality reduction:<span style=\"color:black\"> k-means, expectation-maximization, PCA. </span></span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt\"><span class=GramE><span style=\"color:blue\">4</span></span><span style=\"color:blue\">: </span>neural networks: <span class=SpellE>perceptron</span>, multi-layer networks, back-propagation. </span></h2>\n\t\t\t<h2><span class=GramE><span style=\"font-size:10.0pt;color:blue\">5:</span></span><span style=\"font-size:10.0pt;color:blue\"> </span><span style=\"font-size:10.0pt\">classification II: kernel methods &amp; support vector machines. </span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt;color:black\">\n\t\t\t\t\t<hr size=2 width=\"100%\" align=left>\n\t\t\t\t</span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt;font-family:Arial;color:red\">Textbook</span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt;color:black\">I am writing my own text which will be posted here and on the top of this webpage. Note that I will continuously update it during the course and feedback is appreciated.</span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt;color:#000099\"><br>\n\t\t\t\t\t\tThe textbook that will be used for this course is:</span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt;color:black\">1. C. Bishop: Pattern Recognition and Machine Learning</span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt;color:#000099\"><br>\n\t\t\t\t\t\tOptional side readings are:</span></h2>\n\t\t\t<h2><span style=\"font-size:10.0pt;color:black\">2. D. MacKay: Information Theory, Inference and Learning Algorithms<br>\n\t\t\t\t\t\t3. R.O. <span class=SpellE>Duda</span>, P.E. Hart, D. Stork: Pattern Classification<br>\n\t\t\t\t\t\t4. C.M. Bishop: Neural Networks for Pattern Recognition<br>\n\t\t\t\t\t\t5. T. <span class=SpellE>Hastie</span>, R. <span class=SpellE>Tibshirani</span>, J.H, Friedman: The Elements of Statistical Learning<br>\n\t\t\t\t\t\t6. B.D. Ripley: Pattern Recognition and Neural Networks<br>\n\t\t\t\t\t\t7. T. Mitchell: Machine Learning. <i style=\"mso-bidi-font-style:normal\">(http://www.cs.cmu.edu/~tom/mlbook.html)</i><br>\n\t\t\t\t</span></h2>\n\t\t</div>\n\t</body>\n\n</html>\n", "id": 2824.0}