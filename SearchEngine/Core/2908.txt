{"text": "CS 274A Probabilistic Learning Theory and Algorithms Winter 2 14 Time Mondays and Wednesdays 11 to 12 2 Location DBH 15 Course code 3496 Professor Padhraic Smyth Email smyth at ics uci edu Note for all class related emails make sure to put CS274 at the start of the subject line of your email Office Hours 1 to 11 3 Tuesdays DBH 4212 Syllabus dates may shift during the quarter but content will remain broadly the same Background Notes notes to accompany lectures pointers to relevant sections of the text and background readingHomeworksTextbooks recommended but not required you should read the relevant material in at least one of these texts Bayesian Reasoning and Machine Learning by David Barber Cambridge University Press PDF version freely available online Machine Learning A Probabilistic Perspective by Kevin Murphy MIT Press 2 12 Other Optional Reference Texts Pattern Recognition and Machine Learning by Chris Bishop 2 7 Springer Widely used comprehensive text on statistical learning was the standard text used in machine learning classes until the Murphy book appeared in 2 12 which covers more recent developments in the field Elements of Statistical Learning by Hastie Tibshirani and Friedman Springer 2 9 Excellent text on machine learning from a statistical perspective PDF version available freely online See also An Introduction to Statistical Learning with Applications in R 2 13 which is a more introductory applied version of the original book and is also available freely online All of Statistics A Concise Course in Statistical Inference by Larry Wasserman 2 4 Springer Written as a concise introduction to statistics for computer scientists Note that this list is not intended to be exhaustive there are several other excellent texts on machine learning that were published in recent years Course Goals Students will develop a comprehensive understanding of probabilistic approaches to learning from data Probabilistic learning is a key component in many areas within modern computer science including artificial intelligence data mining speech recognition computer vision bioinformatics and so forth The course will provide a tutorial introduction to the basic principles of probabilistic modeling and then demonstrate the application of these principles to the analysis development and practical use of machine learning algorithms Topics covered will include probabilistic modeling defining likelihoods parameter estimation using likelihood and Bayesian techniques probabilistic approaches to classification clustering and regression and related topics such as model selection and bias variance tradeoffs Prerequisites for taking this class Knowledge of basic concepts in probability multivariate calculus and linear algebra are required for this course Grading PolicyFinal grades will be based on a combination of homeworks and exams 3 homeworks 3 midterm and 4 final Your lowest scoring homework will be dropped and not included in your score No late homeworks Academic Honesty Academic honesty is taken seriously For homework problems or programming assignments you are allowed to discuss the problems or assignments verbally with other class members but under no circumstances can you look at or copy anyone else s written solutions or code relating to homework problems or programming assignments All problem solutions and code submitted must be material you have personally written during this quarter except for any library or utility functions which we supply Failure to adhere to this policy can result in a student receiving a failing grade in the class It is the responsibility of each student to be familiar with UCI s academic honesty policies UCI Catalog Course Description Probabilistic Learning Theory and Algorithms A unified probabilistic framework for learning algorithms Classical pattern recognition algorithms probabilistic mixture models kernel methods hidden Markov models among others Multivariate data analysis concepts for classification and clustering Methodologies such as cross validation and bootstrap Prerequisites basic calculus and linear algebra ", "_id": "http://www.ics.uci.edu/~smyth/courses/cs274/", "title": "cs 274a: main page", "html": "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\"><head>\r\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=ISO-8859-1\" />\r\n<title>CS 274A: Main Page</title></head>\r\n<body style=\"color: rgb(0, 0, 0); background-color: white;\" alink=\"#000099\" link=\"#000099\" vlink=\"#990099\"><div style=\"text-align: center;\">\r\n</div><center style=\"font-family: Calibri;\">\r\n\r\n  <h2><big><font color=\"#330033\"><small> CS 274A: Probabilistic Learning:\r\n    Theory and Algorithms, Winter 2014</small></font></big></h2><hr style=\"width: 100%; height: 2px;\" />\r\n</center>\r\n<ul>\r\n  <li style=\"font-family: Calibri;\"> <b>Time:</b>&nbsp; Mondays and Wednesdays, 11:00 to 12:20 </li>\r\n  <li style=\"font-family: Calibri;\"> <b>Location: </b>DBH 1500&nbsp;</li>\r\n  <li style=\"font-family: Calibri;\"><span style=\"font-weight: bold;\">Course code</span>: 34960</li>\r\n  <li style=\"font-family: Calibri;\"> <b>Professor: </b><a href=\"http://www.ics.uci.edu/%7Esmyth\">Padhraic\r\n    Smyth</a></li>\r\n  <ul><li style=\"font-family: Calibri;\"> <b>Email:</b> smyth at ics.uci.edu<font face=\"Verdana, Arial, Helvetica, sans-serif\"><font face=\"Verdana, Arial, Helvetica, sans-serif\"><font face=\"Verdana, Arial, Helvetica, sans-serif\"><span style=\"font-family: Calibri;\">. Note: for all class-related emails make sure to put [CS274] at the start of the subject line of your email</span></font></font></font></li></ul>\r\n  <ul><li style=\"font-family: Calibri;\"> <b>Office Hours:</b> 10:00 &nbsp;to 11:30, Tuesdays, DBH 4212&nbsp;<br /></li></ul><li style=\"font-family: Calibri;\"><a href=\"syllabus.xhtml\"><span style=\"font-weight: bold;\">Syllabus</span></a> &nbsp;(dates may shift during the quarter but content will remain broadly the same)</li><li style=\"font-family: Calibri;\"><font style=\"font-weight: bold;\"><a href=\"background_notes.html\">Background Notes</a>:</font><font> notes to accompany lectures, pointers to relevant sections of the text, and background reading</font></li><li style=\"font-family: Calibri;\"><font><a style=\"font-weight: bold;\" href=\"homeworks.xhtml\">Homeworks</a><br /></font></li><font face=\"Verdana, Arial, Helvetica, sans-serif\"><br /><font face=\"Verdana, Arial, Helvetica, sans-serif\"><li><font face=\"Verdana, Arial, Helvetica, sans-serif\"><font face=\"Verdana, Arial, Helvetica, sans-serif\"><small><big><span style=\"font-weight: bold; font-family: Calibri;\">Textbooks </span><span style=\"font-family: Calibri;\">(recommended but not required: you should read the relevant material in at least one of these texts):</span><br style=\"font-family: Calibri;\" /></big></small></font></font><ul><li><font face=\"Verdana, Arial, Helvetica, sans-serif\"><font face=\"Verdana, Arial, Helvetica, sans-serif\"><small><span style=\"font-weight: normal;\"><big><span style=\"font-family: Calibri;\"><span style=\"font-weight: bold;\"></span></span></big></span></small></font></font><a style=\"font-family: Calibri;\" href=\"http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.Online\">Bayesian Reasoning and Machine Learning</a><span style=\"font-family: Calibri;\">, by David Barber, Cambridge University Press (PDF version freely available online).&nbsp;</span></li><li style=\"font-family: Calibri;\"><a href=\"http://www.cs.ubc.ca/%7Emurphyk/MLbook/index.html\">Machine Learning: A Probabilistic Perspective</a>, by Kevin Murphy, MIT Press, 2012.<span style=\"font-weight: bold;\">&nbsp;<br /></span></li></ul></li><li><font face=\"Verdana, Arial, Helvetica, sans-serif\"><font face=\"Verdana, Arial, Helvetica, sans-serif\"><font face=\"Verdana, Arial, Helvetica, sans-serif\"><font face=\"Verdana, Arial, Helvetica, sans-serif\"><small><big><span style=\"font-weight: bold; font-family: Calibri;\">Other Optional Reference Texts:&nbsp;</span><br style=\"font-family: Calibri;\" /></big></small></font></font><ul><li><small><a href=\"http://research.microsoft.com/en-us/um/people/cmbishop/prml/index.htm\">Pattern Recognition and Machine Learning</a>,\r\nby Chris Bishop, 2007, Springer. Widely used comprehensive text on\r\nstatistical learning - was the standard text used in machine learning\r\nclasses until the Murphy book appeared in 2012 (which covers more\r\nrecent developments in the field).</small><font face=\"Verdana, Arial, Helvetica, sans-serif\"><font face=\"Verdana, Arial, Helvetica, sans-serif\"><small><span style=\"font-weight: normal;\"><big><span style=\"font-family: Calibri;\"><span style=\"font-weight: bold;\"></span></span></big></span></small></font></font><span style=\"font-family: Calibri;\"></span></li><li><span style=\"font-family: Calibri;\"><a href=\"http://statweb.stanford.edu/%7Etibs/ElemStatLearn/\">Elements of Statistical Learning</a>,\r\nby Hastie, Tibshirani, and Friedman,&nbsp; Springer, 2009. Excellent\r\ntext on machine learning from a statistical perspective. PDF version\r\navailable freely online. See also <a href=\"http://www-bcf.usc.edu/%7Egareth/ISL/\">An Introduction to Statistical Learning with Applications in R,</a> 2013, which is a more introductory/applied version of the original book, and is also available freely online.</span></li><li><span style=\"font-weight: normal;\"><a style=\"font-family: Calibri;\" href=\"http://www.stat.cmu.edu/%7Elarry/all-of-statistics/index.html\">All of Statistics: A Concise Course in Statistical Inference</a><span style=\"font-family: Calibri;\">, by Larry Wasserman, 2004, Springer. Written as a concise introduction to statistics for computer scientists.<span style=\"font-weight: bold;\">&nbsp;</span></span></span></li><li><span style=\"font-weight: normal;\"><span style=\"font-family: Calibri;\">Note\r\nthat this list is not intended to be exhaustive - there are several\r\nother excellent texts on machine learning that were published in recent\r\nyears.<span style=\"font-weight: bold;\"><br /></span></span></span></li></ul></font></font><font face=\"Verdana, Arial, Helvetica, sans-serif\"><font face=\"Verdana, Arial, Helvetica, sans-serif\"><font face=\"Verdana, Arial, Helvetica, sans-serif\"><small><span style=\"font-weight: normal;\"><big><span style=\"font-family: Calibri;\"><span style=\"font-weight: bold;\"><br /></span></span></big></span></small></font></font></font></li><font face=\"Verdana, Arial, Helvetica, sans-serif\"><li style=\"font-weight: bold; font-family: Calibri;\"><font face=\"Verdana, Arial, Helvetica, sans-serif\"><font face=\"Verdana, Arial, Helvetica, sans-serif\"><span style=\"font-weight: normal;\"></span><small><small><span style=\"font-weight: normal;\"><big><big><span style=\"font-weight: bold; font-family: Calibri;\">Course Goals:</span><span style=\"font-family: Calibri;\"> </span></big></big></span></small></small></font></font><font face=\"Verdana, Arial, Helvetica, sans-serif\"><font face=\"Verdana, Arial, Helvetica, sans-serif\"><span style=\"font-weight: normal;\"><span style=\"font-family: Calibri;\">Students will\r\ndevelop a comprehensive understanding of probabilistic approaches to\r\nlearning from data. Probabilistic learning is a key component in many\r\nareas within modern computer science, including artificial\r\nintelligence, data mining, speech recognition, computer vision,\r\nbioinformatics, and so forth. The course will provide a tutorial\r\nintroduction to the basic principles of probabilistic modeling and then\r\ndemonstrate the application of these principles to the analysis,\r\ndevelopment,\r\nand practical use of machine learning algorithms.&nbsp; Topics covered\r\nwill include probabilistic modeling, defining likelihoods, parameter\r\nestimation using likelihood and Bayesian techniques,&nbsp;probabilistic\r\napproaches to classification, clustering, and regression,\r\nand related topics such as model selection and bias/variance tradeoffs.&nbsp;</span></span></font></font><font face=\"Verdana, Arial, Helvetica, sans-serif\"><font face=\"Verdana, Arial, Helvetica, sans-serif\"><small><small><span style=\"font-weight: normal;\"><big><br /></big></span></small></small></font></font></li></font></font></font><font style=\"font-family: Calibri;\" face=\"Verdana, Arial, Helvetica, sans-serif\"><font style=\"font-family: Calibri;\"><li style=\"font-weight: bold;\"><p><span style=\"font-weight: normal;\"><span style=\"font-weight: bold;\">Prerequisites for taking this class:</span> Knowledge of\r\nbasic concepts in probability, multivariate calculus, and linear\r\nalgebra\r\nare required for this course.&nbsp;<span style=\"font-weight: bold;\">&nbsp;</span></span></p></li></font></font></ul><ul><font face=\"Verdana, Arial, Helvetica, sans-serif\"><font face=\"Verdana, Arial, Helvetica, sans-serif\"><font face=\"Verdana, Arial, Helvetica, sans-serif\"><li style=\"font-weight: bold; font-family: Calibri;\"><b style=\"font-family: Calibri;\">Grading Policy</b><span style=\"font-weight: normal; font-family: Calibri;\"></span><span style=\"font-weight: normal; font-family: Calibri;\"><br />Final grades will be based on a combination of\r\nhomeworks and exams: 30% homeworks, 30% midterm, and 40%\r\nfinal. Your lowest scoring homework will be dropped and not included in your score. No late homeworks.<br /><br /><span style=\"font-weight: bold;\"> </span></span><span style=\"font-weight: normal;\"></span></li><li style=\"font-weight: bold; font-family: Calibri;\"><b>Academic Honesty&nbsp;</b><br /><font style=\"font-weight: normal;\">Academic\r\nhonesty is taken seriously. For homework problems or programming\r\nassignments you are allowed to discuss the problems or assignments\r\nverbally with\r\nother class members, but under no circumstances can you look at or copy\r\nanyone else's written solutions or code relating to homework problems\r\nor programming assignments. All problem solutions and code submitted\r\nmust be material you have personally written during this quarter,\r\nexcept\r\nfor any library or utility functions which we supply. Failure to adhere\r\nto this policy can result in a student receiving a failing grade in the\r\nclass. It is the responsibility of each student to be familiar with\r\n<a href=\"http://www.editor.uci.edu/catalogue/appx/appx.2.htm#academic\">\r\nUCI's academic honesty policies</a>.&nbsp;<br /><br /></font></li><li style=\"font-weight: bold;\"><span style=\"font-family: Calibri;\">UCI Catalog Course Description:&nbsp; </span><span style=\"font-weight: normal; font-family: Calibri;\"><br />Probabilistic Learning:\r\nTheory and Algorithms: A unified probabilistic framework for learning\r\nalgorithms. Classical pattern recognition algorithms, probabilistic\r\nmixture models, kernel methods, hidden Markov models, among others.\r\nMultivariate data analysis concepts for classification and clustering.\r\nMethodologies such as cross-validation and bootstrap. Prerequisites:\r\nbasic calculus and linear algebra. </span><font style=\"font-family: Calibri;\" face=\"Verdana, Arial, Helvetica, sans-serif\">&nbsp; </font><font face=\"Verdana, Arial, Helvetica, sans-serif\"><font face=\"Verdana, Arial, Helvetica, sans-serif\"><br style=\"font-family: Calibri;\" />\r\n<br />\r\n</font>\r\n</font><p><font face=\"Verdana, Arial, Helvetica, sans-serif\"> </font></p>\r\n<hr />\r\n<font face=\"Verdana, Arial, Helvetica, sans-serif\"><font face=\"Verdana, Arial, Helvetica, sans-serif\"><br />\r\n<br />\r\n<br />\r\n</font>\r\n</font></li><font face=\"Verdana, Arial, Helvetica, sans-serif\"></font></font></font></font></ul></body></html>", "id": 2908.0}