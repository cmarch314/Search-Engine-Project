{"text": "ACM Computing Surveys 28A 4 December 1996 http www cs jhu edu goodrich pubs io html Copyright 1996 by the Association for Computing Machinery Inc See the permissions statement below Strategic Directions in Computing Research Working Group on Storage I O Issues in Large Scale Computing Position statement Thomas H Cormen Dartmouth College Department of Computer Science 6211 Sudikoff Laboratory Hanover NH 3755 351 USA thc cs dartmouth edu http www cs dartmouth edu thc Michael T Goodrich The Johns Hopkins University Department of Computer Science Whiting School of Engineering Baltimore MD 21218 http www ics uci edu goodrich Abstract We present the challenge of synthesizing a coherent model that combines the best aspects of the Parallel Disk Model and Bulk Synchronous Parallel models to develop and analyze algorithms that use parallel I O computation and communication Categories and Subject Descriptors B 3 2 Memory Structures Design Styles Mass storage e g magnetic optical Primary memory B 4 4 Input Output and Data Communications Performance Analysis and Design Aids Formal models Worst case analysis D 1 3 Programming Techniques Concurrent Programming Parallel programming D 4 2 Operating Systems Storage Management Secondary Storage D 4 4 Operating Systems Communications Management Input Output Message sending Network communication E 2 Data Storage Representations Contiguous representations E 5 Files Sorting searching F 1 2 Computation by Abstract Devices Modes of Computation Parallelism and concurrency F 2 2 Analysis of Algorithms and Problem Complexity Nonnumerical Algorithms and Problems Sorting and searching General Terms Algorithms Design Languages Performance Theory Additional Key Words and Phrases I O external memory secondary memory communication disk drive parallel disks sorting Parallel Disk Model Bulk Synchronous Parallel Model A Bridging Model for Parallel Computation Communication and I O The past decade has seen the introduction of new and useful models for analyzing the computational and communication complexities of parallel algorithms as well as useful models to measure I O complexity Yet no useful model measures all of computational communication and I O complexity simultaneously Usefulness of a model implies two characteristics First the model should be realistic in the sense that its prediction for any algorithm should correspond to observed behavior of real systems Second the model should be simple enough to use and understand that one can design analyze and implement algorithms without having had extensive experience with the model We maintain that the Bulk Synchronous Parallel or BSP model Valiant 199 and LogP Culler et al 1993 models are useful for computational and communication complexities of parallel algorithms The Parallel Disk Model or PDM Vitter Shriver 1994 is useful for I O complexity The BSP and LogP models however ignore I O and the PDM does not account for computation or communication Because we think of I O as so much slower than computation or communication the PDM apparently captures the most salient factor in the wall clock time for algorithms that use parallel I O What is apparent may not be the case however Early experiences with algorithms implemented in the PDM indicate that although wall clock time for a given algorithm follows the prediction of the model the algorithms themselves are not I O bound Even with synchronous i e blocking I O the time spent waiting for I O is typically less than 5 of the total wall clock time This behavior suggests that each parallel disk access gives rise to a given amount of computation and communication for a particular algorithm A sorting algorithm for example might repeatedly process memoryloads of data by performing a large parallel read an in memory sort across all processors and a large parallel write The time to perform the in memory sorts might exceed the combined times of the parallel reads and writes although it is roughly the same among the memoryloads Typical algorithms developed for the PDM are similar to this hypothetical sorting algorithm in that they make repeated passes over the data and each pass repeatedly reads in a large amount of data processes it and writes out a large amount of data Processing time including communication tends to be about the same each time for a given algorithm Of course it varies from algorithm to algorithm If these early observations continue to hold as we gain more experience in implementing algorithms for the PDM we will draw the conclusion that the PDM s predictive power is helpful for analyzing I O time but limited by omitting computation and communication Note however a striking similarity between the BSP model and typical PDM algorithms bulk processing In the BSP model for example communcation in a network of processors is considered to be the prime computational bottleneck hence a computation is organized as a sequence of rounds where each round consists of each processor performing computations on its internal memory followed by the sending and receiving of a limited number of messages Rounds and communication in BSP algorithms are like memoryload processing and I O respectively in PDM algorithms The challenge therefore is to synthesize a coherent model that combines the best aspects of the PDM and BSP models to develop and analyze algorithms that use parallel I O computation and communication Along with such a model we need programming primitives to enable algorithm implementation under the model These primitives must be portable and have performance matching the model s requirements We view developing such a model as a challenge because we believe that it will be difficult to simultaneously satisfy the two requirements that it be realistic yet easy to use Our concern is that a realistic model may have so many parameters as to make it unusable The PDM without considering processing has four parameters problem size memory size disk block size and disk count The BSP model also has four parameters problem size processor count latency of the network and the gap time between consecutive messages in pipelined computations Thus some natural questions to ask include the following Can all these parameters be merged in some synthesis Is there a block size notion in BSP that might be consistent with the PDM S block size What is the right set of at most four or five important parameters In summary we think that it would be valuable to propose a bridging parallel computational model that incorporates computation communication and I O in an accurate and easy to use manner We hope that discussions at the workshop will lead to such a model References Alpern et al 1994 Alpern B Carter L Feig E and Selker T 1994 The Uniform Memory Hierarchy Model of Computation Algorithmica 12 2 3 August and September 1994 pages 72 1 9 Cormen Goodrich 1996 Cormen T H and Goodrich M T 1996 Position Statement Strategic Directions in Computing Research Working Group on Storage I O Issues in Large Scale Computing Computing Surveys 28A 4 December 1996 http www cs jhu edu goodrich pubs io html Cormen Hirschl 1996 Cormen T H and Hirschl M 1996 Early Experiences in Evaluating the Parallel Disk Model with the ViC Implementation Parallel Computing to appear Also available as Dartmouth College Computer Science Technical Report TR96 293 at ftp ftp cs dartmouth edu TR TR96 293 ps Z Culler et al 1993 Culler D Karp R Patterson D Sahay A Schauser K E Santos E Subramonian R and von Eicken T 1993 LogP Towards a Realistic Model of Parallel Computation Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming May 1993 pages 1 12 Valiant 199 Valiant L G 199 A Bridging Model for Parallel Computation Communications of the ACM 33 8 August 199 pages 1 3 111 Vitter Shriver 1994a Vitter J S and Shriver E A M 1994 Algorithms for Parallel Memory I Two level Memories Algorithmica 12 2 3 August and September 1994 pages 11 147 Vitter Shriver 1994b Vitter J S and Shriver E A M 1994 Algorithms for Parallel Memory II Hierarchical Multilevel Memories Algorithmica 12 2 3 August and September 1994 pages 148 169 Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page Copyrights for components of this work owned by others than ACM must be honored Abstracting with credit is permitted To copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission and or a fee Request permissions from Publications Dept ACM Inc fax 1 212 869 481 or permissions acm org Last modified Mon Oct 21 19 18 4 EDT Michael T Goodrich", "_id": "http://www.ics.uci.edu/~goodrich/pubs/io.html", "title": "acm computing surveys: sdcr working group on storage i/o, \ncormen/goodrich", "html": "<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML//EN\">\n<HTML> <HEAD>\n<TITLE>ACM Computing Surveys: SDCR Working Group on Storage I/O, \nCormen/Goodrich</TITLE>\n</HEAD>\n\n<BODY bgcolor=\"#ffffff\">\n\n<P>\n<small>\n\n<em><a href=\"http://www.acm.org/\">ACM</a> <a\nhref=\"http://www.cs.jhu.edu/~goodrich/pubs/io.html\">Computing\nSurveys</a></em> <b>28A</b>(4), December 1996,\nhttp://www.cs.jhu.edu/~goodrich/pubs/io.html.  Copyright &#169;\n1996 by the Association for Computing Machinery, Inc.  See the <a\nhref=\"#permissions-statement\">permissions statement</a> below.\n\n</small>\n</P>\n\n\n<BR>\n\n<center>\n<h1><a href=\"http://www.medg.lcs.mit.edu/sdcr/\">\n        Strategic Directions in Computing Research</a></h1>\n<p>\n<h1><a href=\"http://www.cs.duke.edu/~jsv/SDCR96-IO/SDCR96-IO.html \">Working\n        Group on Storage I/O Issues in Large-Scale Computing</a></h1>\n<p>\n<h1>Position statement</h1>\n</center>\n\n<BR>\n\n<P>\n<strong><a href=\"http://www.cs.dartmouth.edu/~thc\">Thomas H. Cormen</a></strong>\n<BR>\n<address>\n<a href=\"http://www.dartmouth.edu/\">Dartmouth College</a>,\n<a href=\"http://www.cs.dartmouth.edu/\">Department of Computer Science</a><BR>\n6211 Sudikoff Laboratory, Hanover, NH 03755-3510, USA<BR>\n<a href=\"mailto:thc@cs.dartmouth.edu\">thc@cs.dartmouth.edu</a>, \n<a\nhref=\"http://www.cs.dartmouth.edu/~thc\"\n>http://www.cs.dartmouth.edu/~thc</a><BR>\n</address>\n\n<BR>\n<strong><a href=\"http://www.cs.jhu.edu/goodrich/home.html\">Michael T. Goodrich</a></strong>\n<BR>\n<address>\n<a href=\"http://www.jhu.edu/\">The Johns Hopkins University</a>,\n<a href=\"http://www.cs.jhu.edu/\">Department of Computer Science</a><BR>\nWhiting School of Engineering, Baltimore, MD 21218<BR>\n<a href=\"http://www.ics.uci.edu/~goodrich/\">http://www.ics.uci.edu/~goodrich/</a><BR>\n</address>\n\n\n<BR>\n<BR>\n\n<blockquote>\n<HR>\n\n<strong>Abstract:</strong>\n\nWe present the challenge of synthesizing a coherent model that\ncombines the best aspects of the Parallel Disk Model and Bulk\nSynchronous Parallel models to develop and analyze algorithms that use\nparallel I/O, computation, and communication.\n\n<small>\n\n<P>\n\nCategories and Subject Descriptors: \nB.3.2 <b>[Memory Structures]</b>: Design Styles - <i>Mass storage\n(e.g., magnetic, optical), Primary memory</i>;\nB.4.4 <b>[Input/Output and Data Communications]</b>:\nPerformance Analysis and Design Aids -\n<i>Formal models, Worst-case analysis</i>;\nD.1.3 <b>[Programming Techniques]</b>: Concurrent Programming - <i>\nParallel programming</i>;\nD.4.2 <b>[Operating Systems]</b>: Storage Management - <i>Secondary\n\t      Storage</i>; \nD.4.4 <b>[Operating Systems]</b>: Communications Management -\n\t  <i>Input/Output; Message sending; Network communication</i>;  \nE.2 <b>[Data Storage Representations]</b>: <i>Contiguous\n\t      representations</i>; \nE.5 <b>[Files]</b>: <i>Sorting/searching</i>;\nF.1.2 <b>[Computation by Abstract Devices]</b>: Modes of Computation -\n<i>Parallelism and concurrency</i>;\nF.2.2 <b>[Analysis of Algorithms and Problem Complexity]</b>:\nNonnumerical Algorithms and Problems - <i>Sorting and searching</i>; \n\n<P>\n\nGeneral Terms: Algorithms, Design, Languages, Performance, Theory.\n\n<P>\n\nAdditional Key Words and Phrases: I/O, external memory, secondary\nmemory, communication, disk drive, parallel disks, sorting, Parallel\nDisk Model, Bulk Synchronous Parallel Model.\n\n</small>\n\n<HR>\n</blockquote>\n\n<BR>\n\n<h2>A Bridging Model for Parallel Computation, Communication, and I/O</h2>\n\n<P>\nThe past decade has seen the introduction of new and useful models for\nanalyzing the computational and communication complexities of parallel\nalgorithms, as well as useful models to measure I/O complexity.  Yet\nno useful model measures all of computational, communication, and I/O\ncomplexity simultaneously.\n\n<P>\nUsefulness of a model implies two characteristics.  First, the model\nshould be realistic in the sense that its prediction for any algorithm\nshould correspond to observed behavior of real systems.  Second, the\nmodel should be simple enough to use and understand that one can\ndesign, analyze, and implement algorithms without having had extensive\nexperience with the model.\n\n<P>\nWe maintain that the Bulk Synchronous Parallel, or BSP, model <a\nhref=\"#Valiant90\">[Valiant 1990]</a> and LogP <a\nhref=\"#CullerKaPaSaScSaSuEi93\">[Culler et al. 1993]</a> models are\nuseful for computational and communication complexities of parallel\nalgorithms.  The Parallel Disk Model, or PDM, <a\nhref=\"#VitterSh94a\">[Vitter Shriver 1994]</a> is useful for I/O\ncomplexity.  The BSP and LogP models, however, ignore I/O, and the PDM\ndoes not account for computation or communication.  Because we think\nof I/O as so much slower than computation or communication, the PDM\napparently captures the most salient factor in the wall-clock time for\nalgorithms that use parallel I/O.\n\n<P>\nWhat is apparent may not be the case, however.\n\n<P>\n<a href=\"#CormenHi96\">Early experiences</a> with algorithms\nimplemented in the PDM indicate that although wall-clock time for a\ngiven algorithm follows the prediction of the model, the algorithms\nthemselves are not I/O bound.  Even with synchronous (i.e., blocking)\nI/O, the time spent waiting for I/O is typically less than 50% of the\ntotal wall-clock time.  This behavior suggests that each parallel disk\naccess gives rise to a given amount of computation and communication\nfor a particular algorithm.\n\n<P>\nA sorting algorithm, for example, might repeatedly process\n\"memoryloads\" of data by performing a large parallel read, an\nin-memory sort across all processors, and a large parallel write.  The\ntime to perform the in-memory sorts might exceed the combined times of\nthe parallel reads and writes, although it is roughly the same among\nthe memoryloads.  Typical algorithms developed for the PDM are similar\nto this hypothetical sorting algorithm in that they make repeated\npasses over the data and each pass repeatedly reads in a large amount\nof data, processes it, and writes out a large amount of data.\nProcessing time (including communication) tends to be about the same\neach time for a given algorithm.  Of course it varies from algorithm\nto algorithm.\n\n<P>\nIf these early observations continue to hold as we gain more\nexperience in implementing algorithms for the PDM, we will draw the\nconclusion that the PDM's predictive power is helpful (for analyzing\nI/O time) but limited (by omitting computation and communication).\n\n<P>\nNote, however, a striking similarity between the BSP model and typical\nPDM algorithms: bulk processing.  In the BSP model, for example,\ncommuncation in a network of processors is considered to be the prime\ncomputational bottleneck; hence, a computation is organized as a\nsequence of rounds, where each round consists of each processor\nperforming computations on its internal memory, followed by the\nsending and receiving of a limited number of messages.  Rounds and\ncommunication in BSP algorithms are like memoryload processing and\nI/O, respectively, in PDM algorithms.\n\n<P>\nThe challenge, therefore, is to synthesize a coherent model that\ncombines the best aspects of the PDM and BSP models to develop and\nanalyze algorithms that use parallel I/O, computation, and\ncommunication.  Along with such a model, we need programming\nprimitives to enable algorithm implementation under the model.  These\nprimitives must be portable and have performance matching the model's\nrequirements.\n\n<P>\nWe view developing such a model as a challenge because we believe that\nit will be difficult to simultaneously satisfy the two requirements\nthat it be realistic yet easy to use.  Our concern is that a realistic\nmodel may have so many parameters as to make it unusable.  The PDM,\nwithout considering processing, has four parameters: problem size,\nmemory size, disk block size, and disk count.  The BSP model also has\nfour parameters: problem size, processor count, latency of the\nnetwork, and the \"gap\" time between consecutive messages in pipelined\ncomputations.  Thus, some natural questions to ask include the\nfollowing:\n<UL>\n<LI>Can all these parameters be merged in some synthesis?  \n\n<LI>Is there a block size notion in BSP that might be consistent with\nthe PDM'S block size?\n\n<LI>What is the right set of at most four or five important parameters?\n</UL>\n\n<P>\nIn summary, we think that it would be valuable to propose a bridging\nparallel computational model that incorporates computation,\ncommunication and I/O in an accurate and easy to use manner.  We hope\nthat discussions at the workshop will lead to such a model.\n\n<h2>References</h2>\n\n<dl>\n  <dt><a name=\"AlpernCaFeSe94\"><b>\n      [Alpern et al. 1994]</b></a></dt>\n  <dd>Alpern, B., Carter, L., Feig, E., and Selker, T., 1994.\n      The Uniform Memory Hierarchy Model of Computation,\n      <em>Algorithmica</em>, 12:2/3, August and September 1994,\n      pages 72-109.\n\n  <p>\n  <dt><a name=\"CormenGo96\"><b>\n      [Cormen Goodrich 1996]</b></a></dt> \n  <dd>Cormen, T. H., and Goodrich, M. T., 1996.\n      <a href=\"http://www.cs.jhu.edu/~goodrich/pubs/io.html\">\n       Position Statement</a>, \n       Strategic Directions in Computing Research:   \n       Working Group on Storage I/O Issues in Large-Scale Computing,\n      <em><a\n      href=\"http://www.acm.org/surveys/\">Computing Surveys</a></em>,\n      <b>28A</b>(4), December 1996,\n      <a href = \"http://www.cs.jhu.edu/~goodrich/pubs/io.html\">\n       http://www.cs.jhu.edu/~goodrich/pubs/io.html</a>.\n\n  <p>\n  <dt><a name=\"CormenHi96\"><b>\n      [Cormen Hirschl 1996]</b></a></dt>\n  <dd>Cormen, T. H., and Hirschl, M., 1996.\n      Early Experiences in Evaluating the Parallel Disk Model with the\n      ViC* Implementation, <em>Parallel Computing</em>, to appear.  Also\n      available as Dartmouth College Computer Science Technical Report\n      TR96-293 at <a href=\"ftp://ftp.cs.dartmouth.edu/TR/TR96-293.ps.Z\">\n      ftp://ftp.cs.dartmouth.edu/TR/TR96-293.ps.Z</a>\n\n  <p>\n  <dt><a name=\"CullerKaPaSaScSaSuEi93\"><b>\n      [Culler et al. 1993]</b></a></dt>\n  <dd>Culler, D., Karp, R., Patterson, D., Sahay, A., Schauser, K. E.,\n      Santos, E., Subramonian, R., and von Eicken, T. 1993.\n      LogP: Towards a Realistic Model of Parallel Computation,\n      <em>Proceedings of the Fourth ACM SIGPLAN Symposium on Principles\n      and Practice of Parallel Programming</em>, May 1993, pages 1-12.\n\n  <p>\n  <dt><a name=\"Valiant90\"><b>\n      [Valiant 1990]</b></a></dt> \n  <dd>Valiant, L. G., 1990.\n      A Bridging Model for Parallel Computation,\n      <em>Communications of the ACM</em>, \n      33:8, August 1990, pages 103-111.\n\n  <p>\n  <dt><a name=\"VitterSh94a\"><b>\n      [Vitter Shriver 1994a]</b></a></dt>\n  <dd>Vitter, J. S., and Shriver, E. A. M., 1994.\n      Algorithms for Parallel Memory I: Two-level Memories,\n      <em>Algorithmica</em>,\n      12:2/3, August and September 1994, pages 110-147.\n\n  <p>\n  <dt><a name=\"VitterSh94b\"><b>\n      [Vitter Shriver 1994b]</b></a></dt>\n  <dd>Vitter, J. S., and Shriver, E. A. M., 1994.\n      Algorithms for Parallel Memory II: Hierarchical Multilevel Memories,\n      <em>Algorithmica</em>,\n      12:2/3, August and September 1994, pages 148-169.\n\n</dl>\n\n\n<P>\n<hr>\n\n<P><a name=\"permissions-statement\"><small>Permission to make digital\nor hard copies of part or all of this work for personal or classroom\nuse is granted without fee provided that copies are not made or\ndistributed for profit or commercial advantage and that copies bear\nthis notice and the full citation on the first page.  Copyrights for\ncomponents of this work owned by others than ACM must be honored.\nAbstracting with credit is permitted.  To copy otherwise, to\nrepublish, to post on servers, or to redistribute to lists, requires\nprior specific permission and/or a fee.  Request permissions from\nPublications Dept, ACM Inc., fax +1 (212) 869-0481, or\n<TT>permissions@acm.org</TT>.</small></P>\n\n\n<P>\n<hr>\n<!-- hhmts start -->\nLast modified: Mon Oct 21 19:18:04 EDT\n<!-- hhmts end -->\n<address><a href=\"http://www.ics.uci.edu/~goodrich/\">Michael T.  Goodrich</a>\n</address>\n\n</BODY> </HTML>\n\n", "id": 1836.0}