{"text": "Afternoon 1 35 PM AOLpress and AOLserver Long Long started by informing everyone that GNNpress and GNNserver are now known as AOLpress and AOLserver In AOLserver the current version of resource is stored in the file system while the meta data indices etc are stored in a database Long noted that people think of their web sites in terms of a hierarchy and hence it is disorienting for the server to place a resource at a location other than the one specified by the user With regard to server side includes and other dynamic content AOL has no strong solution for getting to the actual source Their current approach disallows editing a page with dynamic content However AOLpress users seem to be doing it anyway although it is not clear what mechanism they use His recommendation for how to get a document s source before server side includes processing is to use content negotiation when requesting the source and asking for MIME type text x html ssi Unfortunately this approach does not handle the case where the source is not HTML A discussion then ensued about pretty printing canonicalizing the HTML written by an authoring tool There were some questions about how much canonicalizing an authoring tool should attempt Long mentioned that AOL used to get grief from customers because their tool automatically cleans up its HTML output but they no longer do since the tool now additionally pretty prints the HTML source It appears that users perceive enough extra value in the pretty printing of the generated HTML that they are willing to accept the cleanup Masinter There are people who are claiming to do HTML authoring who want their mail client and mail servers to send around marked up mail They are using a kind of HTML that they make human readable through operations such as centering HTML source text between CENTER tags Whitehead Is there an interoperability concern with regard to pretty printing Long Probably not I would be happy if we just made it so that everybody s client could do a PUT to everybody s server That would be the most basic form of interoperability everything else is icing on the cake Nielsen The W3C could put up a playground where everyone could test out the PUT interoperability of their authoring tools so long as we don t get into the test suite business Long mentioned that the focus of his talk is not interoperability He followed Dan Connolly s suggestion to go through your system and find out where you feel fuzzy and the least likely to be doing something in the same way as everyone else Continuing his presentation Long described how moving pages brings up the issue of how to move relative links Moving image files is easy because the IMG tag indicates the image is considered part of the document and should be moved However moving audio files is not as easy because they are included via a A HREF tag which could refer to a close or distant resource The best solution to this problem is to employ collections and move mini webs Unfortunately many beginning users do not use the AOLpress mini web functionality Another solutions is to have a usage practice where relative URLs are employed for references internal to a collection and external URLs are used for references outside of it This led into a discussion on the semantics of x y style relative URLs I e they re not foo or http Current versioning implementation slide Long reported on a prototype effort to add versioning capability to the AOLserver In this prototype reads employ a Content Version header to retrieve a stated version of a resource Writes provide a Derived From header so the server knows how to check in the version of the resource The server prevents dirty writes This implementation solves the CREATE problem i e it does not need a separate CREATE method for the initial creation of a resource because specifying a null Derived From header directs the server to perform a resource creation Since this prototype was completed the header syntax has changed for HTTP 1 1 An incomplete report on this is available at URL http staff navisoft com users d dave w3cwd wd version 96 128 htm As an aside Long mentioned that users really wanted to know if they were going to overwrite a document This was accommodated by providing a dialog box with OK and Cancel options whenever a document was about to be overwritten This required performing a HEAD operation on the resource before a PUT could take place At this point there was a discussion of entity tags Etags an opaque string which identifies a snapshot and their potential use in versioning Long believes entity tags are sufficient to avoid lost updates Masinter recommended as an action item the review of entity tags to see if they are adequate for versioning Nielsen stated that use of entity tags may have some interaction with caching that would be undesirable Long continued his presentation by describing locking within AOLserver and AOLpress The AOLserver implements LOCK and UNLOCK methods If a resource is currently locked a Locked By header is returned stating who currently has the lock During an unlock if you have write permission on a resource you can unlock it If you do not have write permissions a message is displayed stating that the resource is currently locked and by who An OPTIONS method is used to detect server support for locking Access Control slide users groups multiple membership METHOD URL specific inherited machine address netmask It is possible to allow deny a user group netmask for a particular URL There is a forms based user interface for access control functionality There was a discussion of the value of this approach The main benefit is that clients do not need to understand the content of the forms they just merely need to be able to put them up Versions slide Resource revisions are time stamped and saved Revisions are accessed through a prefixed URL with a time stamp in it Relative links and images are resolved as of that revision s time stamp PUT vs POST slide Long expressed his position that PUT is a much better method for writing content than POST PUT simplifies access control PUT simplifies infrastructure such as caching proxies gateways etc since it is ambiguous whether an arbitrary POST is transmitting an order for a pizza or writing a resource There are also general use servers which are starting to support the PUT method Namespace methods slide DELETE This is now supported in HTTP 1 1 MKDIR In AOLpress they wanted to support a Save As dialog box which includes a new directory option since people think of their web space as a file system BROWSE A method which looks at the contents of a directory An example of the result of a BROWSE method is the return of an application x navidir MIME type application x navidir MIME type name Dave then displayed a slide on the issue of whether we should standardize Server API s As rationale for why this might be desirable Long mentioned that if as an author you need to do something to the server for your pages it would be nice to use an existing API rather than talking to the site administrator for every change ISAPI was mentioned as a possibility Variants slide each variant has its own name there is a need to standardize the naming scheme does this apply to versioning as well Assigning variants to resources similar to access control PICS support slide data driven support for metadata systems Should raters provide well known URLs to forms driven interfaces for generators that produce rating specific encodings Nielsen mentioned that the W3C has a tool where you can plug in a PICS rating system and it pops up the correct buttons sliders etc and allows you to set the rating for a page Miscellany slide cross server searching WAIS MARC cross vendor auto linking Search the web automatically create links or give choices of what you might want to link to better support for link checking A question was raised about redirecting URLs and how to tell if a resource has been permanently moved or only temporarily moved Brown It would be useful to specify what is the guaranteed URL Masinter You can register your URLs with the OCLC Persistent URL Service at URL http www purl org Nielsen You can abstract the Web space from the file space by having the server remap the resources that it owns FrontPage Schulert Release 1 1 was shipped this Spring and release 2 will be shipped sometime this Fall Vermeer was bought in January by Microsoft FrontPage was a client server web authoring tool from the beginning Ideally FrontPage wants to be server independent they do not want to be in the server business and would prefer to use HTTP as the base upon which they build FrontPage When they first started development of FrontPage using the POST method was their only choice since it was widely available on existing servers They implemented an RPC like mechanism on top of POST There are somewhere between 2 and 3 entry points into FrontPage using this RPC like mechanism FrontPage has three server extensions executables one for each of the three levels of access control available During design FrontPage was aimed more towards the mainstream user rather than the power user The design was approached as a standard client server development where functionality was placed on whichever side of the wire it most made sense The FrontPage server extensions include much functionality such as the ability to return a link map for the site enhanced semantics for some operations and the ability to set access control from the client It does not include all features of SSA client control There are capabilities for link repair such as being able to inform FrontPage that a given URL has changed and having FrontPage fix it in all of the pages on the server There is a bulk upload multi resource at a time capability FrontPage features webbots which are objects that can be dropped into HTML and which are active when a page is uploaded to the server An example webbot is the day last changed bot which automatically inserts the date the web page was last changed into the source HTML for the page Other webbots include a substitution bot which can for example replace a company name in all web pages on a server and a table of contents bot Schulert stated that the FrontPage group would like to get out of the business of maintaining the CGI scripts and server extensions Whitehead How tightly are the webbots tied to using POST Schulert We just need a way of invoking the webbot behavior Masinter If you want to get out of maintaining those three CGIs could you make FrontPage work with PUT Schulert We can see a path out of it using server side Java or Visual Basic Schulert stated that you can post a web up to a server using FTP You can post just changes that way too This solves the problem of directly editing the server when you don t want to make direct changes and a versioning system is not available Hence we could use PUT in a very basic way like using FTP to have an interoperable write capability with other servers FrontPage currently has a bundled search engine but Schulert does not view FrontPage as being in that business either the search engine is just enough to provide basic functionality Ideally Schulert would like version control and search engines to be pluggable on the server In FrontPage 1 1 there is conflict detection and collision prevention but no resolution help There was some discussion surrounding the problems of what you author is not what you are getting there can be things that the author knows that the server doesn t get For example if I delete a slide from a presentation or a message in a discussion the adjacent next and previous links are munged and the server may know nothing about remedying that Schulert sees there being an HTTP level problem in the long term in this change of abstraction and also having conflicting server side extensions How do people s extensions cooperate with each other without knowing about each other Schulert stated that there is a core set of functionalities beyond PUT Suggestions PUT is a great first start Can post a web to a server using FTP using FrontPage 1 1 PUT isn t good enough Neither is GET PUT would be the equivalent of their FTP posting capability FrontPage 1 1 allows operation in a mode where FrontPage does not control access control Goal of Working Group Whitehead Whitehead next led a discussion about the goals and membership of the working group Whitehead displayed a slide which stated that the goal of the working group should be to make distributed authoring as pervasive as browsing is today Brown I don t know about pervasive Long I keep getting E mail from customers who are using Netscape and notice a typo or spelling mistake and want to fix it That s the world we want to address Masinter We could be modest and not change the world and simply have interoperability among our tools Fein But we want to have interoperability for things we would like to do not just what we are trying to do Masinter Yes we should look about two years ahead Nielsen We should have something at level very quickly I can t see farther than six months out Whitehead then recommended that the group adopt as its goal objective aspiration to ensure that distributed web content authoring tools are broadly interoperable Masinter We should change ensure to enable so it would read enable distributed web content authoring tools to be broadly interoperable Seiwald We ought to keep that goal in mind as a mission statement There was a suggestion to add a statement about standardizing features that exist today Nielsen asked that interoperability be limited to the HTTP framework There was a discussion about what was meant by interoperability and what level of interoperability should be strived for by the working group From this discussion it was clear that there are different kinds and scopes of interoperability and that working towards broad interoperability was a reasonable statement of the group s goal The working group adopted as its goal Enable distributed web content authoring tools to be broadly interoperable Sponsorship of Working Group by World Wide Web Consortium Whitehead next led a discussion about whether this working group should seek sponsorship by the W3C Nielsen mentioned that the W3C has a web page on the process used to create a W3C working group at URL http www w3 org pub WWW Consortium Process WorkingGroup html Nielsen mentioned that being a W3C working group implies that you have to honor the W3C agreements including the meeting procedures and it has to work on a focused output Seiwald What is the difference between being 14 random people or being affiliated with the Internet Engineering Task Force IETF Masinter If you are a working group there are legal safeguards For the IETF you have to have a chair an editor and a proposal and a draft They don t require an initial meeting and you are subject to the approval of the IETF Steering Committee for being appropriate to the IETF Nielsen Becoming a W3C working group makes sense because it is within the domain of the area of interest of W3C Seiwald Does the result go into the IETF Masinter You can if you need to make an Internet standard There must be an open public review at the IETF so that is another gate This effort shouldn t be uncoordinated from the HTTP Working Group however it is not just HTTP that is of concern here Whitehead favors W3C sponsorship since it is the natural focus on Web related issues Hopes to not have quite the formality of IETF Fein What does this obligate us to on behalf of our employers What other conditions are there Masinter If you work for a company that is a member of W3C then your company has already aligned with the W3C agreements Hamilton I just want to know what the ground rules are rather than finding out retroactively Whitehead stated that he will investigate the intellectual property rights issues of non W3C members participating in a W3C working group Meeting Goals Criteria for Completion Whitehead Whitehead next put up a slide listing desirable meeting goals and started a discussion on what would constitute criteria for completion of the working group s activity Masinter mentioned that he likes the idea of a having a demonstration of interoperability For example multiple authoring tools working against multiple servers and one document being edited by multiple authoring tools It would be desirable that there be round trip preservation of content features For this demonstration of interoperability the authoring tools should of the same level of quality that people already have Whitehead and Burns agreed that this is certainly ambitious and it doesn t require versioning or other advanced capabilities Burns Having this demo actually run seems like a proper exit condition Masinter The IETF condition for advancing to a draft standard is that there be multiple interoperable implementations Whitehead mentioned that a second demo is that N authoring tools do simultaneous editing of the same resource on one server which would test lost update capabilities collision handling etc Brown It appears there are several cases A single client used against multiple servers Multiple clients used against a single server Multiple clients used against multiple servers Whitehead We will need to refine these cases to make them more concrete Long The deliverables are specifications by which each of these demonstrations can claim conformance Whitehead Let s look at key interoperability issues before we specify deliverables Key Interoperability Issues Nielsen s List Nielsen wrote a list of what he considered to be the key interoperability issues on the whiteboard in the conference room He openly admitted that there was significant bias in his choice of issues Semantic Links Make sure that PUT DELETE work What you GET Is not what you author GET for edit versus GET for browse Ability to Edit Dynamic Contents Content Version Derived From ETAG Security digest content MD5 SHTTP Access Control Management Link Management on Server When to Publish Publish header Do we need transactions Advanced Editing Multiple users roles views Adding revision and change information into HTML The HTML item was added during group discussion These interoperability issues then became the focus of discussion Schulert proposed that the GET PUT working bullet is the following example A user is browsing at a web page and sees a misspelling and wants with simple steps to edit the page and put the corrected page back out on the web Nielsen offered a scenario where there are a set of interrelated resources and putting them back to a server atomically is desired Masinter There are tasks below those let someone author a new page let someone do site maintenance Masinter then recommended that we go around the room and see what work people are willing to do as part of the working group Whitehead stated that he will write meeting notes for this meeting but is not taking ownership of individual technical issues due to the number of issues the amount of work required to coordinate the group and his desire to concentrate on versioning issues Masinter is willing to help coordinate the activity of the working group and contribute as an active member of the mailing list He does not want to edit anything new Seiwald I have more interest in the HTTP part than the HTML stuff and would be interested in writing up proposed HTTP changes Willing to author proposals as needed Fein There is more interest in HTML issues than HTTP issues in my case I m not as interested in protocols or back ends as I am in document content itself I could take the Word requirements for HTML and write them down Masinter requested that Ron sort them out as content versus protocol etc A good distinction between what is expected behavior for understanding for not understanding and for sort of understanding a tag would be helpful Nielsen That goes for all existing distributed authoring tools Dawson and Nielsen Fein Schulert and Long all have lists of what they want to do in authoring tools Word FrontPage and AOL They could take on a task to sort out requirements Hamilton I propose that Fein Schulert and Long Word FrontPage AOL use Fein s model employed on his slides for listing functional requirements and features and possible solutions Long and Burns are going to work on assembling something that is wordsmithed on functional requirements and scenarios The tasks scenarios that provide the cover for the features list Nielsen mentioned that he has three scenarios at three different levels in his slides Dawson volunteered to edit the scenarios document Seiwald invited everyone to submit two scenarios not including the three we have Masinter The task scenario document could be long There is nothing wrong with that We should also invite people to contribute There may be substantial contributions from others who are not here Nielsen I think coming up with a requirements document is nice However there are some things that need to be fixed and we could also start working now on the technical issues Masinter We don t have to wait for the scenario document to be complete to start work on the technical issues However we will need the scenarios document to tell other people why we are doing this and how we will know when we are done Looked at Get for Edit versus Get for Browse It s important to lay out what the alternatives are so you can say you discarded them Nielsen gave his suggestions for scenarios which should belong in the scenarios document One person changing a misspelling in a document they found while browsing Checking in a group of resources which are related Deleting an object from a web Two people editing changes to the same resource Specified Deliverables The group came to agreement that the following set of activities and deliverables should be produced by the working group Task oriented list of scenarios which interoperable distributed authoring tools will be able to perform Keith Dawson editor Input of other distributed authoring vendors notably Netscape will be solicited Put a call on the discussion list request input for this activity Nielsen will give us his list of requirements Collate lists of key functionality among AOLpress AOLserver FrontPage Word as well as other distributed authoring tools such as Netscape Dave Long editor Looking at early September for next meeting for at least the groups working on the deliverables above Meeting Adjourned University of California Irvine Jim Whitehead ejw ics uci edu Department of Information and Computer Science 247 ICS2 3425 Irvine CA 92697 3425 Last modified 23 Jul 1996", "_id": "http://www.ics.uci.edu/~ejw/authoring/sanmateo/Minutes_afternoon.html", "title": "detailed minutes of san mateo distributed authoring meeting (afternoon)\n", "html": "<HTML>\n<HEAD>\n<TITLE>Detailed Minutes of San Mateo Distributed Authoring Meeting (Afternoon)\n</TITLE>\n<META NAME=\"GENERATOR\" CONTENT=\"Internet Assistant for Microsoft Word 2.0 beta 1\">\n<LINK REL=\"Author\" TITLE=\"Jim Whitehead Home Page\" HREF=\"http://www.ics.uci.edu/~ejw/\">\n<META http-equiv=\"Reply-To\" content=\"ejw@ics.uci.edu\">\n</HEAD>\n\n<BODY>\n<H2>Afternoon (1:35 PM)</H2>\n\n<H3>AOLpress and AOLserver - Long</H3>\n\n<P>\n\nLong started by informing everyone that GNNpress and GNNserver\n\nare now known as AOLpress and AOLserver.\n\n<P>\n\nIn AOLserver, the current version of resource is stored in the\n\nfile system, while the meta-data, indices, etc. are stored in\n\na database.\n\n<P>\n\nLong noted that people think of their web sites in terms of a\n\nhierarchy, and hence it is disorienting for the server to place\n\na resource at a location other than the one specified by the user.\n\n<P>\n\nWith regard to server-side includes and other dynamic content,\n\nAOL has no strong solution for getting to the actual source. Their\n\ncurrent approach disallows editing a page with dynamic content.\n\n However, AOLpress users seem to be doing it anyway, although\n\nit is not clear what mechanism they use. His recommendation for\n\nhow to get a document's source before server side includes processing\n\nis to use content negotiation when requesting the source, and\n\nasking for MIME type text/x-html-ssi.  Unfortunately, this approach\n\ndoes not handle the case where the source is not HTML.\n\n<P>\n\nA discussion then ensued about pretty printing (canonicalizing)\n\nthe HTML written by an authoring tool.  There were some questions\n\nabout how much canonicalizing an authoring tool should attempt.\n\nLong mentioned that AOL used to get grief from customers because\n\ntheir tool automatically cleans up its HTML output, but they no\n\nlonger do, since the tool now additionally pretty-prints the HTML\n\nsource.  It appears that users perceive enough extra value in\n\nthe pretty-printing of the generated HTML that they are willing\n\nto accept the cleanup.\n\n<P>\n\nMasinter: There are people who are claiming to do HTML authoring\n\nwho want their mail client and mail servers to send around marked-up\n\nmail. They are using a kind of HTML that they make human readable\n\nthrough operations such as centering HTML source text between\n\n&lt;CENTER&gt; tags. \n\n<P>\n\nWhitehead: Is there an interoperability concern with regard to\n\npretty printing? \n\n<P>\n\nLong: Probably not. I would be happy if we just made it so that\n\neverybody's client could do a PUT to everybody's server. That\n\nwould be the most basic form of interoperability -- everything\n\nelse is icing on the cake. \n\n<P>\n\nNielsen: The W3C could put up a playground where everyone could\n\ntest out the PUT interoperability of their authoring tools, so\n\nlong as we don't get into the test suite business. \n\n<P>\n\nLong mentioned that the focus of his talk is not interoperability.\n\nHe followed Dan Connolly's suggestion to go through your system\n\nand find out where you feel fuzzy and the least likely to be doing\n\nsomething in the same way as everyone else.<BR>\n\n\n\n<P>\n\nContinuing his presentation, Long described how moving pages brings\n\nup the issue of how to move relative links. Moving image files\n\nis easy, because the IMG tag indicates the image is considered\n\npart of the document, and should be moved. However, moving audio\n\nfiles is not as easy because they are included via a A HREF tag,\n\nwhich could refer to a close or distant resource.  The best solution\n\nto this problem is to employ collections and move mini-webs. Unfortunately,\n\nmany beginning users do not use the AOLpress mini-web functionality.\n\nAnother solutions is to have a usage practice where relative URLs\n\nare employed for references internal to a collection and external\n\nURLs are used for references outside of it.<BR>\n\n\n\n<P>\n\nThis led into a discussion on the semantics of\n\n<P>\n\n&quot;/x/y&quot; style relative URLs\n\n<P>\n\nI.e., they're not &quot;../foo&quot; or &quot;http://..&quot;\n\n<BR>\n\n\n\n<P>\n\nCurrent versioning implementation slide:\n\n<P>\n\nLong reported on a prototype effort to add versioning capability\n\nto the AOLserver.  In this prototype, reads employ a &quot;Content-Version&quot;\n\nheader to retrieve a stated version of a resource. Writes provide\n\na &quot;Derived-From&quot; header so the server knows how to check-in\n\nthe version of the resource. The server prevents dirty writes.\n\n This implementation solves the &quot;CREATE&quot; problem (i.e.,\n\nit does not need a separate CREATE method for the initial creation\n\nof a resource, because specifying a null Derived-From header directs\n\nthe server to perform a resource creation).  Since this prototype\n\nwas completed, the header syntax has changed for HTTP 1.1. An\n\nincomplete report on this is available at URL:\n\n<P>\n\n<A HREF=\"http://staff.navisoft.com/users/d/dave/w3cwd/wd-version-960128.htm\" ><B>http://staff.navisoft.com/users/d/dave/w3cwd/wd-version-960128.htm</B></A>\n\n<P>\n\nAs an aside, Long mentioned that users really wanted to know if\n\nthey were going to overwrite a document.  This was accommodated\n\nby providing a dialog box with &quot;OK&quot; and &quot;Cancel&quot;\n\noptions whenever a document was about to be overwritten.  This\n\nrequired performing a HEAD operation on the resource before a\n\nPUT could take place.\n\n<P>\n\nAt this point there was a discussion of entity tags (Etags, an\n\nopaque string which identifies a snapshot), and their potential\n\nuse in versioning.  Long believes entity tags are sufficient to\n\navoid lost updates. Masinter recommended as an action item the\n\nreview of entity tags to see if they are adequate for versioning.\n\n Nielsen stated that use of entity tags may have some interaction\n\nwith caching that would be undesirable.\n\n<P>\n\nLong continued his presentation by describing locking within AOLserver\n\nand AOLpress. The AOLserver implements LOCK and UNLOCK methods.\n\n If a resource is currently locked, a &quot;Locked-By&quot; header\n\nis returned stating who currently has the lock.  During an unlock,\n\nif you have write permission on a resource, you can unlock it.\n\n If you do not have write permissions, a message is displayed\n\nstating that the resource is currently locked, and by who. An\n\nOPTIONS method is used to detect server support for locking.<BR>\n\n\n\n<P>\n\nAccess Control slide:\n\n<UL>\n\n<LI>users/groups (multiple membership)\n\n<LI>METHOD URL (specific/inherited)\n\n<LI>machine address (netmask)\n\n</UL>\n\n\n\n<P>\n\n\n\n<P>\n\nIt is possible to (allow, deny) a (user, group, netmask) for a\n\nparticular URL.\n\n<P>\n\nThere is a forms based user interface for access control functionality.\n\nThere was a discussion of the value of this approach.  The main\n\nbenefit is that clients do not need to understand the content\n\nof the forms, they just merely need to be able to put them up.\n\n<BR>\n\n\n\n<P>\n\nVersions slide:\n\n<P>\n\nResource revisions are time stamped and saved.  Revisions are\n\naccessed through a prefixed URL, with a time stamp in it.  Relative\n\nlinks and images are resolved as of that revision's time stamp.\n\n<BR>\n\n\n\n<P>\n\nPUT vs. POST slide:\n\n<P>\n\nLong expressed his position that PUT is a much better method for\n\nwriting content than POST. PUT simplifies access control.  PUT\n\nsimplifies infrastructure such as caching proxies, gateways, etc.,\n\nsince it is ambiguous whether an arbitrary POST is transmitting\n\nan order for a pizza or writing a resource.  There are also general-use\n\nservers which are starting to support the PUT method.<BR>\n\n\n\n<P>\n\nNamespace methods slide:\n\n<UL>\n\n<LI>DELETE (This is now supported in HTTP 1.1.)\n\n<LI>MKDIR (In AOLpress they wanted to support a &quot;Save As...&quot;\n\ndialog box, which includes a new directory option, since people\n\nthink of their web space as a file system.)\n\n<LI>BROWSE (A method which looks at the contents of a directory.)\n\n</UL>\n\n\n\n<P>\n\n\n\n<P>\n\nAn example of the result of a BROWSE method is the return of an\n\napplication/x-navidir MIME type.<BR>\n\n\n\n<P>\n\napplication/x-navidir\n\n<P>\n\n{MIME type} {name}\n\n<P>\n\n...\n\n<P>\n\nDave then displayed a slide on the issue of whether we should\n\nstandardize Server API's.  As rationale for why this might be\n\ndesirable, Long mentioned that if, as an author, you need to do\n\nsomething to the server for your pages, it would be nice to use\n\nan existing API rather than talking to the site administrator\n\nfor every change. ISAPI was mentioned as a possibility.<BR>\n\n\n\n<P>\n\nVariants slide:\n\n<UL>\n\n<LI>each variant has its own name (there is a need to standardize\n\nthe naming scheme)\n\n<LI>does this apply to versioning as well?\n\n<LI>Assigning variants to resources similar to access control\n\n</UL>\n\n\n\n<P>\n\n\n\n<P>\n\nPICS support slide:\n\n<UL>\n\n<LI>data-driven support for metadata systems\n\n</UL>\n\n\n\n<P>\n\nShould raters provide well-known URLs to forms-driven interfaces\n\nfor generators that produce rating-specific encodings?<BR>\n\n\n\n<P>\n\nNielsen mentioned that the W3C has a tool where you can plug-in\n\na PICS rating system, and it pops up the correct buttons, sliders,\n\netc., and allows you to set the rating for a page.<BR>\n\n\n\n<P>\n\nMiscellany slide:\n\n<UL>\n\n<LI>cross-server searching (WAIS, MARC, ...)\n\n<LI>cross-vendor auto-linking? (Search the web, automatically\n\ncreate links, or give choices of what you might want to link to).\n\n<LI>better support for link checking.\n\n</UL>\n\n\n\n<P>\n\n\n\n<P>\n\nA question was raised about redirecting URLs, and how to tell\n\nif a resource has been permanently moved or only temporarily moved.\n\n<P>\n\nBrown: It would be useful to specify what is the guaranteed URL.\n\n<P>\n\nMasinter: You can register your URLs with the OCLC Persistent\n\nURL Service at URL:\n\n<P>\n\n<A HREF=\"http://www.purl.org/\" ><B>http://www.purl.org/</B></A>\n\n<P>\n\nNielsen: You can abstract the Web space from the file space by\n\nhaving the server remap the resources that it owns. \n\n<H3>FrontPage - Schulert<BR>\n\n</H3>\n\n\n\n<P>\n\nRelease 1.1 was shipped this Spring, and release 2.0 will be shipped\n\nsometime this Fall.  Vermeer was bought in January by Microsoft.\n\n<P>\n\nFrontPage was a client-server web authoring tool from the beginning.\n\n Ideally, FrontPage wants to be server independent -- they do\n\nnot want to be in the server business, and would prefer to use\n\nHTTP as the base upon which they build FrontPage.\n\n<P>\n\nWhen they first started development of FrontPage, using the POST\n\nmethod was their only choice, since it was widely available on\n\nexisting servers.  They implemented an RPC-like mechanism on top\n\nof POST.  There are somewhere between 20 and 30 entry points into\n\nFrontPage using this RPC-like mechanism. FrontPage has three server\n\nextensions executables, one for each of the three levels of access\n\ncontrol available.\n\n<P>\n\nDuring design, FrontPage was aimed more towards the mainstream\n\nuser, rather than the power user.  The design was approached as\n\na standard client-server development, where functionality was\n\nplaced on whichever side of the wire it most made sense.\n\n<P>\n\nThe FrontPage server extensions include much functionality, such\n\nas the ability to return a link map for the site, enhanced semantics\n\nfor some operations, and the ability to set access control from\n\nthe client.  It does not include all features of SSA client control.\n\nThere are capabilities for link repair, such as being able to\n\ninform FrontPage that a given URL has changed, and having FrontPage\n\nfix it in all of the pages on the server.  There is a bulk upload\n\n(multi-resource at a time) capability.\n\n<P>\n\nFrontPage features webbots, which are objects that can be dropped\n\ninto HTML and which are active when a page is uploaded to the\n\nserver.  An example webbot is the &quot;day last changed&quot;\n\nbot, which automatically inserts the date the web page was last\n\nchanged into the source HTML for the page.  Other webbots include\n\na &quot;substitution&quot; bot, which can, for example, replace\n\na company name in all web pages on a server, and a &quot;table\n\nof contents&quot; bot. \n\n<P>\n\nSchulert stated that the FrontPage group would like to get out\n\nof the business of maintaining the CGI scripts and server extensions.\n\n<P>\n\nWhitehead: How tightly are the webbots tied to using POST? \n\n<P>\n\nSchulert: We just need a way of invoking the webbot behavior.\n\n\n\n<P>\n\nMasinter: If you want to get out of maintaining those three CGIs,\n\ncould you make FrontPage work with PUT? \n\n<P>\n\nSchulert: We can see a path out of it, using server-side Java\n\nor Visual Basic. \n\n<P>\n\nSchulert stated that you can post a web up to a server using FTP.\n\nYou can post just changes that way, too. This solves the problem\n\nof directly editing the server when you don't want to make direct\n\nchanges and a versioning system is not available.  Hence we could\n\nuse PUT in a very basic way, like using FTP, to have an interoperable\n\nwrite capability with other servers.\n\n<P>\n\nFrontPage currently has a bundled search engine, but Schulert\n\ndoes not view FrontPage as being in that business either -- the\n\nsearch engine is just enough to provide basic functionality. \n\nIdeally, Schulert would like version control, and search engines\n\nto be pluggable on the server.\n\n<P>\n\nIn FrontPage 1.1, there is conflict detection and collision prevention,\n\nbut no resolution help. \n\n<P>\n\nThere was some discussion surrounding the problems of what you\n\nauthor is not what you are getting -- there can be things that\n\nthe author knows that the server doesn't get. For example, if\n\nI delete a slide from a presentation, or a message in a discussion,\n\nthe adjacent next and previous links are munged, and the server\n\nmay know nothing about remedying that. Schulert sees there being\n\nan HTTP-level problem, in the long term, in this change of abstraction\n\nand also having conflicting server-side extensions. How do people's\n\nextensions cooperate with each other without knowing about each\n\nother? \n\n<P>\n\nSchulert stated that there is a core set of functionalities beyond\n\nPUT. \n\n<P>\n\nSuggestions:\n\n<UL>\n\n<LI>PUT is a great first start\n\n<UL>\n\n<LI>Can post a web to a server using FTP using FrontPage 1.1\n\n</UL>\n\n\n\n<LI>PUT isn't good enough.  Neither is GET.\n\n<LI>PUT would be the equivalent of their FTP posting capability.\n\n<UL>\n\n<LI>FrontPage 1.1 allows operation in a mode where FrontPage does\n\nnot control access control.\n\n</UL>\n\n\n\n</UL>\n\n\n\n<P>\n\n\n\n<H3>Goal of Working Group - Whitehead</H3>\n\n\n\n<P>\n\nWhitehead next led a discussion about the goals and membership\n\nof the working group.\n\n<P>\n\nWhitehead displayed a slide which stated that the goal of the\n\nworking group should be to make distributed authoring as pervasive\n\nas browsing is today. \n\n<P>\n\nBrown: I don't know about pervasive. \n\n<P>\n\nLong:  I keep getting E-mail from customers who are using Netscape\n\nand notice a typo or spelling mistake and want to fix it. That's\n\nthe world we want to address. \n\n<P>\n\nMasinter: We could be modest and not change the world, and simply\n\nhave interoperability among our tools. \n\n<P>\n\nFein: But we want to have interoperability for things we would\n\nlike to do, not just what we are trying to do. \n\n<P>\n\nMasinter:  Yes, we should look about two years ahead. \n\n<P>\n\nNielsen: We should have something at level 0 very quickly -- I\n\ncan't see farther than six months out. \n\n<P>\n\nWhitehead then recommended that the group adopt as its goal/objective/aspiration:\n\nto ensure that distributed web content authoring tools are broadly\n\ninteroperable. \n\n<P>\n\nMasinter: We should change ensure to enable, so it would read:\n\nenable distributed web content authoring tools to be<I> broadly\n\ninteroperable</I> .\n\n<P>\n\nSeiwald: We ought to keep that goal in mind, as a mission statement.\n\n<BR>\n\n\n\n<P>\n\nThere was a suggestion to add a statement about standardizing\n\nfeatures that exist today. \n\n<P>\n\nNielsen asked that interoperability be limited to the HTTP framework.\n\n\n\n<P>\n\nThere was a discussion about what was meant by interoperability,\n\nand what level of interoperability should be strived for by the\n\nworking group.  From this discussion, it was clear that there\n\nare different kinds and scopes of interoperability, and that working\n\ntowards broad interoperability was a reasonable statement of the\n\ngroup's goal.\n\n<P>\n\nThe working group adopted as its goal: \n\n<P>\n\n<I>Enable distributed web content authoring tools to be broadly\n\ninteroperable</I><B>.</B>\n\n<H3>Sponsorship of Working Group by World Wide Web Consortium\n\n</H3>\n\n\n\n<P>\n\nWhitehead next led a discussion about whether this working group\n\nshould seek sponsorship by the W3C. Nielsen mentioned that the\n\nW3C has a web page on the process used to create a W3C working\n\ngroup, at URL:\n\n<P>\n\n<A HREF=\"http://www.w3.org/pub/WWW/Consortium/Process/WorkingGroup.html\" ><B>http://www.w3.org/pub/WWW/Consortium/Process/WorkingGroup.html</B></A>\n\n<P>\n\nNielsen mentioned that being a W3C working group implies that\n\nyou have to honor the W3C agreements, including the meeting procedures,\n\nand it has to work on a focused output. \n\n<P>\n\nSeiwald: What is the difference between being 14 random people\n\nor being affiliated with the Internet Engineering Task Force (IETF)?\n\n<P>\n\nMasinter: If you are a working group, there are legal safeguards.\n\nFor the IETF you have to have a chair, an editor, and a proposal\n\nand a draft. They don't require an initial meeting, and you are\n\nsubject to the approval of the IETF Steering Committee for being\n\nappropriate to the IETF. \n\n<P>\n\nNielsen: Becoming a W3C working group makes sense because it is\n\nwithin the domain of the area of interest of W3C. \n\n<P>\n\nSeiwald: Does the result go into the IETF? \n\n<P>\n\nMasinter: You can, if you need to make an Internet standard. There\n\nmust be an open public review at the IETF, so that is another\n\ngate. This effort shouldn't be uncoordinated from the HTTP Working\n\nGroup, however, it is not just HTTP that is of concern here.\n\n<P>\n\nWhitehead favors W3C sponsorship since it is the natural focus\n\non Web-related issues. Hopes to not have quite the formality of\n\nIETF. \n\n<P>\n\nFein: What does this obligate us to on behalf of our employers?\n\nWhat other conditions are there? \n\n<P>\n\nMasinter: If you work for a company that is a member of W3C, then\n\nyour company has already aligned with the W3C agreements. \n\n<P>\n\nHamilton: I just want to know what the ground rules are, rather\n\nthan finding out retroactively. \n\n<P>\n\nWhitehead stated that he will investigate the intellectual property\n\nrights issues of non-W3C members participating in a W3C working\n\ngroup.\n\n<H3>Meeting Goals, Criteria for Completion - Whitehead </H3>\n\n\n\n<P>\n\nWhitehead next put up a slide listing desirable meeting goals,\n\nand started a discussion on what would constitute criteria for\n\ncompletion of the working group's activity.\n\n<P>\n\nMasinter mentioned that he likes the idea of a having a demonstration\n\nof interoperability. For example, multiple authoring tools working\n\nagainst multiple servers and one document being edited by multiple\n\nauthoring tools. It would be desirable that there be round-trip\n\npreservation of content features.  For this demonstration of interoperability,\n\nthe authoring tools should of the same level of quality that people\n\nalready have. \n\n<P>\n\nWhitehead and Burns agreed that this is certainly ambitious, and\n\nit doesn't require versioning, or other advanced capabilities.\n\n<P>\n\nBurns: Having this demo actually run seems like a proper exit\n\ncondition \n\n<P>\n\nMasinter: The IETF condition for advancing to a draft standard\n\nis that there be multiple interoperable implementations \n\n<P>\n\nWhitehead mentioned that a second demo is that N authoring tools\n\ndo simultaneous editing of the same resource on one server, which\n\nwould test lost update capabilities, collision handling, etc.\n\n<P>\n\nBrown: It appears there are several cases: \n\n<OL>\n\n<LI>A single client used against multiple servers.\n\n<LI>Multiple clients used against a single server.\n\n<LI>Multiple clients used against multiple servers.\n\n</OL>\n\n\n\n<P>\n\nWhitehead: We will need to refine these cases to make them more\n\nconcrete.\n\n<P>\n\nLong: The deliverables are specifications by which each of these\n\ndemonstrations can claim conformance.\n\n<P>\n\nWhitehead: Let's look at key interoperability issues before we\n\nspecify deliverables.\n\n<H3>Key Interoperability Issues (Nielsen's List) </H3>\n\n\n\n<P>\n\nNielsen wrote a list of what he considered to be the key interoperability\n\nissues on the whiteboard in the conference room.  He openly admitted\n\nthat there was significant bias in his choice of issues.\n\n<UL>\n\n<LI>Semantic Links \n\n<LI>Make sure that PUT, DELETE work \n\n<LI>What you &quot;GET&quot; Is not what you author - GET for\n\nedit versus GET for browse? \n\n<LI>Ability to Edit Dynamic Contents \n\n<LI>Content-Version, Derived-From, ETAG \n\n<LI>Security (digest, content-MD5, SHTTP) \n\n<LI>Access Control Management \n\n<LI>Link Management on Server \n\n<LI>When to Publish (Publish header) \n\n<LI>Do we need transactions \n\n<LI>Advanced Editing (Multiple users, roles, views) \n\n<LI>Adding revision and change information into HTML \n\n</UL>\n\n\n\n<P>\n\nThe HTML item was added during group discussion.  These interoperability\n\nissues then became the focus of discussion.\n\n<P>\n\nSchulert proposed that the GET-PUT working bullet is the following\n\nexample: A user is browsing at a web page and sees a misspelling\n\nand wants, with simple steps, to edit the page and put the corrected\n\npage back out on the web. \n\n<P>\n\nNielsen offered a scenario where there are a set of interrelated\n\nresources, and putting them back to a server atomically is desired.\n\n\n\n<P>\n\nMasinter: There are tasks below those - let someone author a new\n\npage, let someone do site maintenance. \n\n<P>\n\nMasinter then recommended that we go around the room and see what\n\nwork people are willing to do as part of the working group.\n\n<P>\n\nWhitehead stated that he will write meeting notes for this meeting,\n\nbut is not taking ownership of individual technical issues due\n\nto the number of issues, the amount of work required to coordinate\n\nthe group, and his desire to concentrate on versioning issues.\n\n<P>\n\nMasinter is willing to help coordinate the activity of the working\n\ngroup, and contribute as an active member of the mailing list.\n\nHe does not want to edit anything new. \n\n<P>\n\nSeiwald: I have more interest in the HTTP part than the HTML stuff\n\nand would be interested in writing up proposed HTTP changes. Willing\n\nto author proposals as needed. \n\n<P>\n\nFein: There is more interest in HTML issues than HTTP issues in\n\nmy case. I'm not as interested in protocols or back ends as I\n\nam in document content itself. I could take the Word requirements\n\nfor HTML and write them down. \n\n<P>\n\nMasinter requested that Ron sort them out as content versus protocol\n\netc. A good distinction between what is expected behavior for\n\nunderstanding, for not understanding, and for sort-of understanding\n\na tag would be helpful.\n\n<P>\n\nNielsen: That goes for all existing distributed authoring tools.\n\n\n\n<P>\n\nDawson and Nielsen: Fein, Schulert, and Long all have lists of\n\nwhat they want to do in authoring tools - Word, FrontPage, and\n\nAOL. They could take on a task to sort out requirements. \n\n<P>\n\nHamilton: I propose that Fein, Schulert, and Long (Word, FrontPage,\n\nAOL) use Fein's model (employed on his slides) for listing functional\n\nrequirements and features and possible solutions. \n\n<P>\n\nLong and Burns are going to work on assembling something that\n\nis wordsmithed on functional requirements and scenarios. The tasks\n\n/ scenarios that provide the cover for the features list. \n\n<P>\n\nNielsen mentioned that he has three scenarios at three different\n\nlevels in his slides. \n\n<P>\n\nDawson volunteered to edit the scenarios document. \n\n<P>\n\nSeiwald invited everyone to submit two scenarios, not including\n\nthe three we have. \n\n<P>\n\nMasinter: The task scenario document could be long. There is nothing\n\nwrong with that. We should also invite people to contribute. There\n\nmay be substantial contributions from others who are not here.\n\n\n\n<P>\n\nNielsen: I think coming up with a requirements document is nice.\n\nHowever, there are some things that need to be fixed and we could\n\nalso start working now on the technical issues.\n\n<P>\n\nMasinter: We don't have to wait for the scenario document to be\n\ncomplete to start work on the technical issues.  However, we will\n\nneed the scenarios document to tell other people why we are doing\n\nthis and how we will know when we are done.\n\n<P>\n\nLooked at Get for Edit versus Get for Browse. It's important to\n\nlay out what the alternatives are so you can say you discarded\n\nthem. \n\n<P>\n\nNielsen gave his suggestions for scenarios which should belong\n\nin the scenarios document:\n\n<OL>\n\n<LI>One person changing a misspelling in a document they found\n\nwhile browsing.\n\n<LI>Checking-in a group of resources which are related.\n\n<LI>Deleting an object from a web.\n\n<LI>Two people editing changes to the same resource.\n\n</OL>\n\n\n\n<H3>Specified Deliverables</H3>\n\n<P>\nThe group came to agreement that the following set of activities\nand deliverables should be produced by the working group:\n</P>\n\n<OL>\n\n<LI>Task-oriented list of scenarios which interoperable distributed\nauthoring tools will be able to perform.\n\n<UL>\n\n<LI>Keith Dawson, editor.\n\n<LI>Input of other distributed authoring vendors, notably Netscape,\n\nwill be solicited.\n\n<LI>Put a call on the discussion list request input for this activity.\n\n<LI>Nielsen will give us his list of requirements.\n\n</UL>\n\n<LI>Collate lists of &quot;key functionality&quot; among AOLpress/AOLserver,\nFrontPage, Word, as well as other distributed authoring tools,\nsuch as Netscape\n\n<UL>\n\n<LI>Dave Long, editor \n\n</UL>\n\n</OL>\n\n\n<P>\nLooking at early September for next meeting, for at least the\ngroups working on the deliverables above. </P>\n\n<P>\n*** Meeting Adjourned ***\n</P>\n\n<HR>\n<ADDRESS>\n<A HREF=\"http://www.uci.edu/\">\nUniversity of California, Irvine</A><BR>\n<A HREF=\"http://www.ics.uci.edu/~ejw/\">\nJim Whitehead &lt;ejw@ics.uci.edu&gt;</A><BR>\n<A HREF=\"http://www.ics.uci.edu/\">\nDepartment of Information and Computer Science</A><BR>\n247 ICS2 #3425<BR>\n<A HREF=\"http://www.irvineco.com/\">\nIrvine</A>, CA  92697-3425<BR>\n</ADDRESS>\n<P>\nLast modified: 23 Jul 1996\n</P>\n</BODY>\n</HTML>\n\n", "id": 32434.0}