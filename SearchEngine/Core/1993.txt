{"text": "ICS 28 Spring 1999 Computational Statistics Regression The data model for linear regression is that the data points are constrained to lie on some hyperplane i e there is a linear relation between their coordinates Another way of interpreting this is to treat one of the coordinates as dependent and the rest as independent the dependent coordinate is then a linear function of the independent ones The data may be chosen according to some probability distribution on the line or there may be some known pattern of independent coordinates e g they are chosen on the grid at each point of which one measures the dependent coordinate For the probability distribution models it doesn t much matter which is the dependent and which the independent variables the only restriction is that the hyperplane must not be parallel to the dependent coordinate axis But this choice may make a difference if the independent coordinates are nonrandom or if the noise model adds noise only to the dependent coordinate Data models in which the data points must satisfy multiple linear relations i e they lie on a lower dimensional subspace can often be handled by treating each dependent coordinate separately but this is only appropriate for noise models in which the coordinates are independent e g Gaussian noise Projective Duality There s another picture of regression that is mathematically equivalent but may help human intuition Interpret the coordinates a b defining any line y ax b to instead define a point in a dual a b plane Then the set of lines through any point x y in the primal plane turns into a set of points b x a y in the dual plane this set is just a line So if we have a collection of points xi yi defining a regression problem we can think of them instead as each defining a different line b xi a yi in the a b plane What we want to find is a point in the a b plane that is near all of these lines that is we expect the lines to roughly look like they all go through a common point and we want to find that common point Similar methods transform any higher dimensional regression problem of fitting a single hyperplane to a set of data points into an equivalent problem of fitting a single point to an arrangement of hyperplanes Lp Models for Dependent Noise Let s first consider the case in which the noise is a random variable added only to the independent variable The most common algorithm for fitting this model is least squares If we are trying to find a linear model y m x b where y is the dependent variable x is the vector of independent variables and m and b are the parameters we re trying to find then we measure the error of fit by E sum m xi b yi 2 The least squares fit is the one minimizing this error estimate The big advantage of least squares is that it s easy to solve E is just a positive definite quadratic form so it has a unique minimum Since E is a smooth function of m and b its minimum occurs at the point where its partial derivatives in each coordinate are all zero This gives a system of d equations in d unknowns which one can solve by Gaussian elimination or one can instead use local improvement methods to approximate the optimal fit Thus the time is linear in the number of unknowns and at worst cubic in the dimension More generally the Lp error is formed by using pth powers in place of squares in the definition of E The limit of this the Linfinity norm is just the maximum error of any individual data point The most commonly used of these are the L1 norm also known as least absolute deviation or LAD regression which is less sensitive to outliers than least squares and the Linfinity norm which is more sensitive to outliers and therefore useful in contexts where one wants to detect them e g measuring flatness of a surface The Linfinity estimator can be expressed as a low dimensional linear program find m b e minimizing e and satisfying the linear constraints e m xi b yi e The dimension number of variables to be found is just d 1 Therefore it can be solved in time linear in the number of data points but exponential in the dimension using low dimensional linear programming techniques citations to be filled in later There also exists a similar linear time bound for L1 estimation Z84 All Lp estimators are invariant to coordinate changes and affine transformations among only dependent variables One advantage of assuming dependent noise is that it is preserved by the duality transformation described above the vertical distance of a point xi yi from the line y ax b is the same as the vertical distance of the point a b from the line b xi a yi Just work out the equations they re the same In the discussion one of the students suggested using a convex combination of different Lp metrics which might e g come from a truncated power series for some harder to estimate metric Alternatively the coefficients in the convex combination might themselves come from some sort of statistical estimation to fit the noise actually apparent in the data However I don t know of any references related to these ideas Lp Models for Independent Noise If the noise is added to all coordinates not just the dependent ones it may be appropriate to use the Euclidean distance from the regression hyperplane in place of the vertical distance m x b y used above Alternatively the same considerations as in single point estimation may make a different distance function appropriate L2 estimation with Euclidean distance is known as total least squares GL8 M59 P 1 The optimal estimator must go through the centroid of the data this is not too hard to see if one applies duality but the algebra describing the estimator quality as a function of its parameters is messy and hard to invert However the problem can apparently be solved by singular value decomposition There s a nice connection between L1 estimation in the independent noise model and the famous k sets or k level problem from computational geometry if you fix a choice of slope the problem is just a one dimensional L1 problem which can be solved by the median If we use projective duality what we want to know is the median among the points where the dual hyperplanes cross each possible vertical line this sequence of medians forms a surface in the arrangement and is known as the k level here k n 2 In two dimensions this surface is just a polygonal path formed by walking left to right through the arrangement and turning up or down at every crossing point you pass through A famous problem posed by Erd s and Lov sz is on the number of edges this path can have known bounds are Omega n log k and O nk1 3 Linfinity estimation is equivalent to computing the width of a set minimum distance between parallel supporting hyperplanes it can be computed in O n log n time in the plane but the best time known in three dimensions is O n3 2 citations to be filled in later These independent noise estimators are generally invariant under arbitrary translations or rotations of the coordinate system however they are not invariant to general affine transformations Dual Independent Noise If one has in mind some physical meaning to the dual interpretation in which the data define lines and the estimator one is trying to find is a point near all these lines then it may make sense to use Euclidean distance in the dual plane instead of the primal I e we define the distance from the estimator point a b to a data line b xi a yi to be the usual Euclidean distance of that point from that line With that definition one can make any sort of Lp regression etc The L1 L2 and Linfinity regressions can all be solved easily using simple modifications of the algorithms for dependent noise models This is one big advantage of dual independence over primal independence I don t know of any references or work by actual statisticians on this sort of model Least Median of Squares The LMS estimator defined by Rousseeuw R84 minimizes the median error rather than the sum of errors or squared errors as in L1 or L2 estimation Equivalently we seek a planar strip of minimum possible vertical width that covers at least half the data points It is commonly claimed that this estimator provides the maximum possible robustness to outliers up to half the data may be arbitrarily corrupted This claim seems to rely on an assumption that the data is well spread out along the independent coordinates rather than being tightly clustered along lower dimensional subspaces in a way that would allow an adversary to set up false estimates using large numbers of valid data points But the same assumption must be made for any other robust regression method Unfortunately algorithms for computing the LMS estimator are relatively slow O n2 even in two dimensions SS87 ES9 There are theoretical reasons for believing that no exact algorithm can be faster than this However Mount et al MNRSW97 provide an O n log n time approximation as well as a randomized algorithm that they believe should work well in practice although it lacks theoretical performance guarantees In the dual framework we are given an arrangement of lines and we wish to find the shortest possible vertical line segment that crosses at least half of the lines This estimator assumes a dependent noise model one can define LMS like estimator in similar ways for independent noise in which case one is trying to find a line minimizing the median distance to the points or for dual independent noise in which case one is trying to find a minimum radius circle that crosses at least half of the lines Repeated Median I don t know what this is but it s the subject of MMN98 Here s my guess based on the name I haven t looked the paper itself up yet LMS estimation can be interpreted as finding a set of outliers to throw away and ignore namely the points further away than the median distance it finds But once it throws away those points it then does something fairly stupid with the rest of the points just the Linfinity estimator which is very sensitive to outliers So if the data that s left after throwing away half the points is still suspect it seems more reasonable to instead apply the same outlier detection strategy recursively There s not much to say about algorithms for this at least not without more progress on algorithms for the LMS estimator The reason is that if you have a quadratic algorithm for LMS and apply it recursively as above you can describe the recursion s time by the recurrence T n T n 2 O n2 which is again quadratic So asymptotically the time for this recursive approach would be the same as the time for simple LMS estimation Median Slope The so called Thiel Sen estimator for planar data only finds a line y mx b by choosing m to be the median among the n n 1 2 slopes of lines determined by pairs of data points The remaining parameter b can then be found by choosing the median among the values y mx In the dual framework we are given an arrangement of lines in the a b plane Each pair of lines determines a point where the two lines cross we wish to find the median a coordinate among these n choose 2 crossings points This method is only robust to n 1 1 sqrt 2 29n outliers rather than n 2 for the LMS estimator proof if there are n sqrt 2 valid data points they define roughly n 2 4 slopes too many for the outliers to perturb the median However it can be computed more quickly O n log n time by any of several algorithms BC98 CSSS89 DMN92 KS93 M91b I guess in higher dimensions one could apply any robust single point estimation technique e g centerpoint or least median of squares to the normal vectors to hyperplanes defined by d tuples of points But I don t know of any work on such generalizations Regression Depth A nonfit in the context of dependent noise regression is defined to be a vertical hyperplane these are bad because they can t be used to predict the values of the dependent variable Rousseeuw and Hubert HR99 define the depth of a hyperplane to be the minimum number of points it must cross on any continuous motion that moves it to a nonfit It doesn t make any difference if you restrict the class of continuous motions to be rotations around some lower dimensional axis Equivalently one can say that a plane partitions the data into two subsets the depth is the minimum number of points that would have to change from one subset to the other to get a partition achievable by a vertical hyperplane The depth of a plane is a lower bound on the number of outliers that would have to have occurred to make that plane inaccurate The dual version of this is much easier to understand The depth of a point in an arrangement is just the minimum number of lines or hyperplanes that you would need to cross to get from the point out to infinity A good fit according to this measure is then just a point with high depth that is one that s enclosed by a large number of hyperplanes in every possible direction In one dimension a hyperplane is just a point a vertical hyperplane is just a point at infinity and so the median of a point set gives optimal depth n 2 In two dimensions a very simple construction the catline HR98 has depth n 3 This is optimal since one can find sets of points for which no line is better than n 3 It turns out that any point set in any dimension has a hyperplane of depth at least n d 1 exactly matching the quality bounds known for centerpoints ABET98 However the proof is nonconstructive Just as with the centerpoint you probably want the deepest possible plane not just any deep plane This can be found efficiently in the plane KMRSSS99 and less efficiently in higher dimensions One can also efficiently approximate the deepest plane in linear time in any dimension using geometric sampling techniques SW98 Because the primal formulation of this estimator depends on a definition of a vertical direction it makes sense for the dependent or dual independent noise models but not for independent noise It is invariant under affine transformations among only independent variables arbitrary affine transformations of the dual space or more generally any continuous deformation of space that doesn t ever flatten a tetrahedron produce d 1 coplanar points or make a vertical triangle d points forming a vertical plane NEXT Clustering David Eppstein Theory Group Dept Information Computer Science UC Irvine Last update ", "_id": "http://www.ics.uci.edu/~eppstein/280/regress.html", "title": "computational statistics: regression", "html": "<HTML><HEAD>\n<TITLE>Computational Statistics: Regression</TITLE>\n</HEAD><BODY BGCOLOR=\"#FFFFFF\" TEXT=\"#000000\">\n<!--#config timefmt=\"%d %h %Y, %T %Z\" -->\n\n<A HREF=\"/~theory/\">\n<IMG src=\"/~theory/logo/shortTheory.gif\"\nWIDTH=521 HEIGHT=82 BORDER=0 ALT=\"ICS Theory Group\"></A>\n\n\n<H1><A HREF=\"/~eppstein/280/\">ICS 280, Spring 1999:<BR>\nComputational Statistics</A></H1>\n\n<H2>Regression</H2>\n\nThe data model for linear regression is that the data points\nare constrained to lie on some hyperplane (i.e., there is a linear\nrelation between their coordinates).  Another way of interpreting this\nis to treat one of the coordinates as \"dependent\" and the rest as\n\"independent\": the dependent coordinate is then a linear function of the\nindependent ones.  The data may be chosen according to some probability\ndistribution on the line, or there may be some known pattern of\nindependent coordinates (e.g. they are chosen on the grid), at each\npoint of which one measures the dependent coordinate.\nFor the probability distribution models, it doesn't much matter which is\nthe dependent and which the independent variables (the only restriction\nis that the hyperplane must not be parallel to the dependent coordinate axis).\nBut this choice may make a difference if the independent coordinates are\nnonrandom or if the noise model adds noise only to the dependent coordinate.\n\n<P>\nData models in which the data points must satisfy multiple linear\nrelations (i.e. they lie on a lower dimensional subspace) can often\nbe handled by treating each dependent coordinate separately,\nbut this is only appropriate for noise models in which the coordinates\nare independent (e.g. Gaussian noise).\n\n<H3>Projective Duality</H3>\n\nThere's another picture of regression that is mathematically equivalent\nbut may help human intuition.\nInterpret the coordinates (a,b) defining\nany line y=ax+b to instead define a point in a \"dual\" a-b plane.\nThen the set of lines through any point (x,y) in the primal plane\nturns into a set of points b=(-x)a+y in the dual plane; this set is just\na line.\n\n<P>So if we have a collection of points (x<sub>i</sub>,y<sub>i</sub>)\ndefining a regression problem, we can think of them instead as\neach defining a different line b=(-x<sub>i</sub>)a+y<sub>i</sub>\nin the a-b plane.  What we want to find is a point in the a-b plane\nthat is \"near\" all of these lines; that is, we expect the lines\nto roughly look like they all go through a common point, and we want to\nfind that common point.\n\n<P>Similar methods transform any higher dimensional regression problem\nof fitting a single hyperplane to a set of data points\ninto an equivalent problem of fitting a single point\nto an arrangement of hyperplanes.\n\n<H3><A NAME=\"Lp\">L<sub>p</sub> Models for Dependent Noise</A></H3>\n\nLet's first consider the case in which the noise is a random variable\nadded only to the independent variable.\nThe most common algorithm for fitting this model is least squares:\nIf we are trying to find a linear model\ny=m.x+b (where y is the dependent variable, x is the vector of\nindependent variables, and m and b are the parameters we're trying to find)\nthen we measure the error of fit by\n<DIV ALIGN=CENTER>\nE = sum (m.x<sub>i</sub>+b-y<sub>i</sub>)<sup>2</sup>\n</DIV>\nThe least squares fit is the one minimizing this error estimate.\n\n<P>\nThe big advantage of least squares is that it's easy to solve:\nE is just a positive definite quadratic form, so it has a unique\nminimum.  Since E is a smooth function of m and b, its minimum\noccurs at the point where its partial derivatives in each coordinate\nare all zero.  This gives a system of d equations in d unknowns\nwhich one can solve by Gaussian elimination, or one can\ninstead use local improvement methods to approximate the optimal fit.\nThus the time is linear in the number of unknowns, and at worst\ncubic in the dimension.\n\n<P>\nMore generally the L<sub>p</sub> error is formed by using pth powers\nin place of squares in the definition of E.\nThe limit of this, the L<sub>infinity</sub> norm, is\njust the maximum error of any individual data point.\nThe most commonly used of these are the L<sub>1</sub> norm\n(also known as \"least absolute deviation\" or LAD regression)\nwhich is less sensitive to outliers than least squares,\nand the L<sub>infinity</sub> norm,\nwhich is more sensitive to outliers and therefore useful\nin contexts where one wants to detect them e.g. measuring flatness of a\nsurface.\n\n<P>The L<sub>infinity</sub> estimator can be expressed as a low\ndimensional linear program: find (m,b,e), minimizing e,\nand satisfying the linear constraints\n-e&nbsp;<U>&lt;</U>&nbsp;m.x<sub>i</sub>+b-y<sub>i</sub>&nbsp;<U>&lt;</U>&nbsp;e.\nThe dimension (number of variables to be found) is just d+1.\nTherefore, it can be solved in time linear in the number of data points\n(but exponential in the dimension)\nusing low-dimensional linear programming techniques.\n[citations to be filled in later].\nThere also exists a similar linear time bound for L<sub>1</sub> estimation\n[<A HREF=\"bib.html#Z84\">Z84</A>].\nAll L<sub>p</sub> estimators are invariant to coordinate changes\nand affine transformations among only dependent variables.\n\n<P>One advantage of assuming dependent noise is that\nit is preserved by the duality transformation described above:\nthe vertical distance of a point (x<sub>i</sub>,y<sub>i</sub>)\nfrom the line y=ax+b is the same as the vertical distance\nof the point (a,b) from the line\nb=(-x<sub>i</sub>)a+y<sub>i</sub>.\n(Just work out the equations, they're the same!)\n\n<P>In the discussion, one of the students suggested using a convex combination\nof different L<sub>p</sub> metrics, which might e.g. come from a truncated\npower series for some harder-to-estimate metric.  Alternatively, the\ncoefficients in the convex combination might themselves come\nfrom some sort of statistical estimation, to fit the noise\nactually apparent in the data.  However, I don't know of any references\nrelated to these ideas.\n\n<H3><A NAME=\"tls\">L<sub>p</sub> Models for Independent Noise</A></H3>\n\nIf the noise is added to all coordinates (not just the dependent ones)\nit may be appropriate to use the Euclidean distance from the regression\nhyperplane in place of the vertical distance m.x+b-y used above.\nAlternatively, the same considerations as in\n<A HREF=\"point.html#dist\">single point estimation</A>\nmay make a different distance function appropriate.\n\nL<sub>2</sub> estimation with Euclidean distance is known as \"total least\nsquares\"\n[<A HREF=\"bib.html#GL80\">GL80</A>,\n[<A HREF=\"bib.html#M59\">M59</A>,\n[<A HREF=\"bib.html#P01\">P01</A>].\nThe optimal estimator must go through the centroid of the\ndata (this is not too hard to see if one applies duality) but the\nalgebra describing the estimator quality as a function of its parameters\nis messy and hard to invert.  However, the problem can\napparently be solved by singular value decomposition.\n\n<P>There's a nice connection between L<sub>1</sub> estimation in the\nindependent noise model\nand the famous \"k-sets\" or \"k-level\" problem from computational\ngeometry: if you fix a choice of slope, the problem is just a\none-dimensional L<sub>1</sub> problem which can be solved by the median.\nIf we use projective duality,\nwhat we want to know is the median among the points where the dual\nhyperplanes cross each possible vertical line;\nthis sequence of medians forms a surface in the arrangement and is known\nas the \"k-level\" (here k=n/2).  In two dimensions, this surface is just\na polygonal path, formed by walking left-to-right through the\narrangement and turning up or down at every crossing point you pass through.\nA famous problem posed by Erd&ouml;s\nand Lov&aacute;sz is on the number of edges this path can have;\nknown bounds are Omega(n&nbsp;log&nbsp;k) and O(nk<sup>1/3</sup>).\n\n<P>L<sub>infinity</sub> estimation is equivalent to computing the width of\na set (minimum distance between parallel supporting hyperplanes);\nit can be computed in O(n log n) time in the plane, but the best time\nknown in three dimensions is O(n<sup>3/2</sup>) [citations to be filled\nin later].\n\n<P>These independent noise estimators are generally invariant\nunder arbitrary translations or rotations of the coordinate system,\nhowever they are not invariant to general affine transformations.\n\n<H3>Dual-Independent Noise</H3>\n\nIf one has in mind some physical meaning to the dual interpretation,\nin which the data define lines and the estimator one is trying to find\nis a point near all these lines, then it may make sense to use Euclidean\ndistance in the dual plane instead of the primal.\nI.e., we define the distance from the estimator point (a,b)\nto a data line b=(-x<sub>i</sub>)a+y<sub>i</sub>.\nto be the usual Euclidean distance of that point from that line.\n\n<P>With that definition, one can make any sort of L<sub>p</sub>\nregression etc.  The L<sub>1</sub>, L<sub>2</sub>, and\nL<sub>infinity</sub> regressions can all be solved easily using\nsimple modifications of the algorithms for dependent noise models.\n(This is one big advantage of dual independence over primal independence.)\n\n<P>I don't know of any references, or work by actual statisticians,\non this sort of model.\n\n<H3><A NAME=\"LMS\">Least Median of Squares</A></H3>\n\nThe LMS estimator, defined by Rousseeuw\n[<A HREF=\"bib.html#R84\">R84</A>],\nminimizes the median error (rather than the sum of errors or squared\nerrors as in L<sub>1</sub> or L<sub>2</sub> estimation).\nEquivalently, we seek a planar strip of minimum possible\nvertical width that covers at least half the data points.\n\n<P>It is commonly claimed that\nthis estimator provides the maximum possible robustness to outliers\n-- up to half the data may be arbitrarily corrupted.\nThis claim seems to rely on an assumption that the data\nis well spread out along the independent coordinates,\nrather than being tightly clustered along lower dimensional subspaces in\na way that would\nallow an adversary to set up false estimates using large\nnumbers of valid data points.\nBut the same assumption must be made for any other robust regression method.\n\n<P>\nUnfortunately algorithms for computing the LMS estimator are relatively slow:\nO(n<sup>2</sup>) even in two dimensions\n[<A HREF=\"bib.html#SS87\">SS87</A>,\n<A HREF=\"bib.html#ES90\">ES90</A>].\nThere are theoretical reasons for believing that no exact\nalgorithm can be faster than this.\nHowever Mount et al\n[<A HREF=\"bib.html#MNRSW97\">MNRSW97</A>]\nprovide an O(n log n) time approximation as well as\na randomized algorithm that they believe should work well in practice\n(although it lacks theoretical performance guarantees).\n\n<P>In the dual framework, we are given an arrangement of lines,\nand we wish to find the shortest possible vertical line segment\nthat crosses at least half of the lines.\n\n<P>This estimator assumes a dependent noise model;\none can define LMS-like estimator in similar ways for\nindependent noise (in which case one is trying to find \na line minimizing the median distance to the points)\nor for dual-independent noise\n(in which case one is trying to find a minimum-radius circle\nthat crosses at least half of the lines).\n\n<H3><A NAME=\"repmed\">Repeated Median</A></H3>\n\nI don't know what this is, but it's the subject of\n[<A HREF=\"bib.html#MMN98\">MMN98</A>].\n\n<P>Here's my guess, based on the name -- I haven't looked the paper\nitself up yet.  LMS estimation can be interpreted as finding a set of\noutliers to throw away and ignore (namely the points further away\nthan the median distance it finds).  But once it throws away those\npoints, it then does something fairly stupid with the rest of the points:\njust the L<sub>infinity</sub> estimator, which is very sensitive to outliers.\nSo if the data that's left after throwing away half the points is still\nsuspect, it seems more reasonable to instead apply the same outlier\ndetection strategy recursively.\n\n<P>There's not much to say about algorithms for this, at least not\nwithout more progress on algorithms for the LMS estimator.\nThe reason is that if you have a quadratic algorithm for LMS,\nand apply it recursively as above, you can describe the recursion's time\nby the recurrence T(n)=T(n/2)+O(n<sup>2</sup>) which is again quadratic.\nSo asymptotically, the time for this recursive approach would be the\nsame as the time for simple LMS estimation.\n\n<H3><A NAME=\"medslope\">Median Slope</A></H3>\n\nThe so-called Thiel-Sen estimator (for planar data only) finds\na line y=mx+b\nby choosing m\nto be the median among the n(n-1)/2 slopes of lines determined\nby pairs of data points.  The remaining parameter b can then be\nfound by choosing the median among the values y-mx.\n\n<P>In the dual framework, we are given an arrangement of lines\nin the a-b plane.  Each pair of lines determines a point where\nthe two lines cross; we wish to find the median a-coordinate\namong these (n&nbsp;choose&nbsp;2) crossings points.\n\n<P>This method is only robust to n(1-1/sqrt(2)) ~ 0.29n outliers rather\nthan n/2 for the\nLMS estimator (proof: if there are n/sqrt(2) valid data points, they define\nroughly n^2/4 slopes, too many for the outliers to perturb the median).\nHowever, it can be computed more quickly: O(n log n) time by\nany of several algorithms\n[<A HREF=\"bib.html#BC98\">BC98</A>,\n<A HREF=\"bib.html#CSSS89\">CSSS89</A>,\n<A HREF=\"bib.html#DMN92\">DMN92</A>,\n<A HREF=\"bib.html#KS93\">KS93</A>,\n<A HREF=\"bib.html#M91b\">M91b</A>].\n\n<P>I guess in higher dimensions one could apply any robust single point\nestimation technique (e.g. centerpoint or least median of squares) to\nthe normal vectors to hyperplanes defined by d-tuples of points.\nBut I don't know of any work on such generalizations.\n\n<H3><A NAME=\"depth\">Regression Depth</A></H3>\n\nA \"nonfit\" (in the context of dependent-noise regression) is defined to\nbe a vertical hyperplane; these are bad because they can't be used to\npredict the values of the dependent variable.  Rousseeuw and Hubert [<A\nHREF=\"bib.html#RH99\">HR99</A>] define the \"depth\" of a hyperplane to be\nthe minimum number of points it must cross on any continuous motion that\nmoves it to a nonfit.  It doesn't make any difference if you restrict\nthe class of continuous motions to be rotations around some lower\ndimensional axis.  Equivalently, one can say that a plane\npartitions the data into two subsets; the depth is the \nminimum number of points that would have to change from one subset to\nthe other to get a partition achievable by a vertical hyperplane.\nThe depth of a plane is a lower bound on the number\nof outliers that would have to have occurred to make that plane\ninaccurate.\n\n<P>The dual version of this is much easier to understand.\nThe depth of a point in an arrangement is just the minimum\nnumber of lines (or hyperplanes) that you would need to cross\nto get from the point out to infinity.  A good fit according\nto this measure is then just a point with high depth;\nthat is, one that's enclosed by a large number of hyperplanes\nin every possible direction.\n\n<P>In one dimension, a hyperplane is just a point, a vertical hyperplane\nis just a point at infinity, and so the median of a point set\ngives optimal depth n/2.\nIn two dimensions, a very simple construction, the \"catline\"\n[<A HREF=\"bib.html#HR98\">HR98</A>] has depth n/3.\nThis is optimal, since one can find sets of points for which\nno line is better than n/3.\nIt turns out that any point set in any dimension has a hyperplane of depth\nat least n/(d+1), exactly matching the quality bounds known for centerpoints\n[<A HREF=\"bib.html#ABET98\">ABET98</A>].\nHowever the proof is nonconstructive.\n\n<P>Just as with the centerpoint, you probably want the deepest possible\nplane not just any deep plane.  This can be found efficiently in the\nplane\n[<A HREF=\"bib.html#KMRSSS99\">KMRSSS99</A>]\nand less efficiently in higher dimensions.\nOne can also efficiently approximate the deepest plane\nin linear time in any dimension using\n<A HREF=\"cluster.html#eps\">geometric sampling</A> techniques\n[<A HREF=\"bib.html#SW98\">SW98</A>].\n\n<P>Because (the primal formulation of) this estimator depends on a definition\nof a vertical direction, it makes sense for the dependent or dual-independent\nnoise models but not for independent noise.\nIt is invariant under affine transformations among only\nindependent variables, arbitrary affine transformations of the dual space,\nor more generally any continuous deformation of space that\ndoesn't ever flatten a tetrahedron (produce d+1 coplanar points) or make\na vertical triangle (d points forming a vertical plane).\n\n<H2><A HREF=\"cluster.html\">NEXT: Clustering</A></H2>\n\n<HR><P>\n<A HREF=\"/~eppstein/\">David Eppstein</A>,\n<A HREF=\"/~theory/\">Theory Group</A>,\n<A HREF=\"/\">Dept. Information & Computer Science</A>,\n<A HREF=\"http://www.uci.edu/\">UC Irvine</A>.<BR>\n<SMALL>Last update: <!--#flastmod file=\"regress.html\" --></SMALL>\n</BODY></HTML>\n", "id": 1993.0}