{"text": "Graph algorithms Lecture notes on exponential algorithms Throughout algorithms classes we learn that polynomial time bounds are good exponential bad This attitude has led to systematic avoidance of studying exponential time algorithms in theoretical CS so it s an area where there may be many low hanging fruit This is also evidenced by the big gap between known worst case bounds and experimentally measured behavior Why is it reasonable to study exponential algorithms If they re impractical isn t it as useful as counting angels on pinheads Some exponentials are more impractical than others The better ones lead to solutions of moderate sized real problems Improvements mean big differences in solvable problem sizes typically multiplying problem size by some factor while faster technology doesn t help so much typically adding only a small constant to the problem size Polynomial time algorithmics has been criticized on the basis that if everything s fast you don t care exactly how fast it is This may not always be true e g if server must handle many requests speed matters but exponential problems more likely to lead to visible runtimes where improvements can be perceived or even make the difference between solving and not solving a problem The alternative approach of approximation algorithms more standard in theoretical CS is not always suitable approximations can be bad e g for graph coloring problems the cost of computing a true optimal answer may be made up for by the value of having that answer or it may be a problem where approximation does not make sense One can sometimes make the exponential part of the time bound depend on a parameter other than problem size which could be much smaller fixed parameter tractability Growth rates This is a typical freshman exercise but let s go through it again What is a typical reasonable problem size we can solve for a given exponential time bound Modern computers perform roughly 23 operations second So if some algorithm takes time f n how big can n be to solve the problem in the given time Operations 23 236242248254 Time 1 sec1 minute1 hour3 days 6 months f n max n for given time 1 5n4265115966817671 1n2182623 53493921 2n1141361591822 51 3n79951111271421 4n627486991111 5n51617282922n3 364248543n1923263 34n 1214151718nn91 1113142n256677 Typical naive algorithms take times in the range 2n up and can only solve smaller problems Typical fast worst case bounds are in the range 1 2n 1 5n typical empirically measured time bounds are in the range 1 5n 1 1n So for instance we can solve certain kinds of constraint satisfaction problems exactly up to 5 variables even for the hardest examples and examples coming from applications are often not hardest Exponential algorithms can be practical Backtracking search branch and bound A simple example 3 coloring Start with a recursive generate and test 3 coloring algorithm color G i if i n test if valid coloring if so return success else return failure for each possible color c try giving v i color c color G i 1 uncolor v i return failure There are n levels of recursion and the recursion branches 3 ways each level so the time is 3n One obvious improvement interleave validity testing into recursion rather than waiting until the graph is all colored before discovering some early mistake color G i if i n return success for each color c not already used by a neighbor of v i try giving v i color c color G uncolor v i return failure Unfortunately while this will make a practical improvement the worst case is still 3n The problem is that we need to choose ordering of vertices in a way that allows early termination to happen often One possibility spanning tree preorder e g depth first search numbering then each vertex is colored after its parent except for the tree root which has no parent but we only need to try one of the three colors there so we can reduce the number of branches to 2n 1 Changing the solution space Instead of looking for a 3 coloring let s look for the subset of red vertices of a 3 coloring We can then test whether the remaining vertices can be 2 colored i e whether they form a bipartite graph in linear time This immediately gives us a 2n algorithm that s how many different subsets of vertices there are without as much effort as we took above With more thought we can do even better if we choose a coloring with as many red vertices as possible the red vertices will form a maximal independent set a set of vertices with no edge connecting any two vertices in the set such that every remaining vertex in the graph shares an edge with a vertex in the set So we can solve 3 coloring by listing all maximal independent sets and testing for each whether the complementary set is bipartite The algorithm below solves a slightly more general problem given graph G and sets Y and N list the maximal independent sets containing everything in Y and nothing in N listMIS G Y N if G Y u N output Y and return choose a vertex v if v not adjacent to anything in Y listMIS G Y u v N if v isn t the final neighbor of a vertex in N listMIS G Y N u v How to analyze Obviously the time at most 2n and one can come up with examples like in 3 coloring where a careless vertex ordering leads to at least this much time We d like to do better than this Here s an idea each iteration reduces size of G Y N by only one vertex and we want to reduce the set of undecided vertices more quickly If we add v to Y we can quickly remove all its neighbors the more neighbors the better Define degree v number of neighbors in G N Y Then when we add v to Y the size of G Y N is reduced by 1 degree v If we add v to N we don t get as big a reduction in subproblem size but we MUST choose at least one neighbor in Y and reduce it that way so we want all v s neighbors to have high degree listMIS G Y N if G Y u N output Y and return if some vertex in N has all neighbors in N return without output choose vertex v s t all neighbors have degree neighbor degree v e g let v minimum degree vertex in G Y N listMIS G Y u v N u nbrs v let A N for each neighbor w in G Y N listMIS G Y u w A u nbrs w A A u w Analysis suppose v has degree d then we get d 1 recursive calls each to a problem with at least d 1 fewer vertices degree of neighbors could all equal d addition of w to A could be redundant if w is also in later neighborhoods Oversimplified analysis suppose d is the same in all calls it won t be then n d 1 levels of recursion so time d 1 n d 1 d 1 1 d 1 n The worst case is d 3 time 31 3 n Now do the analysis more carefully analyse by counting the number of leaves of the recursion tree total time is polynomial in this We prove by induction that L n 3n 3 In a call with degree v d then we get L n d 1 L n d 1 d 1 3 n d 1 3 induction hyp d 1 3 d 1 3 3 n 3 3 n 3 Corollary any graph has at most 3n 3 maximal independent sets An example showing the analysis is tight disjoint union of n 3 triangles", "_id": "http://www.ics.uci.edu/~eppstein/265/exponential.html", "title": "exponential algorithms", "html": "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 3.2//EN\">\n<html>\n<head>\n<title>Exponential Algorithms</title>\n</head>\n<body>\n<h1>Graph algorithms: Lecture notes on exponential algorithms</h1>\n\n<p>Throughout algorithms classes we learn that polynomial time\nbounds are good, exponential bad. This attitude has led to\nsystematic avoidance of studying exponential time algorithms in\ntheoretical CS, so it's an area where there may be many low-hanging\nfruit. This is also evidenced by the big gap between known\nworst-case bounds and experimentally measured behavior.</p>\n\n<h2>Why is it reasonable to study exponential algorithms?</h2>\n\n<p>If they're impractical, isn't it as useful as counting angels on\npinheads?</p>\n\n<ul>\n<li>Some exponentials are more impractical than others.</li>\n\n<li>The better ones lead to solutions of moderate-sized real\nproblems.</li>\n\n<li>Improvements mean big differences in solvable problem sizes\n(typically multiplying problem size by some factor) while faster\ntechnology doesn't help so much (typically adding only a small\nconstant to the problem size).</li>\n\n<li>Polynomial time algorithmics has been criticized on the basis\nthat if everything's fast, you don't care exactly how fast it is.\nThis may not always be true (e.g. if server must handle many\nrequests, speed matters) but exponential problems more likely to\nlead to visible runtimes where improvements can be perceived or\neven make the difference between solving and not solving a\nproblem.</li>\n\n<li>The alternative approach of approximation algorithms (more\nstandard in theoretical CS) is not always suitable (approximations\ncan be bad, e.g. for graph coloring problems; the cost of computing\na true optimal answer may be made up for by the value of having\nthat answer; or it may be a problem where approximation does not\nmake sense).</li>\n\n<li>One can sometimes make the exponential part of the time bound\ndepend on a parameter other than problem size, which could be much\nsmaller (\"fixed parameter tractability\").</li>\n</ul>\n\n<h2>Growth rates</h2>\n\n<p>(This is a typical freshman exercise, but let's go through it\nagain.)</p>\n\n<p>What is a typical \"reasonable\" problem size we can solve for a\ngiven exponential time bound?</p>\n\n<p>Modern computers perform roughly 2<sup>30</sup> operations/second. So, if\nsome algorithm takes time f(n), how big can n be to solve the\nproblem in the given time?</p>\n\n<table>\n<tr>\n<td width=100>Operations:</td>\n<td width=100>2<sup>30</sup></td>\n<td width=100>2<sup>36</sup></td>\n<td width=100>2<sup>42</sup></td>\n<td width=100>2<sup>48</sup></td>\n<td width=100>2<sup>54</sup> </td>\n</tr>\n\n<tr>\n<td>Time:</td>\n<td>1 sec</td>\n<td>1 minute</td>\n<td>1 hour</td>\n<td>3 days</td>\n<td>&gt; 6 months</td>\n</tr>\n\n<tr>\n<td colspan=\"6\">\n<hr>\n</td>\n</tr>\n\n<tr>\n<td>f(n):</td>\n<td colspan=\"5\">max n for given time:</td>\n</tr>\n\n<tr>\n<td colspan=\"6\">\n<hr>\n</td>\n</tr>\n\n<tr>\n<td>1.05<sup>n</sup></td>\n<td>426</td>\n<td>511</td>\n<td>596</td>\n<td>681</td>\n<td>767</td>\n</tr>\n\n<tr>\n<td>1.1<sup>n</sup></td>\n<td>218</td>\n<td>262</td>\n<td>305</td>\n<td>349</td>\n<td>392</td>\n</tr>\n\n<tr>\n<td>1.2<sup>n</sup></td>\n<td>114</td>\n<td>136</td>\n<td>159</td>\n<td>182</td>\n<td>205</td>\n</tr>\n\n<tr>\n<td>1.3<sup>n</sup></td>\n<td>79</td>\n<td>95</td>\n<td>111</td>\n<td>127</td>\n<td>142</td>\n</tr>\n\n<tr>\n<td>1.4<sup>n</sup></td>\n<td>62</td>\n<td>74</td>\n<td>86</td>\n<td>99</td>\n<td>111</td>\n</tr>\n\n<tr>\n<td>1.5<sup>n</sup></td>\n<td>51</td>\n<td>61</td>\n<td>72</td>\n<td>82</td>\n<td>92</td>\n</tr>\n\n<tr>\n<td>2<sup>n</sup></td>\n<td>30</td>\n<td>36</td>\n<td>42</td>\n<td>48</td>\n<td>54</td>\n</tr>\n\n<tr>\n<td>3<sup>n</sup></td>\n<td>19</td>\n<td>23</td>\n<td>26</td>\n<td>30</td>\n<td>34</td>\n</tr>\n\n<tr>\n<td>n!</td>\n<td>12</td>\n<td>14</td>\n<td>15</td>\n<td>17</td>\n<td>18</td>\n</tr>\n\n<tr>\n<td>n<sup>n</sup></td>\n<td>9</td>\n<td>10</td>\n<td>11</td>\n<td>13</td>\n<td>14</td>\n</tr>\n\n<tr>\n<td>2<sup>n<sup>2</sup></sup></td>\n<td>5</td>\n<td>6</td>\n<td>6</td>\n<td>7</td>\n<td>7</td>\n</tr>\n</table>\n\n<p>Typical naive algorithms take times in the range 2<sup>n</sup>\nup, and can only solve smaller problems. Typical fast worst case\nbounds are in the range 1.2<sup>n</sup> - 1.5<sup>n</sup>. typical\nempirically measured time bounds are in the range 1.05<sup>n</sup>\n- 1.1<sup>n</sup>. So for instance we can solve certain kinds of\nconstraint satisfaction problems exactly up to 500 variables even\nfor the hardest examples (and examples coming from applications are\noften not hardest): <b>Exponential algorithms can be\npractical</b>.</p>\n\n<h2>Backtracking search (branch and bound)</h2>\n\n<p>A simple example: 3-coloring. Start with a recursive\ngenerate-and-test 3-coloring algorithm:</p>\n\n<pre>\ncolor(G,i) {\n   if (i==n) {\n    test if valid coloring\n        if so, return success\n        else return failure\n   }\n   for (each possible color c) {\n       try giving v[i] color c\n       color(G,i+1)\n   }\n   uncolor v[i]\n   return failure\n}\n</pre>\n\n<p>There are n levels of recursion, and the recursion branches 3\nways each level, so the time is 3<sup>n</sup>. One obvious\nimprovement: interleave validity testing into recursion, rather\nthan waiting until the graph is all colored before discovering some\nearly mistake.</p>\n\n<pre>\ncolor(G,i) {\n   if (i==n) return success\n   for (each color c not already used by a neighbor of v[i]) {\n       try giving v[i] color c\n       color(G)\n   }\n   uncolor v[i]\n   return failure\n}\n</pre>\n\n<p>Unfortunately while this will make a practical improvement, the\nworst case is still 3<sup>n</sup>. The problem is that we need to\nchoose ordering of vertices in a way that allows early termination\nto happen often One possibility: spanning tree preorder (e.g. depth\nfirst search numbering) then, each vertex is colored *after* its\nparent (except for the tree root, which has no parent, but we only\nneed to try one of the three colors there) so we can reduce the\nnumber of branches to 2<sup>n-1</sup>.</p>\n\n<h2>Changing the solution space</h2>\n\n<p>Instead of looking for a 3-coloring, let's look for the subset\nof red vertices of a 3-coloring. We can then test whether the\nremaining vertices can be 2-colored (i.e., whether they form a\nbipartite graph) in linear time. This immediately gives us a\n2<sup>n</sup> algorithm (that's how many different subsets of\nvertices there are) without as much effort as we took above. With\nmore thought, we can do even better: if we choose a coloring with\nas many red vertices as possible, the red vertices will form a\n<i>maximal independent set</i>: a set of vertices, with no edge\nconnecting any two vertices in the set, such that every remaining\nvertex in the graph shares an edge with a vertex in the set. So, we\ncan solve 3-coloring by listing all maximal independent sets and\ntesting for each whether the complementary set is bipartite.</p>\n\n<p>The algorithm below solves a slightly more general problem:\ngiven graph G, and sets Y and N, list the maximal independent sets\ncontaining everything in Y and nothing in N.</p>\n\n<pre>\nlistMIS(G,Y,N)\n{\n    if (G = Y u N) output Y and return\n\n    choose a vertex v\n\n    if (v not adjacent to anything in Y)\n    listMIS(G, Y u {v}, N)\n\n    if (v isn't the final neighbor of a vertex in N)\n        listMIS(G, Y, N u {v})\n}\n</pre>\n\n<p>How to analyze? Obviously the time at most 2<sup>n</sup>, and\none can come up with examples like in 3-coloring where a careless\nvertex ordering leads to at least this much time.</p>\n\n<p>We'd like to do better than this. Here's an idea: each iteration\nreduces size of G-Y-N by only one vertex, and we want to reduce the\nset of undecided vertices more quickly. If we add v to Y, we can\nquickly remove all its neighbors (the more neighbors the better).\nDefine degree(v)=number of neighbors in G-N-Y. Then when we add v\nto Y, the size of G-Y-N is reduced by 1+degree(v). If we add v to\nN, we don't get as big a reduction in subproblem size, but we MUST\nchoose at least one neighbor in Y and reduce it that way (so we\nwant all v's neighbors to have high degree)</p>\n\n<pre>\nlistMIS(G,Y,N)\n{\n    if (G = Y u N) output Y and return\n    if (some vertex in N has all neighbors in N) return without output\n\n    choose vertex v s.t. all neighbors have degree(neighbor) &gt;= degree(v)\n    (e.g. let v = minimum degree vertex in G-Y-N)\n\n    listMIS(G, Y u {v}, N u nbrs(v))\n\n    let A = N\n    for each neighbor w in G-Y-N {\n    listMIS(G, Y u {w}, A u nbrs(w))\n    A = A u {w}\n    }\n}\n</pre>\n\n<p>Analysis: suppose v has degree d then we get d+1 recursive calls\neach to a problem with at least d+1 fewer vertices (degree of\nneighbors could all equal d, addition of w to A could be redundant\nif w is also in later neighborhoods</p>\n\n<p>Oversimplified analysis: suppose d is the same in all calls (it\nwon't be) then n/(d+1) levels of recursion so time =\n(d+1)<sup>n/(d+1)</sup> = ((d+1)<sup>1/(d+1)</sup>)<sup>n</sup>.\nThe worst case is d=3, time (3<sup>1/3</sup>)<sup>n</sup>.</p>\n\n<p>Now do the analysis more carefully: analyse by counting the\nnumber of leaves of the recursion tree (total time is polynomial in\nthis). We prove by induction that L(n) &lt;= 3<sup>n/3</sup> In a\ncall with degree(v)=d, then we get</p>\n\n<pre>\nL(n) &lt;= (d+1) L(n - (d+1))\n     &lt;= (d+1) 3^(n - (d+1))/3  [induction hyp.]\n     = (d+1)/3^(d+1)/3 3^(n/3)\n     &lt;= 3^(n/3)\n</pre>\n\n<p>Corollary: any graph has at most 3<sup>n/3</sup> maximal\nindependent sets An example showing the analysis is tight: disjoint\nunion of n/3 triangles</p>\n</body>\n</html>\n\n", "id": 10427.0}