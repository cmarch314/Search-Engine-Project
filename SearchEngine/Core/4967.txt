{"text": "Syllabus This list is very much in flux and in particular is overly ambitious We probably will not get to the last couple of topics so please check back often Linear regression LMS algorithm Normal equations Matrix derivatives Least squares Probablistic motivation Locally weighted linear regression Nearest neighbors Overfitting Classification and logistic regression Sigmoid loss function Perceptron Iteratively weighted least squares Generalized linear models Exponential family Bernoulli Gaussian Recap with GLM models Linear regression Logistic regression Softmax regression Generative models Guassian Quadratic discriminant analysis Multi variate gaussian Probablistic model Comparison with logistic regression Naive Bayes Laplace smoothing Decision Trees Entropy gain Subspace methods Principle Component Analysis PCA Singular Value Decomposition SVD Linear Discriminant Analysis LDA Canonical Correlation Analysis CCA Independant Componant Analysis ICA Neural nets Seperating hyperplanes Hidden layer models Backpropogation Support vector machines Functional and geometric margins Quadratic Program QP Primal forumaiton Lagrange duality Support vectors Kernals Non separability Sequential Minimal Optimization SMO Coordinate ascent SMO Kernalized subspace methods Boosting Exponential loss function Adaboost Viola Jones face detection Learning Theory Bias Variance Consistency Bounds Union bound Chernoff bound Provably Approximately Correct PAC models Loss functions Empircal versus structural risk Sample complexity VC dimension Regularization and model selection Cross validation Feature selection Bayes statistics for regularization Maximum likelihood ML versus Maximum a Posteriori MAP Structured prediction Multi class generalization Vitterbi optimization of markov models Margin based training Conditional Random Feilds Expectation maximization K means clustering Guassian mixture models Expected complete log likelihood", "_id": "http://www.ics.uci.edu/~dramanan/teaching/ics273a_winter08/syllabus.html", "title": "none", "html": "<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML//EN\">\n<html> <head>\n<title></title>\n</head>\n\n<body>\n<h1>Syllabus</h1>\n\nThis list is very much in flux - and in particular, is overly ambitious.<br> We probably will not get to the last couple of topics, so please check back often.\n\n<ol type = I>\n  <li> Linear regression\n  <ol>\n    <li> LMS algorithm\n    <li> Normal equations\n    <ol type = 1>\n      <li> Matrix derivatives\n      <li> Least squares\n    </ol>          \n    <li> Probablistic motivation\n    <li> Locally weighted linear regression\n    <ol type = 1>\n      <li> Nearest neighbors\n      <li> Overfitting\n    </ol>\n  </ol>\n  <li> Classification and logistic regression\n  <ol>\n    <li> Sigmoid loss function\n    <li> Perceptron\n    <li> Iteratively weighted least squares\n  </ol>\n  <li> Generalized linear models\n  <ol>\n    <li> Exponential family\n    <ol>\n      <li> Bernoulli\n      <li> Gaussian\n    </ol>\n    <li> Recap with GLM models\n    <ol>\n      <li> Linear regression\n      <li> Logistic regression\n      <li> Softmax regression\n    </ol>\n  </ol>\n  <li> Generative models\n  <ol>\n    <li> Guassian/Quadratic discriminant analysis\n    <ol>\n      <li> Multi-variate gaussian\n      <li> Probablistic model\n      <li> Comparison with logistic regression\n    </ol>\n    <li> Naive Bayes\n    <ol>\n      <li> Laplace smoothing\n    </ol>\n  </ol>\n  <li> Decision Trees\n  <ol>\n    <li> Entropy gain\n  </ol>\n  <li> Subspace methods\n  <ol>\n    <li> Principle Component Analysis (PCA)\n    <ol>\n      <li> Singular Value Decomposition (SVD)\n    </ol>\n    <li> Linear Discriminant Analysis (LDA)\n    <li> Canonical Correlation Analysis (CCA)\n    <li> Independant Componant Analysis (ICA)\n  </ol>\n  <li> Neural nets\n  <ol>\n    <li> Seperating hyperplanes\n    <li> Hidden layer models\n    <li> Backpropogation\n  </ol>\n  <li> Support vector machines\n  <ol>\n    <li> Functional and geometric margins\n    <li> Quadratic Program (QP) Primal forumaiton\n    <li> Lagrange duality\n    <li> Support vectors\n    <li> Kernals\n    <li> Non-separability\n    <li> Sequential Minimal Optimization (SMO)\n    <ol>\n      <li> Coordinate ascent\n      <li> SMO\n    </ol>\n    <li> Kernalized subspace methods\n  </ol>\n  <li> Boosting\n  <ol>\n    <li> Exponential loss function\n    <li> Adaboost\n    <li> Viola Jones face detection\n  </ol>\n    \n  <li> Learning Theory\n  <ol>\n    <li> Bias/Variance\n    <ol>\n      <li> Consistency\n    </ol>\n    <li> Bounds\n    <ol>\n      <li> Union bound\n      <li> Chernoff bound\n    </ol>\n    <li> Provably Approximately Correct (PAC) models\n    <li> Loss functions\n    <li> Empircal versus structural risk\n    <li> Sample complexity\n    <li> VC dimension\n  </ol>\n  <li> Regularization and model selection\n  <ol>\n    <li> Cross validation\n    <li> Feature selection\n    <li> Bayes statistics for regularization\n    <ol>\n      <li> Maximum likelihood (ML) versus Maximum a-Posteriori (MAP)\n    </ol>\n  </ol>\n  <li> Structured prediction\n  <ol>\n    <li> Multi-class generalization\n    <li> Vitterbi optimization of markov models\n    <li> Margin-based training\n    <li> Conditional Random Feilds\n  </ol>\n  <li> Expectation maximization\n  <ol>\n    <li> K-means clustering\n    <li> Guassian mixture models\n    <li> Expected complete log-likelihood\n  </ol>\n</ol>\n  \n\n\n<hr>\n<address></address>\n</body> </html>\n", "id": 4967.0}