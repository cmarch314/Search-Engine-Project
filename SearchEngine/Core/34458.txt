{"text": "Analysis of Algorithms Introductioni to Computer Science II ICS 22 Introduction In this lecture we will study various ways to analyze the performance of algorithms Performance concerns the amount of resources that an algorithm uses to solve a problem of a certain size typically we will speak about solving a problem using an array of size N or a linked list with N nodes in it In addition we will mostly be concerned with worst case performance other possibilities are best case simple but not much useful information here and average case too complicated mathematically to pursue in an introductory course Finally we will mostly be concerned with the speed time as a resource of algorithms although we will sometimes discuss the amount of storage that they require too space as a resource we can also talk of worst case best case and average case We want to be able to analyze algorithms not just the methods that implement them That means we should be able to say something interesting about the performance of an algorithm independent from having a version of it written in some programming language that a machine can execute Once we examine machine executable versions there is a lot of technology details to deal with what language we write the code in which compiler we use for that language what speed processor we run it on how fast memory is and even how much caching is involved While this information is important to predict running times it is not fundamental to analyzing the algorithms themselves So we will analyze algoritms independent of technology making this subject more scientific Instead we will analyze an algorithm by predicting how many steps it takes and then go through a series of simplifications leading to characterizing an algorithm by its complexity class Although it initially may seem that we have thrown out useful information we will learn how to predict running times of methods on actual machines using an algorithm s complexity class and timing it on the machine that it will be run on Analyzing Algorithms From Machine Language to Big O Notation In this section we will start with a very concrete and technological approach to analyzing algorithms by looking at how Java compiles such code to machine language and then generalize to a science that is independent of such technology First suppose that we invent a mathematical function Iaw N that computes the number of machine code instructions executed by algorithm a when run on the worst case problem of size N Such a function takes an integer as a parameter N the problem size and returns an integer as a result the number of machine language instructions executed Maximum For example the following code computes the maximum value in an array of integers int max Integer MIN VALUE for int i i a length i if a i max max a i We can easily examine the machine language instructions that Java produces for this code by using the debugger If we specify Mixed on the bottom control of its window Java shows us the Java code interspersed with the machine language instructions Below I have duplicated this information but reformatted it to be more understandable I have put comments at the end of every line and put blank lines between what compiler folks call basic blocks int max Integer MIN VALUE A 51E6B3D 12 9 ldc 9 51E6B3F 3D istore 2 for int i i a length i 51E6B4 4 iconst get B 51E6B41 3E istore 3 store it in i 51E6B42 A7 11 goto x 51E6B53 go to test near bottom if a i max 51E6B45 2B aload 1 get a s base address C 51E6B46 1D iload 3 get i 51E6B47 2E iaload get a i 51E6B48 1C iload 2 get max 51E6B49 A4 7 if icmple x 51E6B5 go to test near bottom if max a i 51E6B4C 2B aload 1 get a s base address D 51E6B4D 1D iload 3 get i 51E6B4E 2E iaload get a i 51E6B4F 3D istore 2 store it in max 51E6B5 84 3 1 iinc 3 1 increment i E 51E6B53 1D iload 3 get i F 51E6B54 2B aload 1 get a s 51E6B55 BE arraylength length 51E6B56 A1FFEF if icmplt x 51E6B45 go to if above if Each basic block can be entered only at the top and exited only at the bottom once a basic block is entered all its instructions are executed There may be multiple ways to enter and exit blocks This code is a bit tortuous to read and hand simulate but you can trace through it An easier way to visualize the code in these basics blocks is as a graph which I will annotate with all the information needed to understand computing instruction counts from it Block A initializes max Block B initializes i in the for loop not the branch leading to testing for termination which is near the bottom Block C compare a i to max either falling through to Block D which updates max or skipping this block Block E increments i Block F tests whether the loop should terminate or execute the body again We can compute the exact number of instructions that are to be executed for any inputs For simplicity let us assume that all the array values are bigger than the smallest integer used to initialize max If the array contains values 9 instructions are executed blocks A B F If the array contains 1 value 23 instructions are executed blocks A B F C D E F If the array contains 2 values either 33 instructions are executed the first value is bigger than the second blocks A B F C D E F C E F or 37 instructions are executed the second value is bigger than the first the worst case A B F C D E F C D E F Assuming the worst case from now on if the array contains 3 values 51 instructions are executed Thus for this example the code to compute the maximum value in an array we can write the formula Iaw N 14N 9 At most there are 14 instructions executed during each loop iteration blocks C D E F the housekeeping to initialize max and initialize i and check the first loop iteration requires 9 instructions blocks A B F In fact computing Iab the number of steps in the best case where the if test is true only on the first iteration is Iab N 14N 9 4 N 1 1 N 13 because the 4 instructions updating the max block D are never executed AFTER the first update this formula works only when N Thus the actual number of instructions has a lower bound of 1 N 13 and an upper bound 14N 9 when N Here N is a length the number of values stored in the array and the worst case run will be on an array of strictly increasing values the if test executed during each iteration of the loop is repeatedly true so the machine instructions to copy that value into max block D are always executed Determining the average case is a problem in discrete math given a random distribution there are many say the numbers are distributed uniformly of values how many times on average do we expect to execute block B meaning the next value to be bigger than all the prior ones this is not a simple problem but we can write programs to help understand it Let s return our focus to Iaw N 14N 9 Although this formula is simple we want to make it even simpler if we can Note that as N gets large and most algorithmic analysis is asymptotic it is concerned with what happens as the problem size N gets very large the lower order term 9 can be dropped from this function to simplify it less precision without losing much accuracy For example if N is 1 Iaw N 1 4 9 if we drop the 9 term the simplified answer is just 1 4 which is 99 3 of the correct answer if we increase N to 1 Iaw N 14 9 if we drop the 9 term the simplfied answer is just 14 which is 99 94 of the correct answer if we increase N to 1 Iaw N 14 9 if we drop the 9 term the simplfied answer is just 14 which is 99 994 of the correct answer Thus as N gets large and 1 is not even a very large problem for computers the lower term is not significant so we will drop it to simplify the formula to Iaw N 14N Mathematically if Td N is the dominant term here 14N we can drop any term T N if T N Td N as N infinity note that 9 14N as N inifinity so that term can be dropped Sort For another example think about sorting an array We can use the following simple to code but inefficient algorithm for int base base N base for int check base 1 check N check if a base a check int temp a base a base a check a check temp The code for this example leads to the following basic blocks Assume that for the worst case input every time two values in the array are compared they are found to be in the wrong order and must be swapped The following right side of an EBNF rule models the correct order of execution of basic blocks AH BF CDEF GH with the restriction that he inner repetition happens one fewer times than the outer repetition If the array contains values 7 instructions are executed blocks A H If the array contains 1 value 21 instructions are executed blocks A H B F G H If the array contains 2 values 63 instructions are executed blocks A H B F C D E F G H B F I H If the array contains 3 values 133 instructions are executed The resulting instruction counting function is Iaw N 28N N 1 2 14N 7 14N2 7 For example if N is 1 Iaw N 14 7 if we drop the 7 term the simplified answer is just 14 which is 99 995 of the correct answer Thus as N gets large and 1 is a tiny problem for computers the lower term is not significant so we will drop it to simplify the formula to Iaw N 14N2 Recall our terms T N can be dropped if the limit T N Td N as N infinity note that 7 14N2 as N inifinity so that term can be dropped Now let s take a look at the constant in front of the dominant term it s value doesn t really matter for three important reasons And by getting rid of it we can simplify the formula again First when computing the run time we are just going to multiply this constant by another constant relating to the machine so we can discard this constant if we just use a different constant for the machine Let Taw N denote the time taken by algorithm by algorithm a when run on the worst case problem of size N If our machine executes 1 billion instructions second then the formula in the second case is Taw N 14N2 1x1 9 or Taw N 14N2 Second a major question that we want answered is how much extra work does an algorithm do if the size of a problem is doubled Note that for the second simplified version of Iaw the one with only the dominant term multiplied by a constant Iaw 2N Iaw N 14 2N 2 14N2 4 meaning doubling the problem size quadruples the number of instructions that this algorithm executes so the constant is actually irrelevant to the computation of this ratio Third a major question that we want answered is how the speed of two algorithms compare specifically we want to know whether Iaw N Ibw N as N infinity which would mean that algorithm a gets faster and faster compared to algorithm b Again the constant is irrelevant to this calculation Big O Notation By ignoring all the lower order terms and constants we would say that algorithm a is O N2 which means that the growth rate of the work performed by algorithm a the number of instructions it executes is on the order of N2 This is called big O notation and we use it to specify the complexity class of an algorithm Big O notation doesn t tell us everything that we need to know about the running time of an algorithm For example if two algorithms are O N2 we don t know which will eventually become faster And if one algorithm is O N and another is O N2 we don t know which will be faster for small N But it does economically tell us quite a bit about the performance of an algorithms see the three important questions above We can compute the complexity class of an algorithm by the process shown above or by doing something much simpler determining how often its most frequently executed statement is executed as a function of N Returning to our first example int max Integer MIN VALUE for int i i a length i if a i max max a i the if statement is executed N times where N is the length of the array a length Returning to our second example for int base base N base for int check base 1 check N check if a base a check int temp a base a base a check a check temp the if statement is executed about N2 times where N is the length of the array a length It is actually executed exactly N N 1 2 times for the first outer loop iteration it is executed N 1 times for the second N 2 times for the last times Know that 1 2 3 N N N 1 2 so 1 2 3 N 1 N N 1 2 or N2 2 N 2 Dropping the lower terms an constant yields N2 Finally note that comparing algorithms by their complexity classes is useful only for large N We cannot state authoritatively whether an O N algorithm or an O N2 algorithm is faster for small N but we can state that once we pass some threshold for N call it N the O N algorithm will always be faster than the O N2 algorithm This ignorance is illustrated by the picture below In this example the O N algorithm takes more time for small N Of course by adjusting constants and lower order terms it could also be the case that the O N algorithm is always faster we cannot tell this information solely from the complexity class Technically an algorithm a is O f n if and only if there exist a number M and N such that Iaw n Mf n for all n N This means for example that any O N algorithm is also O N2 too or O f n for any f n that grows faster than linearly Technically is the symbol to use when you know a tight bound on both sides if there exists M1 M2 and N such that M1f n Iaw n M2f n for all n N we say that algorithm a is f n We will use just Big O notation often pretending it is Thega See Big O Notation in the online Wikipedia for more details Complexity Classes Using big O notation we can broadly categorize algorithms by their complexity classes This categorization supplies one kind of excellent information given the time it takes a method implementing an algorithm to solve a problem of size N we can easily compute how long it would take to solve a problem of size 2N For example if a method implementing a certain sorting algorithm is in the complexity class O N2 and it takes about 1 second to sort 1 values it will take about 4 seconds to sort 2 values That is for complexity class O N2 doubling the size of the problem quadruples the amount of time taken executing a method The algebra to prove this fact is simple Assuming Taw N c N2 where c is some technology constant related to the compiler used the speed of the computer and its memory etc the ratio of the time taken to solve a problem of size 2N to the time take to solve a problem of size N is Taw 2N Taw N c 2N 2 c N 2 4cN2 cN2 4 As we saw before the constants are irrelevant they all disappear no matter what the complexity class Likewise using this method to sort 1 values 1 times more data would take about 2 8 hours that is 1 times longer which is 1 2 Here is a short characterization of some common complexity classes there are many others any expression that is a formula using N We will discuss some of these algorithms in more detail later in this handout and use these complexity class to characterize many methods throughout the quarter Complexity ClassClass NameExampleT 2N O 1 ConstantInsertion at the rear of an array Insertion at front of linked list Parameter passingT N O log2N LogarithmicBinary SearchT N constantO N LinearLinear Search arrays or linked lists 2T N O N log2N Log Linear orLinearithmicFast Sorting2T N constantO N2 QuadraticSlow Simple Sorting4T N O N3 CubicNxN Matrix Multiplication8T N O NC Polynomial orGeometric 2cT N O CN ExponentialProving Boolean Equivalences of N variablesCNT N We can compute log2N ln N ln 2 1 4427 ln N Since log base 2 and log base e are linearly related it really makes no difference which we use when using Big O notation because only the constants which we ignore are different You should also memorize that log21 is about 1 actually log21 24 is exactly 1 and that log2Na alog2N From this fact we can easily compute log21 as log21 2 which is 2log21 which is about 2 Do this for log21 one billion Again we should understand that these simple formulas work only when N gets large This is the core of asymptotic algorithmic analysis Note that complexity classes before and including log linear are considered fast their running time does not increase much faster than the size of the problem increases The later complexity classes O N2 O N3 etc are slow but tractable The final complexity class O 2N grows so fast that it is called intractable only small problems in this complexity class can ever be solved For example assume that Ia1w N 1 constant and Ia2w N 1 log2N logrithmic and Ia3w N 1 N linear etc Assume further that we are running code on a machine executing 1 billion 1 9 operations per second Then the following table gives us an intuitive idea of how running times for algorithms in different complexity classes changes with problem size Complexity ClassN 1 N 1 N 1 N 1 O 1 1x1 7seconds1x1 7seconds1x1 7seconds 1x1 7secondsO log2N 3 3x1 7seconds6 6x1 7seconds1 x1 7seconds 2 x1 7secondsO N 1x1 7seconds1x1 6seconds1x1 5seconds 1x1 2secondsO Nlog2N 3 3x1 7seconds6 6x1 6seconds1 x1 5seconds 2 x1 2secondsO N2 1x1 6seconds1x1 4seconds1x1 2seconds 2 7hoursO N3 1x1 5seconds1x1 2seconds1 seconds 3x1 3yearsO 2N 1x1 5seconds4x1 21centuriesfuggidaboutit fuggidaboutit Time Estimation Based on Complexity Class Up until this point we have continually simplified information about algorithms to make our analysis of them easier Have we strayed so far from reality that our information is useless No In this section we will learn how we can easily and accurately say within 1 predict how long it will take a method to solve a large problem size if we know the complexity class for the method and have measured how long the method takes to execute for some large problem size Notice both the measured and predicted problem sizes must be reasonably large otherwise the simplifications used to compute the complexity class will not be accurate the lower order terms will have a real effect on the answer For a first example we will measure and then predict the running time of a simple quadratic sorting method We will use a driver program discussed below in the Sorting section to repeatedly sort an array containing 1 random values and then predict how long it will take this method to sort an array containing of 1 random values and actually compare this prediction to the measured running time for this problem size Because this sorting method is in the O N2 complexity class we simply assume that we can write T N cN2 where we do not know the value of c yet We run the sorting method five times on an array containing 1 random values and measure the average running time it is 22 seconds Now we solve for c Using N 1 we have T 1 c 1 2 22 c 1 6 c 22 1 6 c 2 2 x 1 8 Thus for large N T N 2 2x1 8 N2 seconds Using this formula we can predict that using this method to sort an array of 1 random values would take about 2 2 seconds The actually amount of time is about 2 7 seconds The prediction is 1 1 2 6 2 2 2 6 or 85 accurate so we barely missed our goal of 9 accuracy It would be more accurate if we measured this sort on a 1 value array and predicted the time to sort a 1 value array For a second example we wil measure and then predict the running time of a more complicated log linear sorting method this algorithm is in the lowest complexity class for all those that accomplish sorting We will use a driver program to repeatedly sort an array containing 1 random values and then predict how long it will take this method to sort an array containing of 1 random values and actually compare this prediction to the measured running time for this problem size which is small enough to measure Because this sorting method is in the O N log2N complexity class we simply assume that we can write T N c N Log2N where we do not know the value of c yet We run the sorting method five times on an array containing 1 random values and measure the average running time it is 15 seconds notice that this method sorts 1 times as many values over 1 times faster than the simple quadratic sorting method on the same amount of data Now we solve for c Using N 1 we have T 1 c 1 log 2 1 15 c 1 66 964 c 15 1 66 964 c 9 x 1 8 Thus for large N T N 9 x1 8 N log 2N seconds Using this formula we can predict that using this method to sort an array of 1 random values would take 1 8 seconds The actually amount of time is about 1 6 seconds The prediction is 1 1 1 8 1 6 1 6 or 87 accurate so we again missed our goal of 9 accuracy but only barely Here is a final word on the accuracy of our predictions If we sort the exact same array a few times the sort testing driver easily does this we will see variations of 1 2 likewise we get a slightly greater spread if we sort different arrays but all of the same size Our model predicts that these would all take the same amount of time So all kinds of things operating system what programs it is running what network connections are open etc influence the actually amount of time taken to sort an array In this light the accuracy of our naive predictions is actually quite good Determining Complexity Classes Empirically We have seen that it is fairly simple given an algorithm to determine its complexity class determin how often its most frequently executed statement is executed as a function of N But what if even that is too hard it is too big or convoluted Well if we have a method implementing the algorithm we can actually time it on a few different sized problems and infer the complexity class from the data First be aware that the standard timer in Java is accurate to only 1 second 1 millisecond Call this one tick So to get any kind of accuracy you should run the method on large enough data to take tens to hundreds of ticks milliseconds So run the method on some data of size N enough for the required number of ticks then of size 2N then of size 4N then of size 8N For algorithms in simple complexity classes you should be able to recognize a pattern which will be approximate by not exact If the sequence of values is 1 seconds 2 3 seconds 3 98 seconds and 8 2 seconds the method seems O N Here each doubling approximately doubled the time the method ran If the sequence of values is 1 seconds 3 8 second 17 3 seconds and 7 3 seconds the method seems O N2 Here each doubling approximately quadrupled the time the method ran Of course things get a bit subtle for a complexity class like O Nlog2N but you ll see it always a bit worse than linear but nowhere near quadratic Of course O Nlog22N would behave simlarly so you must apply this process with a bit of skepticism that you are computing perfect answers Searching O N and O log2N Algorithms Linear seaching whether in an array or in a linked list is O N in the worst case where the value being searched for is not in the data structure each value in the data structure must be examined the inner if statement must be executed N times public static int linearSearch int a int value for int i i a length i if a i value return i return 1 Linear searching in an ordered array is no better it is still O N Again in the worst case where the value being searched for is bigger than any value in the data structure each value in the data structure must be examined But there is a way to search an ordered array that is much faster This algorithm for reasons that will become clear soon is called binary searching Let s explore this algorithm first in a more physical context Suppose that we have 1 names in alphabetical sorted order in a phone book one name and its phone number per page only on the front of a page not the back Here is an algorithm to find a person s name and their related phone number in such a phone book Find the middle page in the remaining book If it contains the name that we are looking for we are done Otherwise we rip out that page and tear the remaining phone book in half If the name we are looking for comes before the page we that ripped out we throw away the second half of the phone book If the name we are looking for comes after the page that we ripped out we throw away the first half of the phone book Repeat this process until we find the name or there are no pages left in the phone book This method is called binary search because each iteration divides the problem size the phone book in half bi means two e g bicycle If the original phone book had 1 pages after the first iteration assuming the name we are looking for isn t right in the middle the remaining book would have about 5 pages actually it would have 499 999 In this algorithm the first comparison eliminates about 5 pages After the second comparison we are down to a phone book containing about 25 pages Here one more comparison eliminates about 25 pages not as good as the first comparison but still much better than linear searching where each comparison eliminates just one page If we keep going we ll either find the name or after about 2 comparisons the phone book will be reduced to have no pages Critical to this method is the fact that the phone book is alphabetized ordered it is also critical to be able to find the middle of the phone book quickly which is why this method doesn t work on linked lists To determine the complexity class of this algorithm operating on a sorted array notice that each comparison cuts the remaining array size in half actually because the midpoint is also eliminated with the comparison the size is cut by a bit more than a half For an N page book the maximum number of iterations log2 N the number of times we can divide N by 2 before it is reduced to 1 or the number of times that we can double 1 before reaching N Notice in this algorithm if the array size doubles the number of iterations increases by just 1 the first comparison would cut the doubled array size back to the original array size Again here are some important facts about logarithms that you should memorize 21 1 24 so log2 1 is about 1 log2 X2 2 log2 X so log2 1 log2 1 2 2 log2 1 is about 2 On a calculator compute log2 N ln N ln 2 where ln is logarithm base e provided on most calculators and by the Math log method For N 1 binary search does about 3 iterations but each iteration is more complicated than linear search For N 1 binary search does 2 iterations or 5 times fewer iterations than the worst case for linear search Practically even for arrays of size 1 both algorithms run quickly but binary search runs 5 times faster so when repeatedly searching such a big array binary search would be much much better seconds vs hours Here a method for implementing the binary search algorithm on arrays public static int binarySearch int a int value int low int high a length 1 for if low high low high bounds inverted so return 1 the value is not in the array int mid low high 2 Find middle of the array if a mid value Found value looking for so return mid return its index otherwise else if value a mid determine which half of the high mid 1 array potential stores the else value and continue seraching low mid 1 only that part of the array The following illustration shows how this method executes in a situation where it finds the value it is searching for Notice how it converges on those indexes in the array that might store the searched for value The following illustration shows how this method executes in a situation where it does not find the value it is searching for Again each iteration of the loop reduces the part of the array being looked at by a factor of two How many times can we reduce a size N array before we are left with a single value log2 N the same number of times we can double the size of an array from 1 value to N Finally note that we cannot perform binary searching efficiently on linked lists because we cannot quickly find the middle of a linked list In fact another self referential data structure trees can be used to perform efficient searches Sorting O N2 and O N log2N Algorithms Sorting is one of the most common operations performed on an array of data We saw in the previous section how sorting an array allows it to be searched much more efficiently Sorting algorithms are often divided into two complexity classes simple to understand algorithms whose complexity class is O N2 and more complicated algorithms whose complexity class is O N log2 N The latter are much faster than the former for large arrays see the Time Estimation section which discussed two such sorting algorithms for an example The fast one was the Arrays sort method which sorts any array of objects efficiently it implements an O Nlog2N algorithm with a small constant Here is a brief description of three O N2 sorting algorithms In bubble sort a next position to fill is compared with all later positions swapping out of order values In selection sort the smallest value in the remaining positions is computed and swapped with the value in the next position In insertion sort the next value is moved backward swapped with the value in the previous position in the region of sorted values until it reaches its correct position These algorithms are arranged in both simplest to most complicated order as well as slowest to fastest order for large N Here is a brief description of three O N log2 N sorting algorithms In merge sort pairs of small adjacent ordered arrays the smallest are 1 member arrays are merged repeated into larger ordered arrays until the result is just one ordered array containing all the values In heap sort values are added to and then removed from a special kind of tree data structure called a heap which we will study later its add and remove operations are both O log2 N so adding and then removing N values is NxO log2 N NxO log2 N O N log2 N total In quick sort a pivot value is chosen and then the array is partitioned into three regions on the left are those values less than the pivot in the middle are those equal to the pivot and on the right are those values greater than the pivot then this process is repeated in the left and right regions if they contain more than one value Heap sort is slower than merge sort but it takes no extra space merge sort requires another array that is as big as the array being sorted Technically Quick sort is O N2 But on most arrays itis the fastest and requires no extra space but on pathologicallyh bad arrays which are rare it can take much longer to execute than the other methods All these sorting algorithms are defined as static methods in the Sort class All method have exactly the same prototype so they can be easily interchanged public static void bubble Object a int size Comparator c which includes An array of Object references to be sorted An int specifying how many references are stored in the array this can be a length if the array is filled An object from a class implementing Comparator which decides which objects belong before which others in the sorted array A driver for testing the performance of these sorting methods is in the Sorting Demo application This application includes both the source code for testing these sorting methods as well as Arrays sort which actually runs slower than my fastest method quicksort I didn t expect that finely tuned system code to be that slow You can examine this source code for this method and compare it to my fast sorting methods for clarity and performance I m going to Finally it has been proven that when using comparisons to sort values all algorithms require at least O N log2 N comparisons Thus there are no general sorting algorithms in any complexity class smaller than log linear although better algorithms ones with smaller constants may exist Analyzing Collection Classes Analyzing a collection class is a bit of an art because to do it accurately we need to understand how often each of its methods is called We can however make one reasonable simplifying assumption for most simple collection classes we assume that N values are added to the collection and then those N values are removed from the collection This doesn t always happen but it is reasonable So in the case of simple array implementations of a stack or queue both add methods push and enqueue are O 1 assuming no new memory must be allocated but the pop remove method is O 1 while the dequeue remove method is O N Because NxO 1 is O N and O N O N is O N adding and then removing N values from the stack collection classes is O N Because NxO N is O N2 and O N O N2 is O N2 adding and then removing N values from the queue collection classes is O N2 As another example look at the array implementation of a simple priority queue keeping the array sorted There the enqueue operation is O N because this method scans the array trying to find the correct position based on its priority highest priority is at the rear for the added value In the worst case it has a priority lower than any other value so the entire array must be moved backward to put that value at the front The dequeue operation is just O 1 because it just removes the value at the rear of the array requiring no other data movement Because NxO N is O N2 and NxO 1 is O N and O N2 O N is O N2 adding and then removing N values from this implementation of a priority queue also has a complexity class O N2 If we instead enqueued the value on the rear and dequeued by searching through the array for the highest priority value we would still have one O N term and one O N2 term leading to O N2 as the overall complexity class But later we will learn how implement priority queues with heaps Both enqueue and dequeue are O log2N worse than O 1 but better than O N Thus adding and then removing N values from this implementation of a priority queue is NxO log2N NxO log2N which is O Nlog2N O Nlog2N which is O Nlog2N So balancing the add and remove operations yields a lower complexity class when both operations occur N times Finally when we use an array to store a collection each time that we double the array we must copy its N values By doubling the size we do this only log2 N times when adding N values for a total of Nlog2N copies therefore we can think of each addition as requiring log2N copies this is called the amortized cost of this operation it really doesn t occur on every add but when averaged over all the adds it is correct So in the case of an array implementation of a stack or queue both add methods are actually O log2 N Because NxO log2 N is O N log2 N NxO 1 is O N and NxO N is O N2 The stack class is actually O N log2 N for N pushes and pops while the stack class is actually O N2 for N enqueues and dequeues By this calculation the array implementations have a slightly higher complexity class than those using linked lists But because linked lists allocated a new object for every value put in the linked list the running time of collections using linked lists can actually be higher We will address this problem again when we cover linked lists Efficiency Pragmatics Generally programmers address efficiency concerns after a program has been written clearly and correctly First get it right then make it fast Sometimes see below there is no need to make a program run any faster other times a program must be made to run faster just to test it if tests cannot be performed quickly enough when debugging the program Programs should run as fast as necessary making a program run faster often requires making it more complicated more difficult to generalize etc For example many scientific programs run in just a few seconds Is there a pragmatic reason to work on them to run faster No because it typically takes a few days to collect the data for the program As a component in the entire task the program part is already fast enough Likewise programs that require lots of user interaction don t need to be made more efficient if the computer spends less than a tenth of a second between the user entering his her data and the program prompting for more it is fast enough Finally in the pinball animation program if the model can update and show itself in less than a tenth of a second there is no reason to make it run faster if it cannot then the animation will be slowed down and there is a reason to improve its performance The most famous of all rules of thumb for efficiency is the rule of 9 1 It states that 9 of the time a program takes to run is a result of executing just 1 of its code That is most time in a program s execution is spent in a small amount of its code Modifying this code is the only way to achieve any significant speedup For example suppose a 1 line program runs in 1 minute By the rule of 9 1 executing 1 lines in this program accounts for 54 seconds while executing the remaining 9 lines account for only 6 seconds So if we could locate and study those 1 lines a small part of the program and get them to execute in half the time the total program would run in 27 6 33 seconds which reduces the execution time for the entire program by almost 5 If instead we could study the other 9 lines and get them to execute instantaneously admittedly a very difficult feat the total program would run in 54 54 seconds which reduced the execution time for the entire program by ony 1 Note that if you randomly change code to improve its efficiency 9 of the time you will not be making any changes resulting in a significant improvement Thus a corollary of the 9 1 rule is that for 9 of the code in a program if we make it clearer but less efficient it will not affect the total execution time of the program by much In the above program if we rewrote the 9 lines to make them as clear and simple as possible with no regard for their efficiency and increased their running time by 5 from 6 to 9 seconds the total program would run in 54 9 63 seconds which is only a 5 increase in total execution time Profiling So how does one go about locating the 1 of the program that is accounting for 9 of its execution time For large programs empirical studies show that programmers DO NOT have good intuition about where this hot code is located Instead we should use a tool that computes this information for us A profiler is just such a tool It runs our program for us at a greatly reduced speed but keeps track of either how many times each line is executed or how much time is spent executing each line or method some profilers can collect both kinds of information often collecting more information slows down the program by a larger factor Then we can examine the results produced by running a program using a profiler and learn which code is executing most of the time and focus on it to improve the speed of the entire program Java has a very simple but not so useful built in profiler To use it select Edit from the Metrowerks CodeWarrior toolbar then select Java Application Release Setting In the Target Settings Panel click on Java Target and in the VM Arguments text field enter Xrunhprof cpu times as is illustrated below When you run your program you will get an output file called java hprof txt which contains some useful performance information beyond the scope of this lecture to explain There are commercial products available to evaluate and display the information collected by a profiler in much more sophisticated ways Typically one can speed up a program by a factor of 3 1 very quickly Further gains are slow unless algorithms from lower complexity classes can be found Problem Set To ensure that you understand all the material in this lecture please solve the the announced problems after you read the lecture If you get stumped on any problem go back and read the relevant part of the lecture If you still have questions please get help from the Instructor a TA or any other student When we say an algorithm is O 1 what do we mean Where does the complexity class O N sqrt N fit in the hierarchy shown in this lecture Suppose that the actual time taken to execute an O N2 sorting algorithm on a very fast machine is 2 2x1 8N2 seconds now suppose that the actual time taken to execute an O N log2 N sorting algorithm on a very slow machine is 7 2x1 4 N log2 N seconds here the slow machine is about 1 slower than the fast one For what size arrays will the slower machine running the faster algorithm sort arrays faster than the faster machine running the slower algorithm Suppose that my Sort class has two methods public static void sortByMethod1 Object a Comparator c public static void sortByMethod2 Object a Comparator c After testing these methods exhaustively we find that the first method works faster than the second for all arrays whose length is less than 13 and the second works faster for all bigger arrays Write a method name superSort that has the same prototype and always runs as fast as the fastest of these two methods Rexamine the code for the first example computing the maximum value stored in an array There is one value hint final that is recomputed but it never changes we could instead write code that is a bit more verbose but executes fewer machine language instructions Identify the redundancy write the Java code to fix it and determine Iaw N for the new program You might check your result using Mixed mode in the debugger If the body of the loop were longer would this improvement have a bigger or lesser effect overall Write a program that fills an array of length N with random values Then run the maximum code incrementing a counter whenever a new maximum is found Run this program for large values of N and try to infer a formula for that approximates how many times this happens Run the program over and over again with the same large N then with double that size quadruple that size etc to test your formula ", "_id": "http://www.ics.uci.edu/~pattis/ICS-23/lectures/aa/lecture.html", "title": "analysis of algorithms", "html": "<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML//EN\">\r\n<html>\r\n\r\n<head>\r\n  <title>Analysis of Algorithms</title>\r\n</head>\r\n\r\n<body BGCOLOR=\"white\">\r\n\r\n<center>\r\n<h1>Analysis of Algorithms</h1>\r\n<p>\r\n<h2>Introductioni to Computer Science II<br>\r\nICS-22<br>\r\n</h2>\r\n<p>\r\n</center>\r\n\r\n\r\n<!-- Introduction -->\r\n\r\n<a name=\"Introduction\"><hr align=\"left\" width=\"33%\">\r\n<table cellspacing=\"0\" cellpadding=\"5\" border=\"0\" rules=\"none\" width=\"100%\">\r\n<tbody>\r\n<tr valign=\"top\">\r\n<td width =\"20%\"><b>Introduction</b></td>\r\n<td width =\"80%\">\r\nIn this lecture we will study various ways to analyze the performance of\r\n  algorithms.\r\nPerformance concerns the amount of resources that an algorithm uses to solve\r\n  a problem of a certain size: typically, we will speak about solving a problem\r\n  using an array of size <b>N</b> or a linked list with <b>N</b> nodes in it.\r\nIn addition, we will mostly be concerned with \"worst-case\" performance; other\r\n  possibilities are \"best-case\" (simple, but not much useful information\r\n  here), and \"average-case\" (too complicated mathematically to pursue in an\r\n  introductory course).\r\nFinally, we will mostly be concerned with the speed (time, as a resource) of\r\n  algorithms, although we will sometimes discuss the amount of storage that\r\n  they require too (space, as a resource: we can also talk of worst-case,\r\n  best-case, and average case).\r\n<p>\r\nWe want to be able to analyze algorithms, not just the methods that implement\r\n  them.\r\nThat means we should be able to say something interesting about the performance\r\n  of an algorithm independent from having a version of it (written in some\r\n  programming language) that a machine can execute.\r\nOnce we examine machine-executable versions, there is a lot of technology \r\n  details to deal with: what language we write the code in, which compiler we\r\n  use for that language, what speed processor we run it on, how fast memory is\r\n  (and even how much caching is involved).\r\nWhile this information is important to predict running times, it is not\r\n  fundamental to analyzing the algorithms themselves.\r\nSo, we will analyze algoritms independent of technology, making this subject\r\n  more scientific.\r\n<p>\r\nInstead, we will analyze an algorithm by predicting how many steps it takes,\r\n  and then go through a series of simplifications leading to characterizing an\r\n  algorithm by its complexity class.\r\nAlthough it initially may seem that we have thrown out useful information, we\r\n  will learn how to predict running times of methods on actual machines, using\r\n  an algorithm's complexity class, and timing it on the machine that it will\r\n  be run on.\r\n</td>\r\n</tbody>\r\n</table>\r\n\r\n\r\n<!-- Complexity Classes -->\r\n\r\n<a name=\"Analyze\"><hr align=\"left\" width=\"33%\">\r\n<table cellspacing=\"0\" cellpadding=\"5\" border=\"0\" rules=\"none\" width=\"100%\">\r\n<tbody>\r\n<tr valign=\"top\">\r\n<td width =\"20%\"><b>Analyzing Algorithms: From Machine Language to Big O Notation</b></td>\r\n<td width =\"80%\">\r\nIn this section we will start with a very concrete and technological approach\r\n  to analyzing algorithms -by looking at how Java compiles such code to\r\n  machine language- and then generalize to a science that is independent of\r\n  such technology.\r\nFirst, suppose that we invent a mathematical function <b>Iaw(N)</b> that\r\n  computes the number of machine code instructions executed by algorithm\r\n  <b>a</b> when run on the <b>w</b>orst-case problem of size <b>N</b>.\r\nSuch a function takes an integer as a parameter (<b>N</b>, the problem size)\r\n  and returns an integer as a result (the number of machine language\r\n  instructions executed).\r\n</td>\r\n</tbody>\r\n</table>\r\n<a name=\"Maximum\">\r\n<table cellspacing=\"0\" cellpadding=\"5\" border=\"0\" rules=\"none\" width=\"100%\">\r\n<tbody>\r\n<tr valign=\"top\">\r\n<td width =\"20%\"><b>Maximum</b></td>\r\n<td width =\"80%\">\r\nFor example, the following code computes the maximum value in an array of\r\n  integers.\r\n<b><pre>  int max = Integer.MIN_VALUE;\r\n  for (int i=0; i&lt;a.length; i++)\r\n    if (a[i] &gt; max)\r\n      max = a[i];</pre></b>\r\n<p>\r\nWe can easily examine the machine language instructions that Java produces for\r\n  this code by using the debugger.\r\nIf we specify <b>Mixed</b> on the bottom control of its window, Java shows\r\n  us the Java code interspersed with the machine language instructions.\r\n<p>\r\n<img src=\"images/mixed.gif\"></image>\r\n<p>\r\nBelow, I have duplicated this information, but reformatted it to be more\r\n  understandable.\r\nI have put comments at the end of every line and put blank lines\r\n  between what compiler folks call <b>basic blocks</b>.\r\n<b><pre>\r\n          int max = Integer.MIN_VALUE;                          : <b>A</b>\r\n051E6B3D: 1209           ldc       (#9) <Integer -2147483648>\r\n051E6B3F: 3D             istore_2\r\n\r\n          for (int i=0; i&lt;a.length; i++)\r\n051E6B40: 04      iconst_0               //get 0                : <b>B</b>\r\n051E6B41: 3E      istore_3               //  store it in i\r\n051E6B42: A70011  goto      0x051E6B53   //go to test near bottom\r\n\r\n          if (a[i] > max)\r\n051E6B45: 2B      aload_1                //get a's base address : <b>C</b>\r\n051E6B46: 1D      iload_3                //  get i\r\n051E6B47: 2E      iaload                 //  get a[i]\r\n051E6B48: 1C      iload_2                //get max\r\n051E6B49: A40007  if_icmple  0x051E6B50  //go to test near bottom if &lt;=\r\n\r\n          max = a[i];\t\t\r\n051E6B4C: 2B      aload_1                //get a's base address : <b>D</b>\r\n051E6B4D: 1D      iload_3                //  get i\r\n051E6B4E: 2E      iaload                 //  get a[i]\r\n051E6B4F: 3D      istore_2               //store it in max\r\n\r\n051E6B50: 840301  iinc      3 1          //increment i          : <b>E</b>\r\n\r\n051E6B53: 1D      iload_3                //get i                : <b>F</b>\r\n051E6B54: 2B      aload_1                //get a's\r\n051E6B55: BE      arraylength            //  length\r\n051E6B56: A1FFEF  if_icmplt  0x051E6B45  //go to if above if &lt;</pre></b>\r\n<p>\r\nEach basic block can be entered only at the top and exited only at the bottom.\r\n  once  a basic block is entered, all its instructions are executed.\r\nThere may be multiple ways to enter and exit blocks.\r\nThis code is a bit tortuous to read and hand simulate, but you can\r\n  trace through it.\r\nAn easier way to visualize the code in these basics blocks is as a graph, which\r\n  I will annotate with all the information needed to understand computing \r\n  instruction counts from it.\r\n</tbody>\r\n</table>\r\n  <img src=\"images/basicblocks.gif\"></image>\r\n<table cellspacing=\"0\" cellpadding=\"5\" border=\"0\" rules=\"none\" width=\"100%\">\r\n<tbody>\r\n<tr valign=\"top\">\r\n<td width =\"20%\">&nbsp</td>\r\n<td width =\"80%\">\r\nBlock <b>A</b> initializes <b>max</b>.\r\nBlock <b>B</b> initializes <b>i</b> in the <b>for</b> loop; not the\r\n  branch leading to testing for termination, which is near the bottom.\r\nBlock <b>C</b> compare <b>a[i]</b> to <b>max</b>, either falling through\r\n  to Block <b>D</b> which updates <b>max</b> or skipping this block.\r\nBlock <b>E</b> increments <b>i</b>.\r\nBlock <b>F</b> tests whether the loop should terminate or execute the\r\n  body (again).\r\nWe can compute the exact number of instructions that are to be executed for\r\n  any inputs.\r\nFor simplicity, let us assume that all the array values are bigger than the\r\n  smallest integer (used to initialize <b>max</b>.\r\n<p>\r\nIf the array contains 0 values, 9 instructions are executed: blocks A, B, F.\r\nIf the array contains 1 value, 23 instructions are executed: blocks A, B, F,\r\n  C, D, E, F.\r\nIf the array contains 2 values, either 33 instructions are executed\r\n  (the first value is bigger than the second: blocks A, B, F, C, D, E, F,\r\n  <b>C, E, F</b>) or 37 instructions are executed (the second value is bigger\r\n  than the first - the worst case: A, B, F, C, D, E, F, <b>C, D, E, F</b>).\r\nAssuming the worst case from now on, if the array contains 3 values, 51\r\n  instructions are executed.\r\n...\r\n<p>\r\nThus, for this example -the code to compute the maximum value in an array-\r\n  we can write the formula.\r\n  <b>Iaw(N) = 14N + 9</b>.\r\nAt most there are <b>14</b> instructions executed during each loop iteration\r\n  (blocks C, D, E, F); the housekeeping to initialize <b>max</b>\r\n  and initialize <b>i</b> and check the first loop iteration requires <b>9</b>\r\n  instructions (blocks A, B, F).\r\nIn fact, computing <b>Iab</b> (the number of steps in the <b>best</b> case,\r\n  where the <b>if</b> test is true only on the first iteration, is\r\n  <b><pre>  Iab(N) = 14N + 9 - 4(N-1) = 10N + 13</pre></b>\r\nbecause the <b>4</b> instructions updating the <b>max</b> (block D) are\r\n  never executed AFTER the first update; this formula works only when\r\n  <b>N&gt;0</b>\r\nThus, the actual number of instructions has a lower bound of\r\n  <b>10N + 13</b> and an upper bound <b>14N + 9</b> (when <b>N&gt;0</b>).\r\n<p>\r\nHere <b>N</b> is <b>a.length</b> (the number of values stored in the array),\r\n  and the <b>worst-case</b> run will be on an array of strictly increasing\r\n  values: the <b>if</b> test executed during each iteration of the loop\r\n  is repeatedly <b>true</b>, so the machine instructions to copy that value\r\n  into <b>max</b> (block D) are always executed.\r\nDetermining the <b>average case</b> is a problem in discrete math: given a\r\n  random distribution (there are many; say the numbers are distributed\r\n  <b>uniformly</b>) of values, how many times (on average) do we expect to\r\n  execute block B, meaning the next value to be bigger than all the prior\r\n  ones; this is not a simple problem but we can write programs to help\r\n  understand it.\r\n<p>\r\nLet's return our focus to <b>Iaw(N) = 14N + 9</b>.\r\nAlthough this formula is simple, we want to make it even simpler if we can.\r\nNote that as <b>N</b> gets large (and most algorithmic analysis is asymptotic:\r\n  it is concerned with what happens as the problem size <b>N</b> gets very\r\n  large), the lower order term (<b>9</b>) can be dropped from this function\r\n  to simplify it (less precision), without losing much accuracy.\r\n<p>\r\nFor example, if <b>N</b> is 100, <b>Iaw(N) = 1,409</b>: if we drop the \r\n  <b>9</b> term, the simplified answer is just 1,400 which is 99.3% of the\r\n   correct answer;\r\nif we increase <b>N</b> to 1,000, Iaw(N) = 14,009: if we drop the <b>9</b>\r\n  term, the simplfied answer is just 14,000 which is 99.94% of the correct\r\n  answer; \r\nif we increase <b>N</b> to 10,000, Iaw(N) = 140,009: if we drop the <b>9</b>\r\n  term, the simplfied answer is just 140,000 which is 99.994% of the correct\r\n  answer.\r\n<p>\r\nThus as <b>N</b> gets large (and 10,000 is not even a very large problem for\r\n  computers) the lower term is not significant so we will drop it to simplify\r\n  the formula to <b>Iaw(N) = 14N</b>.\r\n<p>\r\nMathematically, if Td(N) is the dominant term (here <b>14N</b>, we can drop\r\n  any term T(N) if T(N)/Td(N) -> 0 as <b>N</b> -> infinity: note that\r\n  9/14N -> 0 as <b>N</b> -> inifinity, so that term can be dropped.\r\n</td>\r\n</tbody>\r\n</table>\r\n<a name=\"Sort\">\r\n<table cellspacing=\"0\" cellpadding=\"5\" border=\"0\" rules=\"none\" width=\"100%\">\r\n<tbody>\r\n<tr valign=\"top\">\r\n<td width =\"20%\"><b>Sort</b></td>\r\n<td width =\"80%\">\r\nFor another example, think about sorting an array.\r\nWe can use the following simple to code but inefficient algorithm.\r\n<pre><b>  for (int base=0; base&lt;N; base++)\r\n    for (int check=base+1; check&lt;N; check++)\r\n      if (a[base]>a[check]) {\r\n        int temp = a[base];\r\n        a[base]  = a[check];\r\n        a[check] = temp;\r\n      }</b></pre>\r\nThe code for this example leads to the following basic blocks\r\n</tbody>\r\n</table>\r\n  <img src=\"images/sortbasicblocks.gif\"></image>\r\n<table cellspacing=\"0\" cellpadding=\"5\" border=\"0\" rules=\"none\" width=\"100%\">\r\n<tbody>\r\n<tr valign=\"top\">\r\n<td width =\"20%\">&nbsp</td>\r\n<td width =\"80%\">\r\nAssume that for the worst-case input, every time two values in the array are\r\n  compared, they are found to be in the wrong order and must be swapped.\r\nThe following right side of an EBNF rule models the correct order of\r\n  execution of basic blocks: <b>AH{BF{CDEF}GH}</b>, with the restriction\r\n  that he inner repetition happens one fewer times than the outer\r\n  repetition.\r\n<p>\r\nIf the array contains 0 values, 7 instructions are executed: blocks A, H.\r\nIf the array contains 1 value, 21 instructions are executed: blocks A, H, B, \r\n  F, G, H.\r\nIf the array contains 2 values, 63 instructions are executed: blocks A, H, B, \r\n  F, C, D, E, F, G, H, B, F, I, H.\r\nIf the array contains 3 values, 133 instructions are executed\r\nThe resulting instruction-counting function is\r\n<b><pre>  Iaw(N) = 28N(N-1)/2 + 14N +7 = 14N<sup>2</sup> + 7</pre></b>\r\nFor example, if <b>N</b> is 100, <b>Iaw(N) = 140,007</b>: if we drop the \r\n  <b>7</b> term, the simplified answer is just 140,000 which is\r\n   99.995% of the correct answer.\r\nThus as <b>N</b> gets large (and 100 is a tiny problem for\r\n  computers) the lower term is not significant, so we will drop it to simplify\r\n  the formula to <b>Iaw(N) = 14N<sup>2</sup></b>.\r\n<p>\r\nRecall our terms T(N) can be dropped if the limit T(N)/Td(N) -> 0 as <b>N</b>\r\n  -> infinity: note that 7/14N<sup>2</sup> -> 0 as <b>N</b> ->\r\n  inifinity, so that term can be dropped.\r\n<p>\r\nNow, let's take a look at the constant in front of the dominant term; it's\r\n  value doesn't really matter for three important reasons.\r\nAnd, by getting rid of it we can simplify the formula again.\r\n<ol>\r\n<li>First, when computing the run time we are just going to multiply this\r\n      constant by another constant relating to the machine, so we can\r\n      discard this constant if we just use a different constant for the \r\n      machine.\r\n    Let <b>Taw(N)</b> denote the time taken by algorithm by algorithm\r\n      <b>a</b> when run on the <b>w</b>orst-case problem of size <b>N</b>.\r\n    If our machine executes 1 billion instructions/second, then the formula\r\n      in the second case is <b>Taw(N) = 14N<sup>2</sup>/1x10<sup>9</sup></b>,\r\n      or <b>Taw(N) = .000000014N<sup>2</sup></b>.\r\n<p>\r\n<li>Second, a major question that we want answered is how much extra work does\r\n      an algorithm do if the size of a problem is doubled.\r\n    Note that for the second simplified version of <b>Iaw</b> (the one with\r\n      only the dominant term multiplied by a constant), \r\n      <b>Iaw(2N)/Iaw(N) = 14(2N)<sup>2</sup> / 14N<sup>2</sup> = 4</b>\r\n     (meaning doubling the problem size quadruples the number of instructions\r\n     that this algorithm executes), so the constant is actually irrelevant to\r\n     the computation of this ratio.\r\n<p>\r\n<li>Third, a major question that we want answered is how the speed of two\r\n       algorithms compare: specifically, we want to know whether\r\n       Iaw(N)/Ibw(N) -> 0 as <b>N</b> -> infinity, which would mean that\r\n       algorithm <b>a</b> gets faster and faster compared to algorithm\r\n       <b>b</b>.\r\n    Again, the constant is irrelevant to this calculation.\r\n</ol>\r\n</tbody>\r\n</table>\r\n<a name=\"BigO\">\r\n<table cellspacing=\"0\" cellpadding=\"5\" border=\"0\" rules=\"none\" width=\"100%\">\r\n<tbody>\r\n<tr valign=\"top\">\r\n<td width =\"20%\"><b>Big O Notation</b></td>\r\n<td width =\"80%\">\r\nBy ignoring all the lower-order terms and constants, we would say that\r\n  algorithm <b>a</b> is <b>O(N<sup>2</sup>)</b>, which means that the growth\r\n  rate of the work performed by algorithm <b>a</b> (the number of instructions\r\n     it executes) is <b>on the order</b> of N<sup>2</sup>.\r\nThis is called <b>big O</b> notation, and we use it to specify the\r\n  <b>complexity class</b> of an algorithm.\r\n<p>\r\nBig O notation doesn't tell us everything that we need to know about the\r\n   running time of an algorithm.\r\nFor example, if two algorithms are <b>O(N<sup>2</sup>)</b>, we don't know\r\n   which will eventually become faster).\r\nAnd, if one algorithm is <b>O(N)</b> and another is <b>O(N<sup>2</sup>)</b>, we\r\n  don't know which will be faster for small <b>N</b>.\r\nBut, it does economically tell us quite a bit about the performance of an\r\n  algorithms (see the three important questions above).\r\nWe can compute the complexity class of an algorithm by the process shown above,\r\n  or by doing something much simpler: determining how often its most frequently\r\n  executed statement is executed as a function of <b>N</b>.\r\n<p>\r\nReturning to our first example,\r\n<b><pre>  int max = Integer.MIN_VALUE;\r\n  for (int i=0; i&lt;a.length; i++)\r\n    if (a[i] &gt; max)\r\n      max = a[i];</pre></b>\r\n<p>\r\nthe <b>if</b> statement is executed <b>N</b> times, where <b>N</b> is\r\n  the length of the array: <b>a.length</b>.\r\n<p>\r\nReturning to our second example,\r\n<pre><b>  for (int base=0; base&lt;N; base++)\r\n    for (int check=base+1; check&lt;N; check++)\r\n      if (a[base]>a[check]) {\r\n        int temp = a[base];\r\n        a[base]  = a[check];\r\n        a[check] = temp;\r\n      }</b></pre>\r\nthe <b>if</b> statement is executed about <b>N<sup>2</sup></b> times, where\r\n  <b>N</b> is the length of the array: <b>a.length</b>.\r\nIt is actually executed exactly <b>N(N-1)/2</b> times: for the first outer\r\n loop iteration it is executed N-1 times; for the second N-2 times, ... for\r\n the last 0 times.\r\nKnow that <b>1+2+3+...+ N = N(N+1)/2</b>, so <b>1+2+3+...+N-1 = N(N-1)/2</b>\r\n  or <b>N<sup>2</sup>/2 - N/2</b>.\r\nDropping the lower terms an constant yields N<sup>2</sup>.\r\n  <p>\r\nFinally, note that comparing algorithms by their complexity classes is useful\r\n  only for large N.\r\nWe cannot state authoritatively whether an <b>O(N)</b> algorithm or an\r\n  <b>O(N<sup>2</sup>)</b> algorithm is faster for small N; but we can state\r\n  that once we pass some threshold for N (call it N<sub>0</sub>), the\r\n  <b>O(N)</b> algorithm will always be faster than the <b>O(N<sup>2</sup>)</b>\r\n  algorithm.\r\nThis ignorance is illustrated by the picture below.\r\n</tbody>\r\n</table>\r\n  <img src=\"images/graph.gif\"></image>\r\n<table cellspacing=\"0\" cellpadding=\"5\" border=\"0\" rules=\"none\" width=\"100%\">\r\n<tbody>\r\n<tr valign=\"top\">\r\n<td width =\"20%\">&nbsp</td>\r\n<td width =\"80%\">\r\nIn this example, the O(N) algorithm takes more time for small <b>N</b>.\r\nOf course, by adjusting constants and lower-order terms, it could also be the\r\n  case that the O(N) algorithm is always faster; we cannot tell this\r\n  information solely from the complexity class.\r\n<p>\r\nTechnically, an algorithm <b>a</b> is O(f(n)) if and only if there exist a\r\n  number <b>M</b> and <b>N<sub>0</sub></b> such that\r\n  <b>Iaw(n) &lt;= Mf(n)</b> for all <b>n&gt;N<sub>0</sub></b>\r\nThis means for example that any O(N) algorithm is also O(N<sup>2</sup>) too\r\n  (or O(f(n)) for any f(n) that grows faster than linearly).\r\nTechnically &Theta; is the symbol to use when you know a tight bound on both\r\n  sides:\r\nif there exists <b>M<sub>1</sub></b>, <b>M<sub>2</sub></b>, and \r\n  <b>N<sub>0</sub></b> such that\r\n <b>M<sub>1</sub>f(n) &lt;= Iaw(n) &lt;= M<sub>2</sub>f(n)</b> for all\r\n  <b>n&gt;N<sub>0</sub></b>, we say that algorithm <b>a</b> is &Theta;(f(n)).\r\nWe will use just Big O notation, often pretending it is &Thega;.\r\nSee <a href=\"http://en.wikipedia.org/wiki/Big_O_notation\">Big O Notation</a>\r\n  in the online Wikipedia for more details.\r\n</td>\r\n</tbody>\r\n</table>\r\n\r\n\r\n<!-- Complexity Classes -->\r\n\r\n<a name=\"Complexity\"><hr align=\"left\" width=\"33%\">\r\n<table cellspacing=\"0\" cellpadding=\"5\" border=\"0\" rules=\"none\" width=\"100%\">\r\n<tbody>\r\n<tr valign=\"top\">\r\n<td width =\"20%\"><b>Complexity Classes</b></td>\r\n<td width =\"80%\">\r\nUsing big O notation, we can broadly categorize algorithms by their complexity\r\n  classes.\r\nThis categorization supplies one kind of excellent information: given the time\r\n  it takes a method (implementing an algorithm) to solve a problem of size\r\n  <b>N</b>, we can easily compute how long it would take to solve a problem of\r\n  size <b>2N</b>.\r\n<p>\r\nFor example, if a method implementing a certain sorting algorithm is in the\r\n  complexity class O(N<sup>2</sup>), and it takes about 1 second to sort\r\n  10,000 values, it will take about 4 seconds to sort 20,000 values.\r\nThat is, for complexity class O(N<sup>2</sup>), doubling the size of the\r\n  problem quadruples the amount of time taken executing a method.\r\nThe algebra to prove this fact is simple.\r\nAssuming Taw(N) = c*N<sup>2</sup> (where <b>c</b> is some technology constant\r\n  related to the compiler used, the speed of the computer and its memory, etc.)\r\n  the ratio of the time taken to solve a problem of size \r\n    <b>2N</b> to the time take to solve a problem of size <b>N</b> is.\r\n  <pre><b>  Taw(2N)/Taw(N) = c*(2N)<sup>2</sup> / c*(N)<sup>2</sup> = 4cN<sup>2</sup> / cN<sup>2</sup> = 4</b></pre>\r\nAs we saw before, the constants are irrelevant: they all disappear no matter\r\n  what the complexity class.\r\n<p>\r\nLikewise, using this method to sort 1,000,000 values (100 times more data)\r\n  would take about 2.8 hours (that is 10,000 times longer, which is\r\n  100<sup>2</sup>).\r\n<p>\r\nHere is a short characterization of some common complexity classes (there are\r\n  many others: any expression that is a formula using <b>N</b>).\r\nWe will discuss some of these algorithms in more detail later in this handout,\r\n  and use these complexity class to characterize many methods throughout\r\n  the quarter.\r\n<p>\r\n<table cellspacing=\"0\" cellpadding=\"5\" border=\"1\" rules=\"all\">\r\n<thead>\r\n<tr><th>Complexity Class</th><th>Class Name</th><th>Example</th><th>T(2N)</th>\r\n</thead>\r\n<tbody>\r\n<tr><td>O(1)</td>\r\n    <td>Constant</td>\r\n    <td>Insertion at the rear of an array<br>\r\n        Insertion at front of linked list<br>\r\n        Parameter passing</td>\r\n    <td><b>T(N)</b></td>\r\n\r\n<tr><td>O(log<sub>2</sub>N)</td>\r\n    <td>Logarithmic</td>\r\n    <td>Binary Search</td>\r\n    <td><b>T(N)+constant</b></td>\r\n\r\n<tr><td>O(N)</td>\r\n    <td>Linear</td>\r\n    <td>Linear Search (arrays or linked lists)</td>\r\n    <td><b>2T(N)</b></td>\r\n\r\n<tr><td>O(N log<sub>2</sub>N)</td>\r\n    <td>Log-Linear or<br>Linearithmic</td>\r\n    <td>Fast Sorting</td>\r\n    <td><b>2T(N)+constant</b></td>\r\n\r\n<tr><td>O(N<sup>2</sup>)</td>\r\n    <td>Quadratic</td>\r\n    <td>Slow/Simple Sorting</td>\r\n    <td><b>4T(N)</b></td>\r\n\r\n<tr><td>O(N<sup>3</sup>)</td>\r\n    <td>Cubic</td>\r\n    <td>NxN Matrix Multiplication</td>\r\n    <td><b>8T(N)</b></td>\r\n\r\n<tr><td>O(N<sup>C</sup>)</td>\r\n    <td>Polynomial or<br>Geometric</td>\r\n    <td>&nbsp</td>\r\n    <td><b>2<sup>c</sup>T(N)</b></td>\r\n\r\n<tr><td>...</td>\r\n    <td>...</td>\r\n    <td>...</td>\r\n    <td>...</td>\r\n\r\n<tr><td>O(C<sup>N</sup>)</td>\r\n    <td>Exponential</td>\r\n    <td>Proving Boolean Equivalences of <b>N</b> variables</td>\r\n    <td><b>C<sup>N</sup>T(N)</b></td>\r\n</tbody>\r\n</table>\r\n<p>\r\nWe can compute log<sub>2</sub>N = (ln N)/(ln 2) = 1.4427 ln N.\r\nSince log base 2 and log base e are linearly related, it really\r\n  makes no difference which we use when using Big O notation,\r\n  because only the constants (which we ignore) are different.\r\nYou should also memorize that log<sub>2</sub>1000 is about 10 (actually\r\n  log<sub>2</sub>1024 is exactly 10), and that\r\nlog<sub>2</sub>N<sup>a</sup> = alog<sub>2</sub>N.\r\nFrom this fact we can easily compute \r\nlog<sub>2</sub>1,000,000 as\r\nlog<sub>2</sub>1,000<sup>2</sup> which is\r\n 2log<sub>2</sub>1000 which is about 20.\r\nDo this for log<sub>2</sub>1,000,000,000 (one billion).\r\n<p>\r\nAgain, we should understand that these simple formulas work only when\r\n  <b>N</b> gets large.\r\nThis is the core of asymptotic algorithmic analysis\r\nNote that complexity classes before (and including) log-linear are considered\r\n  \"fast\": their running time does not increase much faster than the size of\r\n   the problem increases.\r\nThe later complexity classes O(N<sup>2</sup>), O(N<sup>3</sup>), etc. are slow\r\n  but \"tractable\".\r\nThe final complexity class O(2<sup>N</sup>) grows so fast that it is called\r\n  \"intractable\": only small problems in this complexity class can ever be\r\n  solved.\r\n<p>\r\nFor example, assume that <b>Ia<sub>1</sub>w(N) = 10</b> (constant), and\r\n  <b>Ia<sub>2</sub>w(N) = 10log<sub>2</sub>N</b> (logrithmic), and\r\n  <b>Ia<sub>3</sub>w(N) = 10N</b> (linear), etc.\r\nAssume further that we are running code on a machine executing 1 billion\r\n  (10<sup>9</sup>) operations per second.\r\nThen the following table gives us an intuitive idea of how running times for\r\n  algorithms in different complexity classes changes with problem size.\r\n<table cellspacing=\"0\" cellpadding=\"5\" border=\"1\" rules=\"all\">\r\n<thead>\r\n<tr><th>Complexity Class</th><th>N = 10</th><th>N = 100</th><th>N = 1,000</th>\r\n<th>...</th><th>N = 1,000,000</th>\r\n</thead>\r\n<tbody>\r\n<tr><td>O(1)</td>\r\n    <td>1x10<sup>-7</sup><br>seconds</td>\r\n    <td>1x10<sup>-7</sup><br>seconds</td>\r\n    <td>1x10<sup>-7</sup><br>seconds</td>\r\n    <td>...</td>\r\n    <td>1x10<sup>-7</sup><br>seconds</td>\r\n\r\n<tr><td>O(log<sub>2</sub>N)</td>\r\n    <td>3.3x10<sup>-7</sup><br>seconds</td>\r\n    <td>6.6x10<sup>-7</sup><br>seconds</td>\r\n    <td>10x10<sup>-7</sup><br>seconds</td>\r\n    <td>...</td>\r\n    <td>20x10<sup>-7</sup><br>seconds</td>\r\n\r\n<tr><td>O(N)</td>\r\n    <td>1x10<sup>-7</sup><br>seconds</td>\r\n    <td>1x10<sup>-6</sup><br>seconds</td>\r\n    <td>1x10<sup>-5</sup><br>seconds</td>\r\n    <td>...</td>\r\n    <td>1x10<sup>-2</sup><br>seconds</td>\r\n\r\n<tr><td>O(Nlog<sub>2</sub>N)</td>\r\n    <td>3.3x10<sup>-7</sup><br>seconds</td>\r\n    <td>6.6x10<sup>-6</sup><br>seconds</td>\r\n    <td>10x10<sup>-5</sup><br>seconds</td>\r\n    <td>...</td>\r\n    <td>20x10<sup>-2</sup><br>seconds</td>\r\n\r\n<tr><td>O(N<sup>2</sup>)</td>\r\n    <td>1x10<sup>-6</sup><br>seconds</td>\r\n    <td>1x10<sup>-4</sup><br>seconds</td>\r\n    <td>1x10<sup>-2</sup><br>seconds</td>\r\n    <td>...</td>\r\n    <td>2.7<br>hours</td>\r\n\r\n<tr><td>O(N<sup>3</sup>)</td>\r\n    <td>1x10<sup>-5</sup><br>seconds</td>\r\n    <td>1x10<sup>-2</sup><br>seconds</td>\r\n    <td>10<br>seconds</td>\r\n    <td>...</td>\r\n    <td>3x10<sup>3</sup><br>years</td>\r\n\r\n<tr><td>O(2<sup>N</sup>)</td>\r\n    <td>1x10<sup>-5</sup><br>seconds</td>\r\n    <td>4x10<sup>21</sup><br>centuries</td>\r\n    <td>fuggidaboutit</td>\r\n    <td>...</td>\r\n    <td>fuggidaboutit</td>\r\n</tbody>\r\n</table>\r\n</td>\r\n</tbody>\r\n</table>\r\n\r\n\r\n\r\n<!-- Time Estimation -->\r\n\r\n<a name=\"Time\"><hr align=\"left\" width=\"33%\">\r\n<table cellspacing=\"0\" cellpadding=\"5\" border=\"0\" rules=\"none\" width=\"100%\">\r\n<tbody>\r\n<tr valign=\"top\">\r\n<td width =\"20%\"><b>Time Estimation Based on Complexity Class</b></td>\r\n<td width =\"80%\">\r\nUp until this point we have continually simplified information about\r\n  algorithms to make our analysis of them easier.\r\nHave we strayed so far from reality that our information is useless.\r\nNo!\r\nIn this section we will learn how we can easily and accurately (say, within\r\n  10%) predict how long it will take a method to solve a large problem size,\r\n  if we know the complexity class for the method, and have measured how long\r\n  the method takes to execute for some large problem size.\r\nNotice both the measured and predicted problem sizes must be reasonably large,\r\n  otherwise the simplifications used to compute the complexity class will not\r\n  be accurate: the lower order terms will have a real effect on the answer.\r\n<p>\r\nFor a first example, we will measure, and then predict, the running time of a\r\n  simple, quadratic sorting method.\r\nWe will use a driver program (discussed below, in the Sorting section) to\r\n  repeatedly sort an array containing 1,000 random values, and then predict\r\n  how long it will take this method to sort an array containing of 10,000\r\n  random values (and actually compare this prediction to the measured running\r\n  time for this  problem size).\r\n<ol>\r\n<li>Because this sorting method is in the <b>O(N<sup>2</sup>)</b> complexity\r\n      class, we simply assume that we can write <b>T(N) = cN<sup>2</sup></b>\r\n      where we do not know the value of <b>c</b> yet.\r\n<li>We run the sorting method five times on an array containing 1,000 random\r\n       values and measure the  average running time: it is <b>.022</b> seconds.\r\n</ol>\r\nNow we solve for <b>c</b>.\r\nUsing <b>N = 1000</b> we have\r\n<pre><b>  T(1000) = c 1000<sup>2</sup>\r\n.022    = c 10<sup>6</sup>\r\nc       = .022/10<sup>6</sup>\r\nc       = 2.2 x 10<sup>-8</sup></b></pre> \r\nThus for large <b>N</b>, <b>T(N) =  2.2x10<sup>-8</sup> N<sup>2</sup></b>\r\n  seconds.\r\nUsing this formula, we can predict that using this method to sort an array of\r\n  10,000 random values would take about 2.2 seconds.\r\nThe actually amount of time is about 2.7 seconds.\r\nThe prediction is <b>100[1-(2.6-2.2)/2.6]</b> or 85% accurate (so, we barely\r\n  missed our goal of 90% accuracy).\r\nIt would be more accurate if we measured this sort on a 10,000 value array\r\n  and predicted the time to sort a 100,000 value array.\r\n<p>\r\nFor a second example, we wil measure, and then predict, the running time of a\r\n  more complicated log-linear  sorting method (this algorithm is in the lowest\r\n  complexity class for all those that accomplish sorting).\r\nWe will use a driver program to repeatedly sort an array containing 10,000\r\n   random values, and then predict how long it will take this method to sort\r\n   an array containing of 1,000,000 random values (and actually compare this\r\n   prediction to the measured running time for this problem size, which is\r\n   small enough to measure).\r\n<ol>\r\n<li>Because this sorting method is in the <b>O(N log<sub>2</sub>N)</b>\r\n      complexity class, we simply assume that we can write\r\n     <b>T(N) = c(N Log<sub>2</sub>N)</b> where we do not know the value of\r\n     <b>c</b> yet.\r\n<li>We run the sorting method five times on an array containing 100,000 random\r\n      values and measure the  average running time: it is <b>.15</b> seconds\r\n      (notice that this method sorts 10 times as many values over 10 times\r\n      faster than the simple quadratic sorting method on the same amount of\r\n      data).\r\n</ol>\r\nNow we solve for <b>c</b>.\r\nUsing <b>N = 100,000</b> we have\r\n<pre><b>\r\nT(100,000) = c (100,000 log <sub>2</sub> 100,000)\r\n.15        = c 1,660,964\r\nc          = .15/1,660,964\r\nc          = 9.0 x 10<sup>-8</sup></b></pre>\r\nThus for large <b>N</b>,\r\n  <b>T(N) =  9.0x10<sup>-8</sup>(N log <sub>2</sub>N)</b> seconds.\r\n<!---\r\nNote that the constant for this sorting method is about 3-4 times bigger than\r\n  the constant for the quadratic sorting method -meaning the code is\r\n  probably longer- but the complexity class is better, so the final result\r\n  should be faster for big arrays. \r\n--->\r\nUsing this formula, we can predict that using this method to sort an array of\r\n  1,000,000 random values would take 1.8 seconds.\r\nThe actually amount of time is about 1.6 seconds.\r\nThe prediction is <b>100[1-(1.8-1.6)/1.6]</b> or 87% accurate (so, we again\r\n  missed our goal of 90% accuracy, but only barely).\r\n<p>\r\nHere is a final word on the accuracy of our predictions.\r\nIf we sort the exact same array a few times (the sort testing driver easily\r\n  does this) we will see variations of 10%-20%; likewise we get a slightly\r\n  greater spread if we sort different arrays (but all of the same size).\r\nOur model predicts that these would all take the same amount of time.\r\nSo all kinds of things (operating system, what programs it is running, what\r\n  network connections are open, etc.) influence the actually amount of time\r\n  taken to sort an array.\r\nIn this light, the accuracy of our \"naive\" predictions is actually quite good. \r\n</td>\r\n</tbody>\r\n</table>\r\n\r\n\r\n<!-- Complexity Classes Empirically -->\r\n\r\n<a name=\"Empirically\"><hr align=\"left\" width=\"33%\">\r\n<table cellspacing=\"0\" cellpadding=\"5\" border=\"0\" rules=\"none\" width=\"100%\">\r\n<tbody>\r\n<tr valign=\"top\">\r\n<td width =\"20%\"><b>Determining Complexity Classes Empirically</b></td>\r\n<td width =\"80%\">\r\nWe have seen that it is fairly simple, given an algorithm, to determine\r\n  its complexity class: determin how often its most frequently executed\r\n  statement is executed as a function of <b>N</b>.\r\nBut what if even that is too hard: it is too big or convoluted.\r\nWell, if we have a method implementing the algorithm, we can actually time\r\n  it on a few different-sized problems and infer the complexity class from\r\n  the data.\r\n<p>\r\nFirst, be aware that the standard timer in Java is accurate to only .001\r\n  second (1 millisecond).\r\nCall this one <b>tick</b>\r\nSo, to get any kind of accuracy, you should run the method on large enough\r\n  data to take tens to hundreds of ticks (milliseconds).\r\n<p>\r\nSo, run the method on some data of size <b>N</b>, enough for the required\r\n  number of ticks, then of size <b>2N</b>, then of size <b>4N</b>,\r\n  then of size <b>8N</b>.\r\nFor algorithms in simple complexity classes, you should be able to recognize\r\n  a pattern (which will be approximate by not exact).\r\nIf the sequence of values is 1.0 seconds, 2.03 seconds, 3.98 seconds, and\r\n   8.2 seconds: the method seems O(N).\r\nHere each doubling approximately doubled the time the method ran.\r\nIf the sequence of values is 1.0 seconds, 3.8 second, 17.3 seconds, and\r\n   70.3 seconds: the method seems O(N<sup>2</sup>).\r\nHere each doubling approximately quadrupled the time the method ran.\r\n<p>\r\nOf course, things get a bit subtle for a complexity class like\r\n  O(Nlog<sub>2</sub>N), but you'll see it always a bit worse than linear, but\r\n  nowhere near quadratic.\r\nOf course O(Nlog<sup>2</sup><sub>2</sub>N) would behave simlarly, so you\r\n  must apply this process with a bit of skepticism that you are computing\r\n  perfect answers.\r\n</td>\r\n</tbody>\r\n</table>\r\n\r\n\r\n<!-- Searching -->\r\n\r\n<a name=\"Searching\"><hr align=\"left\" width=\"33%\">\r\n<table cellspacing=\"0\" cellpadding=\"5\" border=\"0\" rules=\"none\" width=\"100%\">\r\n<tbody>\r\n<tr valign=\"top\">\r\n<td width =\"20%\"><b>Searching: O(N) and O(log<sub>2</sub>N) Algorithms</b></td>\r\n<td width =\"80%\">\r\nLinear seaching, whether in an array or in a linked list, is O(N); in the worst\r\n  case (where the value being searched for is not in the data structure), each\r\n  value in the data structure must be examined (the inner <b>if</b> statement\r\n  must be executed <b>N</b> times).\r\n<pre><b>\r\n  public static int linearSearch (int[] a, int value)\r\n  {\r\n    for (int i=0; i&lt;a.length; i++)\r\n      if (a[i] == value)\r\n        return i;\r\n    return -1;\r\n  }</b></pre>\r\nLinear searching in an ordered array is no better: it is still O(N).\r\nAgain, in the worst case (where the value being searched for is bigger than any\r\n  value in the data structure), each value in the data structure must be\r\n  examined.\r\nBut, there is a way to search an ordered array that is much faster.\r\nThis algorithm, for reasons that will become clear soon, is called\r\n  <b>binary searching</b>.\r\n<p>\r\nLet's explore this algorithm first in a more physical context.\r\nSuppose that we have 1,000,000 names in alphabetical (sorted) order in a phone\r\n  book, one name and its phone number per page (only on the front of a page,\r\n  not the back).\r\nHere is an algorithm to find a person's name (and their related phone number)\r\n  in such a phone book\r\n<ol>\r\n<li>Find the middle page in the (remaining) book\r\n<li>If it contains the name that we are looking for, we are done\r\n<li>Otherwise, we rip out that page and tear the remaining phone book in half\r\n  <ol>\r\n  <li>If the name we are looking for comes before the page we that ripped out,\r\n         we throw away the second half of the phone book\r\n  <li>If the name we are looking for comes after the page that we ripped out,\r\n         we throw away  the first half of the phone book\r\n  </ol>\r\n<li>Repeat this process until we find the name or there are no pages left in\r\n    the phone book\r\n</ol>\r\nThis method is called binary search because each \"iteration\" divides the\r\n  problem size (the phone book) in half (bi means two: e.g., bicycle).\r\n<p>\r\nIf the original phone book had 1,000,000 pages, after the first iteration\r\n  (assuming the name we are looking for isn't right in the middle) the\r\n   remaining book would have about 500,000 pages (actually it would have\r\n   499,999).\r\nIn this algorithm, the first comparison eliminates about 500,000 pages!\r\nAfter the second comparison, we are down to a phone book containing about\r\n  250,000 pages.\r\nHere one more comparison eliminates about 250,000 pages; not as good as the\r\n  first comparison, but still much better than linear searching, where each\r\n  comparison eliminates just one page!\r\nIf we keep going, we\ufffdll either find the name or after about 20 comparisons\r\n  the phone book will be reduced to have no pages.\r\nCritical to this method is the fact that the phone book is alphabetized\r\n  (ordered); it is also critical to be able to find the middle of the phone\r\n   book quickly (which is why this method doesn't work on linked lists).\r\n<p>\r\nTo determine the complexity class of this algorithm (operating on a sorted\r\n  array), notice that each comparison cuts the remaining array size in half\r\n  (actually, because the midpoint is also eliminated with the comparison,\r\n  the size is cut by a bit more than a half).\r\nFor an <b>N</b> page book, the maximum number of iterations log<sub>2</sub>\r\n  <b>N</b> (the number of times we can divide <b>N</b> by 2 before it is\r\n  reduced to 1; or, the number of times that we can double 1 before\r\n  reaching N).\r\nNotice in this algorithm if the array size doubles, the number of iterations\r\n  increases by just 1: the first comparison would cut the doubled array size\r\n  back to the original array size.\r\n<p>\r\nAgain, here are some important facts about logarithms that you should memorize.\r\n<ol>\r\n<li>2<sup>10</sup> = 1,024 so log<sub>2</sub> 1,000 is about 10\r\n<li>log<sub>2</sub> X<sup>2</sup>  = 2 log<sub>2</sub> X;\r\n      so log<sub>2</sub> 1,000,000 = log<sub>2</sub> 1000<sup>2</sup> =\r\n      2 log<sub>2</sub> 1000 is about 20\r\n<li>On a calculator, compute log<sub>2</sub> <b>N</b> = ln <b>N</b> / ln 2\r\n      (where ln is logarithm base e, provided on most calculators -and by the\r\n      <b>Math.log</b> method).\r\n</ol>\r\nFor <b>N</b> = 10, binary search does about 3 iterations, but each iteration\r\n  is more complicated than linear search.\r\nFor <b>N</b> = 1,000,000 binary search does 20 iterations, or 50,000 times\r\n  fewer iterations than the worst case for linear search!\r\nPractically, even for arrays of size 1,000,000 both algorithms run quickly,\r\n  but binary search runs 50,000 times faster so when repeatedly searching such\r\n  a big array, binary search would be much much better (seconds vs. hours).\r\n<p>\r\nHere a method for implementing the binary search algorithm on arrays.\r\n  <pre><b>  public static int binarySearch (int[] a, int value)\r\n  {\r\n    int low  = 0;\r\n    int high = a.length-1\r\n    for(;;) {\r\n      if (low > high)           //low/high bounds inverted, so\r\n        return -1;              //  the value is not in the array\r\n\r\n      int mid = (low+high)/2;   //Find middle of the array\r\n\r\n      if (a[mid] == value)      //Found value looking for, so\r\n        return mid;             //  return its index; otherwise\r\n      else if (value < a[mid])  //determine which half of the\r\n        high = mid-1;           //  array potential stores the\r\n      else                      //  value and continue seraching\r\n        low  = mid+1;           //  only that part of the array\r\n  }</b></pre>\r\nThe following illustration shows how this method executes in a situation where\r\n  it finds the value it is searching for.\r\nNotice how it converges on those indexes in the array that might store the\r\n searched for value.\r\n</tbody>\r\n</table>\r\n  <img src=\"images/bs1.gif\"></image>\r\n<table cellspacing=\"0\" cellpadding=\"5\" border=\"0\" rules=\"none\" width=\"100%\">\r\n<tbody>\r\n<tr valign=\"top\">\r\n<td width =\"20%\">&nbsp</td>\r\n<td width =\"80%\">\r\nThe following illustration shows how this method executes in a situation where\r\n  it does not find the value it is searching for.\r\n</tbody>\r\n</table>\r\n  <img src=\"images/bs2.gif\"></image>\r\n<table cellspacing=\"0\" cellpadding=\"5\" border=\"0\" rules=\"none\" width=\"100%\">\r\n<tbody>\r\n<tr valign=\"top\">\r\n<td width =\"20%\">&nbsp</td>\r\n<td width =\"80%\">\r\nAgain, each iteration of the loop reduces the part of the array being looked\r\n  at by a factor of two.\r\nHow many times can we reduce a size <b>N</b> array before we are left with a\r\n  single value? log<sub>2</sub> N\r\n  (the same number of times we can double the size of an array from 1 value to\r\n   N).\r\n<p>\r\nFinally, note that we cannot perform binary searching efficiently on linked\r\n  lists, because we cannot quickly find the middle of a linked list.\r\nIn fact, another self-referential data structure, trees,  can be used\r\n  to perform efficient searches.  \r\n</td>\r\n</tbody>\r\n</table>\r\n\r\n\r\n\r\n<!-- Sorting -->\r\n\r\n<a name=\"Sorting\"><hr align=\"left\" width=\"33%\">\r\n<table cellspacing=\"0\" cellpadding=\"5\" border=\"0\" rules=\"none\" width=\"100%\">\r\n<tbody>\r\n<tr valign=\"top\">\r\n<td width =\"20%\"><b>Sorting: O(N<sup>2</sup>) and O(N log<sub>2</sub>N) Algorithms</b></td>\r\n<td width =\"80%\">\r\nSorting is one of the most common operations performed on an array of data.\r\nWe saw in the previous section how sorting an array allows it to be searched\r\n  much more efficiently.\r\nSorting algorithms are often divided into two complexity classes: simple to\r\n  understand algorithms whose complexity class is O(N<sup>2</sup>) and more\r\n  complicated algorithms whose complexity class is O(N log<sub>2</sub> N).\r\nThe latter are much faster than the former for large arrays\r\n  (see the <a href=\"#Time\" target=\"main\">Time Estimation</a> section,\r\n  which discussed two such sorting algorithms, for an example).\r\nThe fast one was the <b>Arrays.sort</b> method which sorts any array of objects\r\n  efficiently: it implements an O(Nlog<sub>2</sub>N) algorithm with a small\r\n  constant.\r\n<p>\r\nHere is a brief description of three O(N<sup>2</sup>) sorting algorithms.\r\n<ol>\r\n<li>In bubble sort, a next position to fill is compared with all later\r\n       positions, swapping out-of-order values.\r\n<li>In selection sort, the smallest value in the remaining positions is\r\n      computed and swapped with the value in the next position.\r\n<li>In insertion sort, the next value is moved backward (swapped with the\r\n       value in the previous position in the region of sorted values) until\r\n       it reaches its correct position.\r\n</ol> \r\nThese algorithms are arranged in both simplest-to-most-complicated order, as\r\n  well as slowest-to-fastest order for large N.\r\n<p>\r\nHere is a brief description of three O(N log<sub>2</sub> N) sorting algorithms.\r\n<ol>\r\n<li>In merge sort, pairs of small, adjacent ordered arrays (the smallest are \r\n      1 member arrays) are merged repeated into larger ordered arrays until the\r\n      result is just one ordered array containing all the values.\r\n<li>In heap sort, values are added to and then removed from a special kind of \r\n      tree data structure called a heap (which we will study later: its add\r\n      and remove operations are both O(log<sub>2</sub> N), so adding and then\r\n      removing <b>N</b> values is NxO(log<sub>2</sub> N) + NxO(log<sub>2</sub>\r\n      N) = O(N log<sub>2</sub> N) total.\r\n<li>In quick sort, a pivot value is chosen and then the array is partitioned\r\n      into three regions: on the left are those values less than the pivot, in \r\n      the middle are those equal to the pivot, and on the right are those\r\n      values greater than the pivot; then this process is repeated in the\r\n      left and right regions (if they contain more than one value).\r\n</ol>\r\n Heap sort is slower than merge sort, but it takes no extra space (merge sort\r\n  requires another array that is as big as the array being sorted).\r\nTechnically, Quick sort is O(N<sup>2</sup>).\r\nBut on most arrays itis the fastest (and requires no extra space), but on\r\n  pathologicallyh bad arrays, which are rare, it can take much longer to\r\n  execute than the other methods.\r\n<p>\r\nAll these sorting algorithms are defined as <b>static</b> methods in the\r\n  <b>Sort</b> class.\r\nAll method have exactly the same prototype (so they can be easily interchanged)\r\n  <pre><b>  public static void bubble (Object[] a, int size, Comparator c)</b></pre>\r\n  which includes\r\n<ol>\r\n<li>An array of <b>Object</b> references to be sorted.\r\n<li>An <b>int</b> specifying how many references are stored in the array;\r\n      this can be <b>a.length</b> if the array is filled.\r\n<li>An object from a class implementing <b>Comparator</b>, which decides which\r\n        objects belong before which others in the sorted array.\r\n</ol>\r\nA driver for testing the performance of these sorting methods is in the\r\n  <a href=\"../../programs/sorting.zip\">Sorting Demo</a> application.\r\nThis application includes both the source code for testing these sorting\r\n  methods, as well as <b>Arrays.sort</b> which actually runs slower than\r\n  my fastest method: quicksort.\r\nI didn't expect that finely tuned system code to be that slow.\r\nYou can examine this source code for this method and compare it to my\r\n  fast sorting methods for clarity and performance (I'm going to).\r\n<p>\r\nFinally, it has been proven that when using comparisons to sort values, all\r\n  algorithms require at least O(N log<sub>2</sub> N) comparisons.\r\nThus, there are no general sorting algorithms in any complexity class smaller\r\n  than log-linear (although better algorithms -ones with smaller constants- may\r\n  exist).\r\n</td>\r\n</tbody>\r\n</table>\r\n\r\n\r\n<!-- Collections -->\r\n\r\n<a name=\"Collections\"><hr align=\"left\" width=\"33%\">\r\n<table cellspacing=\"0\" cellpadding=\"5\" border=\"0\" rules=\"none\" width=\"100%\">\r\n<tbody>\r\n<tr valign=\"top\">\r\n<td width =\"20%\"><b>Analyzing Collection Classes</b></td>\r\n<td width =\"80%\">\r\nAnalyzing a collection class is a bit of an art, because to do it accurately\r\n  we need to understand how often each of its methods is called.\r\nWe can, however, make one reasonable simplifying assumption for most simple\r\n  collection classes:  we assume that <b>N</b> values are added to the\r\n  collection and then those <b>N</b> values are removed from the collection.\r\nThis doesn't always happen, but it is reasonable.\r\n<p>\r\nSo, in the case of simple array implementations of a stack or queue, both \"add\"\r\n  methods (<b>push</b> and <b>enqueue</b>) are O(1) (assuming no new memory\r\n  must be allocated); but the <b>pop</b> remove method is O(1) while the\r\n  <b>dequeue</b>) remove method is  O(N).\r\n  Because NxO(1) is O(N), and O(N)+O(N) is O(N), adding and then removing\r\n  <b>N</b> values from the stack collection classes is O(N).\r\n  Because NxO(N) is O(N<sup>2</sup>), and O(N)+O(N<sup>2</sup>) is\r\n  O(N<sup>2</sup>), adding and then removing <b>N</b> values from the queue\r\n  collection classes is O(N<sup>2</sup>).\r\n<p>\r\nAs another example, look at the array implementation of a simple priority\r\n  queue, keeping the array sorted.\r\nThere, the <b>enqueue</b> operation is O(N) because this method scans the\r\n  array trying to find the correct position (based on its priority; highest\r\n  priority is at the rear) for the added value.\r\nIn the worst case, it has a priority lower than any other value, so the entire\r\n  array must be moved backward to put that value at the front.\r\nThe <b>dequeue</b> operation is just O(1), because it just removes the value\r\n  at the rear of the array, requiring no other data movement.\r\nBecause NxO(N) is O(N<sup>2</sup>), and NxO(1) is O(N), and\r\n  O(N<sup>2</sup>)+O(N) is O(N<sup>2</sup>), adding and then removing\r\n  <b>N</b> values from this implementation of a priority queue also has a\r\n  complexity class O(N<sup>2</sup>). \r\n<p>\r\nIf we instead enqueued the value on the rear and dequeued by searching through\r\n  the array for the highest priority value, we would still have one O(N) term\r\n  and one O(N<sup>2</sup>) term, leading to O(N<sup>2</sup>) as the overall\r\n  complexity class.\r\nBut, later we will learn how implement priority queues with heaps.\r\nBoth <b>enqueue</b> and <b>dequeue</b> are O(log<sub>2</sub>N): worse than\r\n  O(1) but better than O(N).\r\nThus, adding and then removing <b>N</b> values from this implementation of a\r\n  priority queue is  NxO(log<sub>2</sub>N) + NxO(log<sub>2</sub>N) which is\r\n  O(Nlog<sub>2</sub>N) + O(Nlog<sub>2</sub>N) which is O(Nlog<sub>2</sub>N).\r\nSo, \"balancing\" the add and remove operations yields a lower complexity class\r\n  when both operations occur <b>N</b> times.\r\n<p>\r\nFinally, when we use an array to store a collection, each time that we double\r\n  the array we must copy its <b>N</b> values.\r\nBy doubling the size, we do this only log<sub>2</sub> <b>N</b> times when \r\n adding <b>N</b> values, for a total of Nlog<sub>2</sub>N copies;\r\n  therefore, we can think of each addition as requiring log<sub>2</sub>N \r\n  copies (this is called the \"amortized cost\" of this operation: it really\r\n  doesn't occur on every add, but when averaged over all the adds it is\r\n  correct). \r\nSo, in the case of an array implementation of a stack or queue, both \"add\"\r\n  methods are actually O(log<sub>2</sub> N).\r\nBecause NxO(log<sub>2</sub> N) is O(N log<sub>2</sub> N), NxO(1) is O(N), and\r\n  NxO(N) is O(N<sup>2</sup>),\r\nThe stack class is actually O(N log<sub>2</sub> N) for <b>N</b> pushes and\r\n  pops, while the stack class is actually O(N<sup>2</sup>) for <b>N</b>\r\n  enqueues and dequeues.\r\nBy this calculation, the array implementations have a slightly higher\r\n  complexity class than those using linked lists.\r\nBut, because linked lists allocated a new object for every value put in the\r\n  linked list, the running time of collections using linked lists can actually\r\n  be higher.\r\nWe will address this problem again when we cover linked lists.\r\n</td>\r\n</tbody>\r\n</table>\r\n\r\n\r\n<!-- Pragmatics -->\r\n\r\n<a name=\"Pragmatics\"><hr align=\"left\" width=\"33%\">\r\n<table cellspacing=\"0\" cellpadding=\"5\" border=\"0\" rules=\"none\" width=\"100%\">\r\n<tbody>\r\n<tr valign=\"top\">\r\n<td width =\"20%\"><b>Efficiency Pragmatics</b></td>\r\n<td width =\"80%\">\r\nGenerally programmers address efficiency concerns after a program has been\r\n  written clearly and correctly: \"First get it right, then make it fast.\"\r\nSometimes (see below) there is no need to make a program run any faster; other\r\n  times a program must be made to run faster just to test it (if tests cannot\r\n  be performed quickly enough when debugging the program).\r\n<p>\r\nPrograms should run as fast as necessary; making a program run faster often\r\n  requires making it more complicated, more difficult to generalize, etc.\r\nFor example, many scientific programs run in just a few seconds.\r\nIs there a pragmatic reason to work on them to run faster?\r\nNo, because it typically takes a few days to collect the data for the program.\r\nAs a component in the entire task, the program part is already fast enough.\r\nLikewise, programs that require lots of user interaction don't need to be made\r\n  more efficient: if the computer spends less than a tenth of a second between\r\n  the user entering his/her data and the program prompting for more, it is fast\r\n  enough.\r\nFinally, in the pinball animation program, if the model can update and show\r\n  itself in less than a tenth of a second, there is no reason to make it run\r\n  faster; if it cannot, then the animation will be slowed down, and there is a\r\n  reason to improve its performance.\r\n<p>\r\nThe most famous of all rules of thumb for efficiency is the rule of 90/10.\r\nIt states that 90% of the time a program takes to run is a result of executing\r\n  just 10% of its code.\r\nThat is, most time in a program's execution is spent in a small amount of its\r\n  code.\r\nModifying this code is the only way to achieve any significant speedup.\r\n<p>\r\nFor example, suppose a 10,000 line program runs in 1 minute.\r\nBy the rule of 90/10, executing 1,000 lines in this program accounts for 54\r\n  seconds, while executing the remaining 9,000 lines account for only 6\r\n  seconds.\r\nSo, if we could locate and study those 1,000 lines (a small part of the\r\n  program) and get them to execute in half the time, the total program would\r\n  run in 27+6 = 33 seconds, which reduces the execution time for the entire\r\n  program by almost 50%.\r\nIf instead we could study the other 9,000 lines and get them to execute\r\n  instantaneously (admittedly, a very difficult feat!), the total program\r\n  would run in 54+0 = 54 seconds, which reduced the execution time for the\r\n  entire program by ony 10%.\r\nNote that if you randomly change code to improve its efficiency, 90% of the\r\n  time you will not be making any changes resulting in a significant\r\n  improvement.\r\n<p>\r\nThus, a corollary of the 90/10 rule is that for 90% of the code in a program,\r\n  if we make it clearer but less efficient, it will not affect the total\r\n  execution time of the program by much.\r\nIn the above program, if we rewrote the 9,000 lines to make them as clear and\r\n  simple as possible (with no regard for their efficiency) and increased their\r\n  running time by 50% (from 6 to 9 seconds), the total program would run in\r\n  54+9 = 63 seconds, which is only a 5% increase in total execution time.\r\n</tbody>\r\n</table>\r\n<a name=\"Profiling\">\r\n<table cellspacing=\"0\" cellpadding=\"5\" border=\"0\" rules=\"none\" width=\"100%\">\r\n<tbody>\r\n<tr valign=\"top\">\r\n<td width =\"20%\"><b>Profiling</b></td>\r\n<td width =\"80%\">\r\nSo, how does one go about locating the 10% of the program that is\r\n  accounting for 90% of its execution time?\r\nFor large programs, empirical studies show that programmers DO NOT have good\r\n  intuition about where this \"hot\" code is located.\r\nInstead, we should use a tool that computes this information for us.\r\nA <b>profiler</b> is just such a tool.\r\nIt runs our program for us (at a greatly reduced speed) but keeps track of\r\n  either how many times each line is executed or how much time is spent\r\n  executing each line or method (some profilers can collect both kinds of \r\n  information, often collecting more information slows down the program by\r\n  a larger factor).\r\nThen we can examine the results produced by running a program using a profiler\r\n  and learn which code is executing most of the time, and focus on it to\r\n  improve the speed of the entire program.\r\n<p>\r\nJava has a very simple (but not so useful) built-in profiler.\r\nTo use it, select <b>Edit</b> from the <b>Metrowerks CodeWarrior</b> toolbar,\r\n  then select <b>Java Application Release Setting</b>.\r\nIn the <b>Target Settings Panel</b>, click on <b>Java Target</b> and in the\r\n  <b>VM Arguments</b> text field, enter\r\n  <b>-Xrunhprof:cpu=times</b> as is illustrated below.\r\n</tbody>\r\n</table>\r\n  <img src=\"images/profile.gif\"></image>\r\n<table cellspacing=\"0\" cellpadding=\"5\" border=\"0\" rules=\"none\" width=\"100%\">\r\n<tbody>\r\n<tr valign=\"top\">\r\n<td width =\"20%\">&nbsp</td>\r\n<td width =\"80%\">\r\nWhen you run your program, you will get an output file called\r\n  <b>java.hprof.txt</b> which contains some useful performance information\r\n  (beyond the scope of this lecture to explain).\r\nThere are commercial products available to evaluate and display the\r\n  information collected by a profiler in much more sophisticated ways.\r\n<p>\r\nTypically one can speed up a program by a factor of 3-10 very quickly.\r\nFurther gains are slow, unless algorithms from lower complexity classes\r\n  can be found.\r\n</td>\r\n</tbody>\r\n</table>\r\n\r\n\r\n\r\n\r\n\r\n\r\n<!-- Problem Set -->\r\n\r\n<a name=\"ProblemSet\"><hr align=\"left\" width=\"33%\">\r\n<table cellspacing=\"0\" cellpadding=\"5\" border=\"0\" rules=\"none\" width=\"100%\">\r\n<tbody>\r\n<tr valign=\"top\">\r\n<td width =\"20%\"><b>Problem Set</b></td>\r\n<td width =\"80%\">\r\n  To ensure that you understand all the material in this lecture, please solve\r\n    the the announced problems after you read the lecture.\r\n  <p>\r\n  If you get stumped on any problem, go back and read the relevant part of the\r\n     lecture.\r\n  If you still have questions, please get help from the Instructor, a TA,\r\n    or any other student.\r\n\r\n<ol> \r\n<li>When we say an algorithm is O(1) what do we mean?\r\n<p>\r\n<li>Where does the complexity class O( <b>N</b> sqrt(<b>N</b>) ) fit in the\r\n      hierarchy shown in this lecture?\r\n<p>\r\n<li>Suppose that the actual time taken to execute an O(N<sup>2</sup>) sorting\r\n   algorithm on a very  fast machine is <b>2.2x10<sup>-8</sup>N<sup>2</sup></b>\r\n   seconds; now suppose that the actual time taken to execute an\r\n   O(N log<sub>2</sub> N) sorting algorithm on a very slow machine is\r\n   <b>7.2x10<sup>-4</sup>(N log<sub>2</sub> N)</b> seconds (here the slow\r\n    machine is about 10,000 slower than the fast one).\r\n    For what size arrays will the slower machine running the faster algorithm\r\n      sort arrays faster than the faster machine running the slower algorithm?\r\n<p>\r\n<li>Suppose that my <b>Sort</b> class has two methods\r\n<pre><b>  public static void sortByMethod1(Object[] a, Comparator c);\r\n  public static void sortByMethod2(Object[] a, Comparator c);</b></pre>\r\nAfter testing these methods exhaustively, we find that the first method\r\n  works faster than the second for all arrays whose length is less than\r\n  13, and the second works faster for all bigger arrays.\r\nWrite a method name <b>superSort</b> that has the same prototype and always\r\n  runs as fast as the fastest of these two methods.\r\n<p>\r\n<li>Rexamine the code for the first example, computing the maximum\r\n      value stored in an array.\r\n    There is one value (hint: <b>final</b>) that is recomputed, but\r\n      it never changes; we could instead write code that is a bit\r\n      more verbose, but executes fewer machine language instructions.\r\n    Identify the redundancy, write the Java code to fix it, and\r\n      determine <b>Iaw(N)</b> for the new program.\r\n    You might check your result using <b>Mixed</b> mode in the debugger.\r\n    If the body of the loop were longer, would this improvement have\r\n      a bigger or lesser effect overall?\r\n<p>\r\n<li>Write a program that fills an array of length <b>N</b> with random values.\r\n    Then run the <b>maximum</b> code, incrementing a counter whenever a new\r\n      maximum is found.\r\n    Run this program for large values of <b>N</b> and try to infer a formula\r\n      for that approximates how many times this happens.\r\n    Run the program over and over again, with the same large <b>N</b>,\r\n      then with double that size, quadruple that size, etc. to test your\r\n      formula.\r\n<!--\r\nPrecompute a.length and store it.\r\n<b>Iaw(N) = 12N + 10</b>\r\n--->\r\n</ol>\r\n</td>\r\n</tbody>\r\n</table>\r\n\r\n\r\n</body>\r\n</html>\r\n\r\n<!---\r\n  write a program: fill array of <b>N</b> with random numbers 1-N, how many\r\n    values are bigger than all the prior values.\r\n  term + ... \r\n  I belive it is logrithmic\r\n--->", "id": 34458.0}