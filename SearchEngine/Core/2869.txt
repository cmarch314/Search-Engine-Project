{"text": "Main Home Overview Call For Papers Organizers Schedule Invited Speakers Keynotes Accepted Papers Related Links ICML 13 Workshop Machine Learning Meets Crowdsourcing Important Dates ICML Workshop June 21 2 13 Room Lobby 5 6 7 right behind the registration desk Poster session 1 th floor area D map Schedule Keynotes Papers Overview Our ability to solve challenging scientific and engineering problems relies on a mix of human and machine intelligence The machine learning ML research in the past two decades has created a set of powerful theoretical and empirical tools for exploiting machine intelligence On the other side the recent rise of human computation and crowdsourcing approaches enables us to systematically harvest and organize human intelligence for solving problems that are easy for human but difficult for computers The past few years have witnessed widespread use of the crowdsourcing paradigm including task solving platforms like Amazon Mechanical Turk and CrowdFlower crowd powered scientific projects like GalaxyZoo and Foldit game as well as various successful crowdsourcing business such as crowdfunding and open Innovation to name a few This trend yields both new opportunities and challenges for the machine learning community On one side crowdsourcing systems provide machine learning researchers with the ability to gather large amount of valuable data and information leading advances in challenging problems in areas like computer vision and natural language processing On the other side crowdsourcing confronts challenges on increasing its reliability efficiency and scalability for which machine learning can provide power computational tools More importantly building systems that seamlessly integrate machine learning and crowdsourcing techniques can greatly push the frontier of our ability to solve challenging and large scale problems The goal of this workshop is to bring together experts on fields related to crowdsourcing such as economics game theory cognitive science and human computer interaction with the machine learning community to have a workshop focused on areas where crowdsourcing can contribute to machine learning and vice versa We are interested in a wide variety of topics including but not limited to State of the field What are the emerging crowdsourcing tasks and new opportunities for machine learning What are the latest and greatest tasks being tackled by crowdsourcing and human intelligence and how do these tasks highlight the need for new machine learning approaches that aren t being studied already Integrating machine and human intelligence How to build practical systems that seamlessly integrate machine and human intelligence Machine learning algorithms can help the crowdsourcing component to manage work flows and control workers qualities while the crowds can be used to handle the tasks that are difficult for machines to adaptively boost the performance of machine learning algorithms Machine learning for crowdsourcing Many machine learning approaches have been applied to crowdsourcing on problems such as output aggregation quality control work flow management and incentive mechanism design We expect to see more machine learning contribution to crowdsourcing either by novel ML methods or on new crowdsourcing problems Crowdsourcing for machine learning Machine learning largely relies on big and high quality data which can be provided by crowdsourcing systems perhaps in an automatic and adaptive way Also most machine learning algorithms have many design choices that require human intelligence including tuning hyper parameters selecting score functions and designing kernel functions How can we systematically outsource these typically expert level design choices to the crowds in order to achieve results that match expert level human experience Crowdsourcing complicated tasks How to design work flows and aggregate answers in crowdsourcing systems that collect structured labels such as bounding box annotations in computer vision protein folding structures in biology or solve complicated tasks such as proof reading and machine translation How can machine learning provide help in these cases Theoretical analysis There are many open theoretical questions in crowdsourcing that can be addressed by statistics and learning theory Examples include analyzing label aggregation algorithms such as EM or budget allocation strategies Invited Speakers Jeffrey P Bigham University of Rochester Yiling Chen Harvard University Panagiotis G Ipeirotis NYU Stern School of Business Edith Law Harvard University Mark Steyvers UC Irvine Call for Papers Submissions should follow the ICML format and are encouraged to be up to eight pages Papers submitted for review do not need to be anonymized There will be no official proceedings but the accepted papers will be made available on the workshop website Accepted papers will be either presented as a talk or poster We welcome submissions both on novel research work as well as extended abstracts on work recently published or under review in another conference or journal please state the venue of publication in the later case we particularly encourage submission of visionary position papers on the emerging trends on crowdsourcing and machine learning Please submit papers in PDF format here Organizers Paul Bennett Dengyong Zhou John Platt Microsoft Research Redmond Qiang Liu UC Irvine Xi Chen Qihang Lin CMU Abstracts of Invited Talks Jeffrey P Bigham Crowd Agents Interactive Crowd Powered Systems in the Real World Over the past few years we have been developing and deploying interactive crowd powered systems that help people get things done in their everyday lives For instance VizWiz answers visual questions for blind people in less than a minute Legion drives robots in response to natural language commands Chorus supports consistent dialog between end users and the crowd and Scribe converts streaming speech to text in less than five seconds Overall thousands of people have engaged with these systems providing an interesting look at how end users interact with crowd work in their everyday lives These systems have collectively informed a new model for real time crowd work that I call crowd agents which is proving to be especially useful for building interactive crowd powered systems In this model a diverse and changing crowd the kind easily recruited on the web is made to act as a single high quality actor through interface support and computational mediation of each individual s work These systems allow us to deploy truly intelligent interactive systems today and present challenging problems for machine learning going forward to support and eventually replace the humans in the loop Yiling Chen Financial Incentives and Crowd Work Online labor markets such as Amazon Mechanical Turk MTurk have emerged as platforms that facilitate the allocation of productive effort across global economies Many of these markets compensate workers with monetary payments We study the effects of performance contingent financial rewards on work quality and worker effort in MTurk via two experiments We find that the magnitude of performance contingent financial rewards alone affects neither quality nor effort However when workers working on two tasks of the same type in a sequence the change in the magnitude of the reward over the two tasks affects both In particular both work quality and worker effort increase alternatively decrease as the reward increases alternatively decreases for the second task This suggests the existence of the anchoring effect on workers perception of incentives in MTurk and that this effect can be leveraged in workflow design to increase the effectiveness of financial incentives Panagiotis G Ipeirotis Rewarding Crowdsourced Workers We describe techniques for rewarding workers in a crowdsourcing setting We describe a real time monetary payment scheme that rewards workers according to their quality in the presence of uncertainty in quality estimation while at the same time guaranteeing stable or increasing salaries We report experimental results indicating that the proposed scheme encourages long term engagement avoiding churn and avoiding the common problem of adverse selection and moral hazard We also describe a set of non monetary psychological schemes that actively discourage low quality workers from participating in tasks We finish showing that mice and crowdsourced workers are not that different after all Edith Law Mixed Expertise Crowdsourcing To date most of the research in human computation focuses on tasks that can be performed by any person with basic perceptual capabilities and common sense knowledge In this talk I will discuss new directions towards mixed expertise crowdsourcing where the crowd consists of people with drastically different motivations levels and domains of expertise as well as availabilities I will illustrate the new opportunities and challenges in mixed expertise crowdsourcing by outlining existing work and describing my two ongoing projects Curio a micro task marketplace for crowdsourcing scientific tasks and SimplyPut a crowdsourcing platform for improving health literacy through the collaborative summarization of medical information Mark Steyvers Aggregating Human Judgments in Combinatorial Problems We analyze the collective performance of individuals in combinatorial problems involving the rankings of events and items e g what is the order of US presidents as well as traveling salesperson and minimum spanning tree problems We compare situations in which a group of individuals independently answer these questions with an iterated learning environment in which individuals pass their solution to the next person in a chain We introduce Bayesian information aggregation models for both the independent and information sharing environments and treat the collective group knowledge as a latent variable that can be estimated from the observed judgments across individuals The models allow for individual differences in expertise and confidence in other individuals judgments Initial results suggest that information sharing environments lead to better collective performance despite the fact that information sharing increases correlations between judgments In addition the models estimates of expertise are more indicative of actual performance than the users self rated expertise Finally we study situations where the same individual solves the same problem at different points in time We show that the consistency in answers across repeated problems provides an additional signal to estimate expertise Accepted Papers Sivan Sabato Adam Kalai Feature Multi Selection among Subjective Features Adish Singla Andreas Krause Truthful Incentives for Privacy Tradeoff Mechanisms for Data Gathering in Community Sensing Hongwei Li Bin Yu Dengyong Zhou Error Rate Analysis of Labeling by Crowdsourcing Supplementary Peng Ye David Doermann Combining preference and absolute judgements in a crowd sourced setting Vaibhav Rajan Sakyajit Bhattacharya L Elisa Celis Deepthi Chander Koustuv Dasgupta Saraschandra Karanam CrowdControl An online learning approach for optimal task scheduling in a dynamic crowd platform Alexey Tarasov Sarah Jane Delany Brian Mac Namee Improving Performance by Re Rating in the Dynamic Estimation of Rater Reliability Michael Wick Ari Kobren Andrew McCallum Probabilistic Reasoning about Human Edits in Information Integration Joel Lehman Risto Miikkulainen Leveraging Human Computation Markets for Interactive Evolution Jian Peng Qiang Liu Alexander Ihler Bonnie Berger Crowdsourcing for structured labeling with applications to protein folding Related Workshops Conferences and Resources ICML 2 13 Workshop on Machine Learning Meets Crowdsourcing Conference on Human Computation Crowdsourcing HCOMP 2 13 HCOMP 2 13 Workshop on Crowdsourcing at Scale ICML 2 12 Workshop on Machine Learning in Human Computation Crowdsourcing ICML 2 11 Workshop on Combining Learning Strategies to Reduce Label Cost NIPS 2 12 Workshop on Human Computation for Science and Computational Sustainability NIPS 2 11 Workshop on Computational Social Science and the Wisdom of Crowds NIPS 2 1 Workshop on Computational Social Science and the Wisdom of Crowds CVPR 2 1 Workshop on Advancing Computer Vision with Humans in the Loop ACVHL 1st 4th Human Computation Workshop HCOMP CrowdCamp 2 12 2 13 CHI 2 11 Workshop on Crowdsourcing and Human Computation See more information on CrowdResearch org or Mathew Lease s crowdsourcing site Page generated 2 13 1 31 19 29 PDT by jemdoc source ", "_id": "http://www.ics.uci.edu/~qliu1/MLcrowd_ICML_workshop/index.html", "title": "icml &rsquo;13 workshop: machine learning meets crowdsourcing", "html": "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.1//EN\"\n  \"http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\">\n<head>\n<meta name=\"generator\" content=\"jemdoc, see http://jemdoc.jaboc.net/\" />\n<meta http-equiv=\"Content-Type\" content=\"text/html;charset=utf-8\" />\n<link rel=\"stylesheet\" href=\"jemdoc.css\" type=\"text/css\" />\n<title>ICML &rsquo;13 Workshop: Machine Learning Meets Crowdsourcing</title>\n</head>\n<body>\n<table summary=\"Table for page layout.\" id=\"tlayout\">\n<tr valign=\"top\">\n<td id=\"layout-menu\">\n<div class=\"menu-category\">Main</div>\n<div class=\"menu-item\"><a href=\"index.html\" class=\"current\">Home</a></div>\n<div class=\"menu-item\"><a href=\"index.html#Overview\">Overview</a></div>\n<div class=\"menu-item\"><a href=\"index.html#CFP\">Call&nbsp;For&nbsp;Papers</a></div>\n<div class=\"menu-item\"><a href=\"index.html#Organizers\">Organizers</a></div>\n<div class=\"menu-item\"><a href=\"schedule.html\">Schedule</a></div>\n<div class=\"menu-item\"><a href=\"index.html#Speakers\">Invited&nbsp;Speakers</a></div>\n<div class=\"menu-item\"><a href=\"index.html#Keynotes\">Keynotes</a></div>\n<div class=\"menu-item\"><a href=\"index.html#Papers\">Accepted&nbsp;Papers</a></div>\n<div class=\"menu-item\"><a href=\"index.html#Related\">Related&nbsp;Links</a></div>\n</td>\n<td id=\"layout-content\">\n<div id=\"toptitle\">\n<h1>ICML &rsquo;13 Workshop: Machine Learning Meets Crowdsourcing</h1>\n<div id=\"subtitle\"><!--in conjunction with ICML 2013, Atlanta --></div>\n</div>\n<div class=\"infoblock\">\n<div class=\"blockcontent\">\n<p><b>Important Dates:</b> <img src=\"pics/MSR_3.png\" style=\"float:right\" alt=\"MSR\" height=\"90\"></p>\n<ul>\n<li><p><a href=\"http://icml.cc/2013/\">ICML</a> Workshop, June 21, 2013 <br />  </p>\n</li>\n<li><p>Room: Lobby 506-7 (right behind the registration desk)</p>\n</li>\n<li><p>Poster session: 10th floor area D <a href=\"posters_Fri.pdf\">(map)</a> </p>\n</li>\n<li><p><a href=\"schedule.html\"><font color=\"blue\">[Schedule]</font></a> &nbsp; <a href=\"keynotes.html\"><font color=\"blue\">[Keynotes]</font></a> &nbsp;  <a href=\"index.html#Papers\"><font color=\"blue\">[Papers]</font></a> </p>\n</li>\n</ul>\n</div></div>\n<h2><a name='Overview'> Overview </a> </h2>\n<p>Our ability to solve challenging scientific and engineering problems relies on a mix of human and machine intelligence. The machine learning (ML) research in the past two decades has created a set of powerful theoretical and empirical tools for exploiting machine intelligence. On the other side, the recent rise of human computation and crowdsourcing approaches enables us to systematically harvest and organize human intelligence, for solving problems that are easy for human but difficult for computers. \nThe past few years have witnessed widespread use of the crowdsourcing paradigm, including task-solving platforms like Amazon Mechanical Turk and CrowdFlower, crowd-powered scientific projects like GalaxyZoo and Foldit game, as well as various successful crowdsourcing business such as crowdfunding and open Innovation, to name a few.</p>\n<p>This trend yields both new opportunities and challenges for the machine learning community. On one side, crowdsourcing systems provide machine learning researchers with the ability to gather large amount of valuable data and information, leading advances in challenging problems in areas like computer vision and natural language processing. On the other side, crowdsourcing confronts challenges on increasing its reliability, efficiency and scalability, for which machine learning can provide power computational tools. More importantly, building systems that seamlessly integrate machine learning and crowdsourcing techniques can greatly push the frontier of our ability to solve challenging and large-scale problems.</p>\n<p>The goal of this workshop is to bring together experts on fields related to crowdsourcing such as economics, game theory, cognitive science and human-computer interaction with the machine learning community to have a workshop focused on areas where crowdsourcing can contribute to machine learning and vice versa. We are interested in a wide variety of topics, including but not limited to:</p>\n<p><!--The workshop is planned to be primarily discussion oriented around invited talks on already published work with a small amount of original work.  --></p>\n<div class=\"infoblock\">\n<div class=\"blockcontent\">\n<p><b>State of the field.</b> What are the emerging crowdsourcing tasks and new opportunities for machine learning? What are the latest and greatest tasks being tackled by crowdsourcing and human intelligence and how do these tasks highlight the need for new machine learning approaches that aren\u2019t being studied already?</p>\n<p><b>Integrating machine and human intelligence.</b> How to build practical systems that seamlessly integrate machine and human intelligence? Machine learning algorithms can help the crowdsourcing component to manage work flows and control workers\u2019 qualities, while the crowds can be used to handle the tasks that are difficult for machines to adaptively boost the performance of machine learning algorithms. </p>\n<p><!--There are already many practical systems that utilize this hybrid approach, such as SpearkText and VisionIQ.--></p>\n<p><b>Machine learning for crowdsourcing.</b> Many machine learning approaches have been applied to crowdsourcing on problems such as output aggregation, quality control, work flow management and incentive mechanism design. We expect to see more machine learning contribution to crowdsourcing, either by novel ML methods, or on new crowdsourcing problems.</p>\n<p><b>Crowdsourcing for machine learning.</b> Machine learning largely relies on big and high quality data, which can be provided by crowdsourcing systems, perhaps in an automatic and adaptive way. Also, most machine learning algorithms have many design choices that require human intelligence, including tuning hyper-parameters, selecting score functions, and designing kernel functions. \nHow can we systematically &ldquo;outsource&rdquo; these typically expert-level design choices to the crowds in order to achieve results that match expert-level human experience?</p>\n<p><!-- See Tamuz et al. [2011] for an approach that outsources the design of kernel into crowds. --></p>\n<p><b>Crowdsourcing complicated tasks.</b> How to design work flows and aggregate answers in crowdsourcing systems that collect structured labels, such as bounding box annotations in computer vision, protein folding structures in biology, or solve complicated tasks such as proof reading, and machine translation? How can machine learning provide help in these cases?</p>\n<p><b>Theoretical analysis</b>. There are many open theoretical questions in crowdsourcing that can be addressed by statistics and learning theory. Examples include analyzing label aggregation algorithms such as EM, or budget allocation strategies. </p>\n</div></div>\n<h2><a name='Speakers'> Invited Speakers </a>  </h2>\n<p><!-- - Eric Horvitz. Microsoft Research   --></p>\n<p><!-- - Bjorn Hartmann. UC Berkeley  --></p>\n<p><!-- - Michael Bernstein. Stanford  --></p>\n<p><!-- - Nathan Eagle. MIT  --></p>\n<ul>\n<li><p><a href=\"http://www.cs.rochester.edu/~jbigham/\">Jeffrey P. Bigham</a>. University of Rochester</p>\n</li>\n<li><p><a href=\"http://yiling.seas.harvard.edu\">Yiling Chen</a>. Harvard University  </p>\n</li>\n<li><p><a href=\"http://people.stern.nyu.edu/panos/\">Panagiotis G. Ipeirotis</a>. NYU Stern School of Business   </p>\n</li>\n<li><p><a href=\"http://people.seas.harvard.edu/~elaw/Home.html\">Edith Law</a>. Harvard University </p>\n</li>\n<li><p><a href=\"http://psiexp.ss.uci.edu/research/\">Mark Steyvers</a>. UC Irvine  </p>\n</li>\n</ul>\n<p><!-- - Dan Weld. University of Washington  --></p>\n<p><!-- - Michael Kearns. University of Pennsylvania  --></p>\n<h2><a name='CFP'>  Call for Papers </a> </h2>\n<p><!--We solicit contribution of short papers on the interface of machine learning and crowdsourcing.--></p>\n<p><!--Thus, apart from papers reporting novel unpublished work, we also welcome submissions describing work in progress or summarizing a longer paper under review for a journal or conference (this should be clearly stated though).\nSubmissions are sought both for new work in the area of human computation as well as for work recently published or soon to be published in another conference or journal; for submissions of the latter kind, the authors must clearly state the venue of publication.--></p>\n<p><!--We that are more forward looking, and perspectives from a variety of disciplines outside of the core AI community.\nHuman Computation is the study of systems where humans perform a major part of the computation or are an integral part of the overall computational system. Over the past few years, we have observed a proliferation of related workshops, new courses, and tutorials, scattered across many conferences. In this 4th Human Computation Workshop (HCOMP 2012), we hope to draw together participants across disciplines -- machine learning, mechanism and market design, information retrieval, decision-theoretic planning, optimization, human computer interaction -- for a stimulating full-day workshop at AAAI in the vibrant metropolitan Toronto this summer. There will be presentation of new works, lively discussions, poster and demo sessions, and invited talks. We welcome early work, and particularly encourage submission of visionary position papers that are more forward looking, and perspectives from a variety of disciplines outside of the core AI community. There will be presentation of new works, lively discussions, poster and demo sessions, and invited talks. We hope to see you there--></p>\n<p>Submissions should follow the <a href=\"http://icml.cc/2013/wp-content/uploads/2012/12/icml2013stylefiles.tar.gz\">ICML format</a> and are encouraged to be up to eight pages. <!--plus an additional fifth page for references.--> \nPapers submitted for review do not need to be anonymized. \nThere will be no official proceedings, but the accepted papers will be made available on the workshop website. Accepted papers will be either presented as a talk or poster. </p>\n<p>We welcome submissions both on novel research work as well as extended abstracts on work recently published or under review in another conference or journal (please state the venue of publication in the later case); we particularly encourage submission of visionary position papers on the emerging trends on crowdsourcing and machine learning. </p>\n<p>Please submit papers in PDF format <a href=\"https://cmt.research.microsoft.com/MLCROWD2013/\">here</a>. </p>\n<p><!--For questions or problems, please email lqiang67+MLcrowd_workshop AT gmail.com. --></p>\n<h2><a name='Organizers'> Organizers </a> </h2>\n<ul>\n<li><p><a href=\"http://research.microsoft.com/en-us/um/people/pauben/\">Paul Bennett</a>, \n<a href=\"http://research.microsoft.com/en-us/um/people/denzho/\">Dengyong Zhou</a>, \n<a href=\"http://research.microsoft.com/en-us/people/jplatt/\">John Platt</a>. Microsoft Research, Redmond </p>\n</li>\n</ul>\n<ul>\n<li><p><a href=\"http://www.ics.uci.edu/~qliu1/\">Qiang Liu</a>. UC Irvine</p>\n</li>\n</ul>\n<ul>\n<li><p><a href=\"http://www.cs.cmu.edu/~xichen/\">Xi Chen</a>, \n<a href=\"http://www.contrib.andrew.cmu.edu/~qihangl/\">Qihang Lin</a>. CMU </p>\n</li>\n</ul>\n<h2><a name='Keynotes'> Abstracts of Invited Talks </a> </h2>\n<h3>Jeffrey P. Bigham : Crowd Agents: Interactive Crowd-Powered Systems in the Real World </h3>\n<p>Over the past few years, we have been developing and deploying\ninteractive crowd-powered systems that help people get things done in\ntheir everyday lives. For instance, VizWiz answers visual questions\nfor blind people in less than a minute, Legion drives robots in\nresponse to natural language commands, Chorus supports consistent\ndialog between end users and the crowd, and Scribe converts streaming\nspeech to text in less than five seconds. Overall, thousands of people\nhave engaged with these systems, providing an interesting look at how\nend users interact with crowd work in their everyday lives. These\nsystems have collectively informed a new model for real-time crowd\nwork that I call \u201ccrowd agents,\u201d which is proving to be especially\nuseful for building interactive crowd-powered systems. In this model,\na diverse and changing crowd \u2013 the kind easily recruited on the web \u2013\nis made to act as a single high-quality actor through interface\nsupport and computational mediation of each individual\u2019s work. These\nsystems allow us to deploy truly intelligent interactive systems\ntoday, and present challenging problems for machine learning going\nforward to support and eventually replace the humans in the loop.</p>\n<h3>Yiling Chen: Financial Incentives and Crowd Work  </h3>\n<p>Online labor markets such as Amazon Mechanical Turk (MTurk) have emerged as platforms that facilitate the allocation of productive effort across global economies. Many of these markets compensate workers with monetary payments. We study the effects of performance-contingent financial rewards on work quality and worker effort in MTurk via two experiments. We find that the magnitude of performance-contingent financial rewards alone affects neither quality nor effort. However, when workers working on two tasks of the same type in a sequence, the change in the magnitude of the reward over the two tasks affects both. In particular, both work quality and worker effort increase (alternatively decrease) as the reward increases (alternatively decreases) for the second task. This suggests the existence of the anchoring effect on workers&rsquo; perception of incentives in MTurk and that this effect can be leveraged in workflow design to increase the effectiveness of financial incentives.</p>\n<h3>Panagiotis G. Ipeirotis : Rewarding Crowdsourced Workers </h3>\n<p>We describe techniques for rewarding workers in a\ncrowdsourcing setting. We describe a real-time monetary payment scheme\nthat rewards workers according to their quality, in the presence of\nuncertainty in quality estimation, while at the same time guaranteeing\nstable (or increasing) salaries. We report experimental results\nindicating that the proposed scheme encourages long-term engagement,\navoiding churn, and avoiding the common problem of adverse selection\nand moral hazard. We also describe a set of non-monetary,\npsychological schemes that actively discourage low-quality workers\nfrom participating in tasks. We finish showing that mice and\ncrowdsourced workers are not that different after all.</p>\n<h3>Edith Law :  Mixed-Expertise Crowdsourcing </h3>\n<p>To date, most of the research in human computation focuses on tasks that can be performed by any person with basic perceptual capabilities and common sense knowledge.  In this talk, I will discuss new directions towards mixed-expertise crowdsourcing, where the crowd consists of people with drastically different motivations, levels and domains of expertise, as well as availabilities.  I will illustrate the new opportunities and challenges in mixed-expertise crowdsourcing, by outlining existing work and describing my two ongoing projects &ndash; Curio, a micro-task marketplace for crowdsourcing scientific tasks, and SimplyPut, a crowdsourcing platform for improving health literacy through the collaborative summarization of medical information.</p>\n<h3>Mark Steyvers: Aggregating Human Judgments in Combinatorial Problems </h3>\n<p>We analyze the collective performance of individuals in combinatorial\nproblems involving the rankings of events and items (e.g. &ldquo;what is the\norder of US presidents?&rdquo;) as well as traveling salesperson and minimum\nspanning tree problems. We compare situations in which a group of\nindividuals independently answer these questions with an iterated\nlearning environment in which individuals pass their solution to the\nnext person in a chain. We introduce Bayesian information aggregation\nmodels for both the independent and information-sharing environments\nand treat the collective group knowledge as a latent variable that can\nbe estimated from the observed judgments across individuals. The\nmodels allow for individual differences in expertise and confidence in\nother individuals&rsquo; judgments. Initial results suggest that\ninformation-sharing environments lead to better collective performance\ndespite the fact that information-sharing increases correlations\nbetween judgments. In addition, the models\u2019 estimates of expertise are\nmore indicative of actual performance than the users\u2019 self-rated\nexpertise. Finally, we study situations where the same individual\nsolves the same problem at different points in time. We show that the\nconsistency  in answers across repeated problems provides an\nadditional signal to estimate expertise.</p>\n<h2><a name='Papers'> Accepted Papers </a> </h2>\n<ul>\n<li><p>Sivan Sabato, Adam Kalai; <a href=\"Papers/ActivePaper2.pdf\">Feature Multi-Selection among Subjective Features</a>. </p>\n</li>\n</ul>\n<ul>\n<li><p>Adish Singla, Andreas Krause; <a href=\"Papers/ActivePaper3.pdf\">Truthful Incentives for Privacy Tradeoff: Mechanisms for Data Gathering in Community Sensing</a>.</p>\n</li>\n</ul>\n<ul>\n<li><p>Hongwei Li, Bin Yu, Dengyong Zhou; <a href=\"Papers/ActivePaper4.pdf\">Error Rate Analysis of Labeling by Crowdsourcing</a>; <a href=\"Papers/ActivePaper4.pdf\">(Supplementary)</a>.</p>\n</li>\n</ul>\n<ul>\n<li><p>Peng Ye, David Doermann; <a href=\"Papers/ActivePaper5.pdf\">Combining preference and absolute judgements in a crowd-sourced setting</a>.</p>\n</li>\n</ul>\n<ul>\n<li><p>Vaibhav Rajan, Sakyajit Bhattacharya, L. Elisa Celis, Deepthi Chander, Koustuv Dasgupta, Saraschandra Karanam; <a href=\"Papers/ActivePaper6.pdf\">CrowdControl: An online learning approach for optimal task scheduling in a dynamic crowd platform</a>.</p>\n</li>\n</ul>\n<ul>\n<li><p>Alexey Tarasov,  Sarah Jane Delany, Brian Mac Namee; <a href=\"Papers/ActivePaper7.pdf\">Improving Performance by Re-Rating in the Dynamic Estimation of Rater Reliability</a>.</p>\n</li>\n</ul>\n<ul>\n<li><p>Michael Wick, Ari Kobren, Andrew McCallum; <a href=\"Papers/ActivePaper8.pdf\">Probabilistic Reasoning about Human Edits in Information Integration</a>.</p>\n</li>\n</ul>\n<ul>\n<li><p>Joel Lehman, Risto Miikkulainen; <a href=\"Papers/ActivePaper9.pdf\">Leveraging Human Computation Markets for Interactive Evolution</a>.</p>\n</li>\n</ul>\n<ul>\n<li><p>Jian Peng, Qiang Liu, Alexander Ihler, Bonnie Berger; <a href=\"Papers/ActivePaper12.pdf\">Crowdsourcing for structured labeling with applications to protein folding</a>.</p>\n</li>\n</ul>\n<h2><a name='Related'> Related Workshops, Conferences and Resources </a>  </h2>\n<ul>\n<li><p><a href=\"http://www.ics.uci.edu/~qliu1/MLcrowd_ICML_workshop/\">ICML 2013 Workshop on Machine Learning Meets Crowdsourcing.</a></p>\n</li>\n<li><p><a href=\"http://www.humancomputation.com/2013/\">Conference on Human Computation & Crowdsourcing (HCOMP), 2013.</a></p>\n</li>\n<li><p><a href=\"https://sites.google.com/site/crowdscale2013/\">HCOMP 2013 Workshop on Crowdsourcing at Scale.</a></p>\n</li>\n<li><p><a href=\"http://crowdml12.wordpress.com\">ICML 2012 Workshop on Machine Learning in Human Computation & Crowdsourcing.</a></p>\n</li>\n<li><p><a href=\"https://sites.google.com/site/comblearn/program\">ICML 2011 Workshop on Combining Learning Strategies to Reduce Label Cost.</a></p>\n</li>\n<li><p><a href=\"http://nips.cc/Conferences/2012/Program/event.php?ID=3140\">NIPS 2012 Workshop on Human Computation for Science and Computational Sustainability.</a></p>\n</li>\n<li><p><a href=\"http://nips.cc/Conferences/2011/Program/event.php?ID=2522\">NIPS 2011 Workshop on Computational Social Science and the Wisdom of Crowds.</a></p>\n</li>\n<li><p><a href=\"http://nips.cc/Conferences/2010/Program/event.php?ID=1990\">NIPS 2010 Workshop on Computational Social Science and the Wisdom of Crowds.</a></p>\n</li>\n<li><p><a href=\"http://filebox.ece.vt.edu/~parikh/acvhl2010.htm\">CVPR 2010 Workshop on Advancing Computer Vision with Humans in the Loop (ACVHL)</a></p>\n</li>\n<li><p><a href=\"http://www.humancomputation.com/2012/About_the_Workshop.html\">1st-4th Human Computation Workshop (HCOMP).</a></p>\n</li>\n<li><p><a href=\"http://crowdresearch.org/crowdcamp/indexCHI2012.html/\">CrowdCamp 2012</a>, <a href=\"http://crowdresearch.org/crowdcamp/\">2013</a>.</p>\n</li>\n<li><p><a href=\"http://crowdresearch.org/chi2011-workshop/\">CHI 2011 Workshop on Crowdsourcing and Human Computation</a></p>\n</li>\n<li><p>See more information on  <a href=\"http://crowdresearch.org\">CrowdResearch.org</a> or <a href=\"http://ir.ischool.utexas.edu/crowd/\">Mathew Lease's crowdsourcing site.</a></p>\n</li>\n</ul>\n<div id=\"footer\">\n<div id=\"footer-text\">\nPage generated 2013-10-31 00:19:29 PDT, by <a href=\"http://jemdoc.jaboc.net/\">jemdoc</a>.\n(<a href=\"index.jemdoc\">source</a>)\n</div>\n</div>\n</td>\n</tr>\n</table>\n</body>\n</html>\n", "id": 2869.0}