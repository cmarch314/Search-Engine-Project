{"text": "Articulated Human Detection with Flexible Mixtures of Parts Yi Yang and Deva Ramanan Introduction We describe a method for human pose estimation in static images based on a novel representation of part models Notably we do not use articulated limb parts but rather capture orientation with a mixture of templates for each part We describe a general flexible mixture model for capturing contextual co occurrence relations between parts augmenting standard spring models that encode spatial relations We show that such relations can capture notions of local rigidity When co occurrence and spatial relations are tree structured our model can be efficiently optimized with dynamic programming We present experimental results on standard benchmarks for pose estimation that indicate our approach is the state of the art system for pose estimation outperforming past work while being orders of magnitude faster Download The latest copy of our code DOWNLOAD Please read the README file for proper installation Publications Yi Yang Deva Ramanan Articulated Pose Estimation with Flexible Mixtures of Parts IEEE Conference on Computer Vision and Pattern Recognition CVPR Colorado Spring USA 2 11 Paper Slides Poster Talk BibTex Examplar Results Related Links Object Detection with Discriminatively Trained Part Based Models Face Detection Pose Estimation and Landmark Localization in the Wild Buffy Stikckmen for 2D Human Pose Estimation Leeds Sports Pose Dataset for Human Pose Estimation", "_id": "http://www.ics.uci.edu/~yyang8/research/pose/index.html", "title": "articulated human detection with flexible mixtures of parts - uc irvine", "html": "<html>\n<head>\n<title>Articulated Human Detection with Flexible Mixtures of Parts - UC Irvine</title>\n<style>\nbody\n{\n\tfont-family : Arial;\n}\n#container\n{\n\twidth : 900px;\n\tmargin : 20px auto;\n\tbackground-color : #fff;\n\tpadding : 20px;\n}\nh1 strong\n{\n\tfont-size : 40px;\n}\nh1\n{\n\tmargin : 0;\n\tpadding : 0;\n\tfont-size : 30px;\n}\ncode\n{\n\tborder : 1px solid #ccc;\n\tdisplay : block;\n\tpadding : 5px;\n\tmargin : 10px;\n}\n#demoframe\n{\n\tborder : 1px solid #ccc;\n\tpadding: 0px;\n\tmargin : 0;\n\twidth : 100%;\n\theight: 320px;\n}\n</style>\n<!--<script type=\"text/javascript\">\n\n  var _gaq = _gaq || [];\n  _gaq.push(['_setAccount', 'UA-17813713-3']);\n  _gaq.push(['_setDomainName', '.mit.edu']);\n  _gaq.push(['_trackPageview']);\n\n  (function() {\n    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;\n    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';\n    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);\n  })();\n\n</script>-->\n</head>\n\n\n<body>\n<div id=\"container\">\n\n<h1 style=\"text-align:center;\"><strong>A</strong>rticulated <strong>H</strong>uman <strong>D</strong>etection with <strong>F</strong>lexible <strong>M</strong>ixtures of <strong>P</strong>arts</h1>\n\n<p style=\"text-align:center;\"><a href=\"http://www.ics.uci.edu/~yyang8\">Yi Yang</a> and <a href=\"http://www.ics.uci.edu/~dramanan\">Deva Ramanan</a></p>\n\n<!-- <p style=\"text-align:center;\">Under Construction, Please visit <a href=\"http://phoenix.ics.uci.edu/software/pose/\">here</a></p> -->\n\n<div style=\"margin-top : 30px;\"></div>\n\n<h2>Introduction</h2>\n<p>We describe a method for human pose estimation in static images based on a novel representation of part models. Notably, we do not use articulated limb parts, but rather capture orientation with a mixture of templates for each part. We describe a general, flexible mixture model for capturing contextual co-occurrence relations between parts, augmenting standard spring models that encode spatial relations. We show that such relations can capture notions of local rigidity. When co-occurrence and spatial relations are tree-structured, our model can be efficiently optimized with dynamic programming. We present experimental results on standard benchmarks for pose estimation that indicate our approach is the state-of-the-art system for pose estimation, outperforming past work while being orders of magnitude faster.</p>\n\n<!--<div style=\"float : right; text-align : center; margin-left : 20px; margin-right: 5px; border : 2px solid black;\">\n<img src=\"pose2011.jpg\" style=\"height : 256px;\">\n</div>-->\n\n<div style=\"text-align : center; padding-top : 0px; padding-bottom : 0px; margin-left : 0px; margin-right: 0px; border : 0px solid black; background-color : #FFF;\">\n<img src=\"figs/splash.jpg\" style=\"height : 300px;\">\n</div>\n\n<!-- \n<h2>Demo</h2>\n<iframe src=\"http://deepthought.ics.uci.edu:8000/?embed=1\" id=\"demoframe\"></iframe> \n-->\n\n<h2>Download</h2>\n\n<p>The latest copy of our code: <a href=\"code/pose-v1.3.zip\">DOWNLOAD</a></p>\n\n<p>Please read the <a href=\"code/README\">README</a> file for proper installation.</p>\n\n<h2>Publications</h2>\n\n<p>Yi Yang, Deva Ramanan. \"<strong>Articulated Pose Estimation with Flexible Mixtures of Parts</strong>\". <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Colorado Spring, USA, 2011.</em> <a href=\"pose2011.pdf\">[Paper]</a><a href=\"pose2011_slides.pptx\">[Slides]</a><a href=\"pose2011_poster.pdf\">[Poster]</a><a href=\"http://techtalks.tv/events/56/44/\">[Talk]</a><a href=\"pose2011.bib\">[BibTex]</a> </p>\n\n<h2>Examplar Results</h2>\n\n<center>\n<table align=\"center\" border=\"0\" cellspacing=\"0\" cellpadding=\"2\">\n<tr>\n<td align=\"center\"><img src=\"figs/PARSE_box_gr/0017\" height=\"201\"></td>\n<td align=\"center\"><img src=\"figs/PARSE_ske_gr/0017\" height=\"201\"></td>\n<td align=\"center\"><img src=\"figs/PARSE_box_gr/0010\" height=\"201\"></td>\n<td align=\"center\"><img src=\"figs/PARSE_ske_gr/0010\" height=\"201\"></td>\n<td align=\"center\"><img src=\"figs/PARSE_box_gr/0125\" height=\"201\"></td>\n<td align=\"center\"><img src=\"figs/PARSE_ske_gr/0125\" height=\"201\"></td>\n</tr>\n</table>\n\n<table align=\"center\" border=\"0\" cellspacing=\"0\" cellpadding=\"2\">\n<tr>\n<td align=\"center\"><img src=\"figs/PARSE_box_gr/0105\" height=\"199\"></td>\n<td align=\"center\"><img src=\"figs/PARSE_ske_gr/0105\" height=\"199\"></td>\n<td align=\"center\"><img src=\"figs/PARSE_box_gr/0133\" height=\"199\"></td>\n<td align=\"center\"><img src=\"figs/PARSE_ske_gr/0133\" height=\"199\"></td>\n<td align=\"center\"><img src=\"figs/PARSE_box_gr/0086\" height=\"199\"></td>\n<td align=\"center\"><img src=\"figs/PARSE_ske_gr/0086\" height=\"199\"></td>\n</tr>\n</table>\n\n<table align=\"center\" border=\"0\" cellspacing=\"0\" cellpadding=\"2\">\n<tr>\n<td align=\"center\"><img src=\"figs/PARSE_box_gr/0037\" height=\"175\"></td>\n<td align=\"center\"><img src=\"figs/PARSE_ske_gr/0037\" height=\"175\"></td>\n<td align=\"center\"><img src=\"figs/PARSE_box_gr/0153\" height=\"175\"></td>\n<td align=\"center\"><img src=\"figs/PARSE_ske_gr/0153\" height=\"175\"></td>\n</tr>\n</table>\n</center>\n\n<!-- <br clear=\"both\"> -->\n\n<h2>Related Links</h2>\n<ul>\n<li> <a href=\"http://www.cs.brown.edu/~pff/latent/\">Object Detection with Discriminatively Trained Part Based Models</a></li> \n<li> <a href=\"http://www.ics.uci.edu/~xzhu/face/\">Face Detection, Pose Estimation and Landmark Localization in the Wild</a></li>\n<li> <a href=\"http://www.robots.ox.ac.uk/~vgg/data/stickmen/\">Buffy Stikckmen for 2D Human Pose Estimation</a></li>\n<li> <a href=\"http://www.comp.leeds.ac.uk/mat4saj/lsp.html\">Leeds Sports Pose Dataset for Human Pose Estimation</a></li>\n</ul>\n\n<!--\n<h2>Update History</h2>\n<center>\n<table style=\"align:center; border:1px solid black; border-spacing:0px; width:100%\">\n<tr>\n<th style=\"border:1px solid black; width:20%\"><strong>Date</strong></th>\n<th style=\"border:1px solid black; width:80%\"><strong>Description</strong></th>\n</tr>\n<tr>\n<td style=\"border:1px solid black; width:20%\">7/21/2012</td>\n<td style=\"border:1px solid black; width:80%\">Hello World</td>\n</tr>\n</table>\n</center>\n\n\n<h2>License</h2>\n<p>Copyright &copy; 2011 Yi Yang and Deva Ramanan</p>\n<p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p>\n<ul>\n<li>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</li>\n</ul>\n<p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>\n-->\n</div>\n\n</body>\n</html>\n\n", "id": 16085.0}