{"text": "Qiang Liu home Publications CV PDF TA CS178 Qiang Liu Qiang Liu Ph D Candidate Advisor Prof Alexander Ihler Information Computer Science University of California at Irvine qliu1 at uci edu Qiang sounds like Chee ah ng and Liu as l yo New I will be an assistant professor in the Department of Computer Science at Dartmouth College starting in Summer 2 15 PhD positions are available Please email me if interested Workshops co organized Crowdsourcing Theory Algorithms and Applications NIPS 13 Machine Learning Meets Crowdsourcing ICML 13 Research My research area is machine learning and statistics with interests spreading over the pipeline of data collection mainly crowdsourcing learning inference decision making and various applications under the framework of probabilistic graphical models Crowdsourcing All machine learning processes start from data collection Crowdsourcing is a modern approach to collect large amounts of labeled data by hiring anonymous workers through online platforms such as Amazon Mechanical Turk Unfortunately the crowdsourced workers are often unreliable and uncontrollable raising many challenging computational questions such as how to aggregate labels from workers with different expertise how to combine and balance noisy but cheap crowdsourced labels and accurate but expensive expert labels and how to crowdsource complicated objectives such as protein structures We reform the problem of aggregating crowdsourced labels into a standard inference problem on a factor graph which we solve using a class of variational inference algorithms We show that both the na ve majority voting method and a previous algorithm by Karger et al 2 12 are special cases of one of our belief propagation type algorithms with special priors We demonstrate significant improvement on the performance by using better priors see NIPS2 12 code Control items with known answers can be used to evaluate workers performance and hence improve the combined results on the target items with unknown answers This raises the problem of how many control items to use when the total number of items each workers can answer is limited more control items evaluates the workers better but leaves fewer resources for the target items that are of direct interest and vice versa We perform theoretical analysis and provide surprisingly simple answers for this problem see here A preliminary thought on combining structured labels such as the protein folding see here Learning Learning refers to constructing probabilistic models from empirical data either to estimate the model parameters with predefined model structures or even to estimate the model structures solely from data I am interested in developing efficient possibly distributed learning algorithms that perform well on real world data Here is an efficient distributed learning algorithm based on smartly combining local estimators defined by pseudo likelihood components ICML2 12 Here is a structure learning algorithm for recovering scale free networks thought to appear commonly in the real world AISTATS2 11 notable paper award Here are some earlier works on contrastive divergence and MCMC MLE ICML2 1 AISTATS2 1 Inference With given graphical models either handcrafted or learned from data inference refers to answering queries such as marginal probability or partition function maximum a posteriori MAP estimation or marginal MAP the hybrid of marginalization and MAP I am interested in developing efficient inference algorithms mostly based on variational methods and in the form of belief propagation like message passing algorithms Marginal MAP is notoriously difficult even on tree structured graphs We developed a general variational dual representation for marginal MAP and propose a set of variational approximation algorithms including an interesting mixed product BP that is a hybrid of max product sum product and a special argmax product message updates and a convergent proximal point algorithm that works by iteratively solving pure marginalization tasks See JMLR2 13 UAI2 11 Slides We proposed an efficient approximate inference algorithm for calculating the log partition function that unifies Rina Dechter s one pass mini bucket algorithm with iterative variational algorithms such as tree reweighted BP Our method inherits the advantages of both and easily scales to large clique sizes Our algorithm can provide both upper and lower bounds for the log partition function See ICML2 11 Tree reweighted BP provides an upper bound on the log partition function while na ve mean field and structured mean field give lower bounds We show that tree reweighted BP provably gives a lower bound if its weights are set to take negative values in a particular way We also show that such negative tree reweighted BP reduces to structured mean field as the weights approach infinity For the full story see UAI2 1 Structured decision making In practice we often need to take a sequence of actions to achieve a predefined goal usually under uncertain environments where information is observed sequentially and interactively as we progress Decision networks also called influence diagrams are graphical model style representations of such structured decision making problems under uncertainty Just like Bayesian networks generalize Markov chains or hidden Markov chains decision networks generalize Markov decision processes MDP or partially observable decision processes POMDP Unfortunately the problem of finding the optimal actions for decision networks is much more challenging than answering queries on Bayesian networks especially in cases where limited information is observed or where multi agent cooperation is required such as in robot soccer games We extend the powerful variational inference framework for solving decision networks based on which we propose an efficient BP type algorithm and a convergent proximal point algorithm Our framework enables us to translate basically any variational algorithm to solve influence diagrams See UAI2 12 Applications I am interested in applying these machine learning methods in many application areas Natural language processing How well can computers solve the SAT sentence completion question This is the work I involved when I was interning in Microsoft Research Redmond ACL2 12 Sensor networks Here is a distributed algorithm for learning parameters in sensor networks ICML2 12 My algorithm for solving influence diagrams provides a powerful way to design optimal decentralized detection networks UAI2 12 Bioinformatics PNAS2 12 AISTATS2 11 Bioinformatics2 1 Page generated 2 14 7 25 19 13 38 PDT by jemdoc source ", "_id": "http://www.ics.uci.edu/~qliu1/", "title": "qiang liu", "html": "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.1//EN\"\n  \"http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\">\n<head>\n<meta name=\"generator\" content=\"jemdoc, see http://jemdoc.jaboc.net/\" />\n<meta http-equiv=\"Content-Type\" content=\"text/html;charset=utf-8\" />\n<link rel=\"stylesheet\" href=\"jemdoc.css\" type=\"text/css\" />\n<title>Qiang Liu</title>\n</head>\n<body>\n<table summary=\"Table for page layout.\" id=\"tlayout\">\n<tr valign=\"top\">\n<td id=\"layout-menu\">\n<div class=\"menu-category\">Qiang Liu</div>\n<div class=\"menu-item\"><a href=\"index.html\" class=\"current\">home</a></div>\n<div class=\"menu-item\"><a href=\"publication.html\">Publications</a></div>\n<div class=\"menu-item\"><a href=\"PDF/qiang_cv.pdf\">CV&nbsp;(PDF)</a></div>\n<div class=\"menu-item\"><a href=\"TA_CS178.html\">TA&nbsp;(CS178)</a></div>\n</td>\n<td id=\"layout-content\">\n<div id=\"toptitle\">\n<h1>Qiang Liu</h1>\n</div>\n<table class=\"imgtable\"><tr><td>\n<a href=\"IMGLINKTARGET\"><img src=\"pics/QiangMS.jpg\" alt=\"alt text\" width=\"123px\" height=\"171px\" /></a>&nbsp;</td>\n<td align=\"left\"><p>Qiang Liu <br />\nPh.D. Candidate <br />\nAdvisor: <a href=\"http://www.ics.uci.edu/~ihler/\">Prof. Alexander Ihler</a><br />\n<a href=\"http://www.ics.uci.edu/\">Information &amp; Computer Science</a> <br />\n<a href=\"http://www.uci.edu/\">University of California at Irvine</a> <br />\n<a href=\"mailto:qliu@uci.edu\">qliu1(at)uci.edu</a></p>\n<p><font size=\"2\" ><i> (\"Qiang\" sounds like \"Chee-ah-ng\", and \"Liu\" as \"l-yo\")<i></font>  </p>\n<p><!-- I am in my 5th year of my PhD. I am currently supported by a [http://research.microsoft.com/en-us/collaboration/global/northam/northam-fellows.aspx   Microsoft Research PhD fellowship].--></p>\n</td></tr></table>\n<p><br /></p>\n<p><b> <font color=\"red\">New.      </font> </b></p>\n<p>I will be an assistant professor in the \n<a href=\"http://web.cs.dartmouth.edu\">Department of Computer Science</a>\nat \n<a href=\"http://dartmouth.edu\">Dartmouth College</a> starting in Summer 2015. </p>\n<p><font color=\"red\"> PhD positions are available</font>.  <font color=\"red\">Please</font> <a href=\"mailto:Qiang.Liu@dartmouth.edu\"><font color=\"red\">email me</font></a>  <font color=\"red\">if interested.</font>\n<br />\n<br /></p>\n<p>Workshops co-organized: </p>\n<ul>\n<li><p><a href=\"http://www.ics.uci.edu/~qliu1/nips13_workshop/\">Crowdsourcing: Theory, Algorithms and Applications, NIPS&rsquo;13</a>, </p>\n</li>\n<li><p><a href=\"http://www.ics.uci.edu/~qliu1/MLcrowd_ICML_workshop/\">Machine Learning Meets Crowdsourcing, ICML&rsquo;13</a>.</p>\n</li>\n</ul>\n<p><!-- I co-organized a NIPS'13 workshop on [http://www.ics.uci.edu/~qliu1/nips13_workshop/ Crowdsourcing: Theory, Algorithms and Applications].  \n\n[http://www.ics.uci.edu/~qliu1/MLcrowd_ICML_workshop/  Here] is a related workshop I organized previously.--></p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </p>\n<p><!-- I co-organized an ICML'13 workshop on [http://www.ics.uci.edu/~qliu1/MLcrowd_ICML_workshop/ Machine Learning Meets Crowdsourcing].  --></p>\n<h2>Research</h2>\n<p>My research area is machine learning and statistics, with interests spreading over the pipeline of \n<a href=\"http://www.ics.uci.edu/~qliu1/#crowdsourcing\">data collection</a> (mainly <a href=\"http://www.ics.uci.edu/~qliu1/#crowdsourcing\">crowdsourcing</a>), \n<a href=\"http://www.ics.uci.edu/~qliu1/#learning\">learning</a>, \n<a href=\"http://www.ics.uci.edu/~qliu1/#inference\">inference</a>, \n<a href=\"http://www.ics.uci.edu/~qliu1/#decisionmaking\">decision making</a>, \nand various <a href=\"http://www.ics.uci.edu/~qliu1/#application\">applications</a> \nunder the framework of probabilistic graphical models.</p>\n<div class=\"infoblock\">\n<div class=\"blockcontent\">\n<p><b><a name='crowdsourcing'> Crowdsourcing</a>.</b> \nAll machine learning processes start from data collection. Crowdsourcing is a modern approach to collect large amounts of labeled data by hiring anonymous workers through online platforms such as Amazon Mechanical Turk. \nUnfortunately, the crowdsourced workers are often unreliable and uncontrollable, raising many challenging computational questions, such as how to aggregate labels from workers with different expertise, how to combine and balance noisy (but cheap) crowdsourced labels and accurate (but expensive) expert labels, and how to crowdsource complicated objectives such as protein structures. </p>\n<ul>\n<li><p>We reform the problem of aggregating crowdsourced labels into a standard inference problem on a factor graph, which we solve using a class of variational inference algorithms. We show that both the na&iuml;ve majority voting method and a previous algorithm by Karger et al. 2012 are special cases of one of our belief-propagation-type algorithms with special priors. We demonstrate significant improvement on the performance by using better priors, see <a href = \"http://www.ics.uci.edu/~qliu1/PDF/crowdsrc_nips12.pdf\">NIPS2012</a>, <a href = \"http://www.ics.uci.edu/~qliu1/codes/crowd_tool.zip\">code.</a> </p>\n</li>\n</ul>\n<ul>\n<li><p>Control items with known answers can be used to evaluate workers&rsquo; performance, and hence improve the combined results on the target items with unknown answers. This raises the problem of how many control items to use when the total number of items each workers can answer is limited: more control items evaluates the workers better, but leaves fewer resources for the target items that are of direct interest, and vice versa. We perform theoretical analysis and provide surprisingly simple answers for this problem, see <a href = \"http://www.ics.uci.edu/~qliu1/PDF/main_nips2013.pdf\">here.</a></p>\n</li>\n</ul>\n<ul>\n<li><p>A preliminary thought on combining structured labels such as the protein folding, see <a href = \"http://www.ics.uci.edu/~qliu1/MLcrowd_ICML_workshop/Papers/ActivePaper12.pdf\">here.</a></p>\n</li>\n</ul>\n<p><!-- We study the problem of estimating continuous quantities, such as prices, probabilities, and point spreads, using a crowdsourcing approach. A challenging aspect of combining the crowd\u2019s answers is that workers\u2019 reliabilities and biases are usu- ally unknown and highly diverse. \nWe give theoretical results for this problem under different scenarios, and provide a simple rule of thumb for crowdsourcing practitioners. As a byproduct, we also provide theoretical analysis of the accuracy of different consensus methods. --></p>\n</div></div>\n<div class=\"infoblock\">\n<div class=\"blockcontent\">\n<p><b><a name='learning'> Learning</a>.</b> \nLearning refers to constructing probabilistic models from empirical data, either to estimate the model parameters with predefined model structures, or even to estimate the model structures solely from data? I am interested in developing efficient, possibly distributed, learning algorithms, that perform well on real world data.</p>\n<ul>\n<li><p>Here is an efficient distributed learning algorithm based on smartly combining local estimators defined by pseudo-likelihood components: \n<a href=\"http://www.ics.uci.edu/~qliu1/PDF/distributed_mple_fit.pdf\">ICML2012</a>.</p>\n</li>\n</ul>\n<ul>\n<li><p>Here is a structure learning algorithm for recovering scale-free networks, thought to appear commonly in the real world: \n<a href=\"http://www.ics.uci.edu/~qliu1/PDF/aistats11.pdf\">AISTATS2011</a> (notable paper award).</p>\n</li>\n</ul>\n<ul>\n<li><p>Here are some earlier works on contrastive divergence and MCMC-MLE: \n<a href=\"http://www.ics.uci.edu/~ihler/papers/icml10.pdf\">ICML2010</a>; \n<a href=\"http://www.ics.uci.edu/~ihler/papers/aistats10.pdf\">AISTATS2010</a>.</p>\n</li>\n</ul>\n</div></div>\n<div class=\"infoblock\">\n<div class=\"blockcontent\">\n<p><b><a name='inference'> Inference</a>.</b>\nWith given graphical models, either handcrafted or learned from data, inference refers to answering queries, such as marginal probability (or partition function), maximum a posteriori (MAP) estimation, or marginal MAP, the hybrid of marginalization and MAP. I am interested in developing efficient inference algorithms, mostly based on variational methods and in the form of belief-propagation-like message passing algorithms.</p>\n<ul>\n<li><p>Marginal MAP is notoriously difficult even on tree-structured graphs. We developed a general variational dual representation for marginal MAP, and propose a set of variational approximation algorithms, including an interesting &ldquo;mixed-product&rdquo; BP that is a hybrid of max-product, sum-product and a special &ldquo;argmax-product&rdquo; message updates, and a convergent proximal point algorithm that works by iteratively solving pure marginalization tasks. See \n<a href=\"http://www.ics.uci.edu/~qliu1/PDF/marginalMAP_jmlr.pdf\">JMLR2013</a>; \n<a href=\"http://www.ics.uci.edu/~qliu1/PDF/uai11.pdf\">UAI2011</a>\n<a href=\"http://www.ics.uci.edu/~qliu1/PDF/marginalMAP_only.pdf\">(Slides)</a>.</p>\n</li>\n</ul>\n<ul>\n<li><p>We proposed an efficient approximate inference algorithm for calculating the log-partition function that unifies Rina Dechter's &ldquo;one-pass&rdquo; mini-bucket algorithm with iterative variational algorithms, such as tree reweighted BP. Our method inherits the advantages of both, and easily scales to large clique sizes. Our algorithm can provide both upper and lower bounds for the log-partition function. See <a href=\"http://www.ics.uci.edu/~qliu1/PDF/icml11.pdf\">ICML2011</a>.</p>\n</li>\n</ul>\n<ul>\n<li><p>Tree reweighted BP provides an upper bound on the log-partition function, while na&iuml;ve mean field and structured mean field give lower bounds. We show that tree reweighted BP provably gives a lower bound if its weights are set to take negative values in a particular way. We also show that such &ldquo;negative&rdquo; tree reweighted BP reduces to structured mean field as the weights approach infinity. For the full story, see <a href=\"http://www.ics.uci.edu/~qliu1/PDF/neg_weights.pdf\">UAI2010</a>.</p>\n</li>\n</ul>\n</div></div>\n<div class=\"infoblock\">\n<div class=\"blockcontent\">\n<p><b><a name='decisionmaking'> Structured decision making</a>.</b> \nIn practice, we often need to take a sequence of actions to achieve a predefined goal, usually under uncertain environments where information is observed sequentially and interactively as we progress. Decision networks (also called influence diagrams) are graphical model style representations of such structured decision making problems under uncertainty. Just like Bayesian networks generalize Markov chains or hidden Markov chains, decision networks generalize Markov decision processes (MDP), or partially observable decision processes (POMDP). Unfortunately, the problem of finding the optimal actions for decision networks is much more challenging than answering queries on Bayesian networks, especially in cases where limited information is observed or where multi-agent cooperation is required (such as in robot soccer games).</p>\n<ul>\n<li><p>We extend the powerful variational inference framework for solving decision networks, based on which we propose an efficient BP-type algorithm and a convergent proximal point algorithm. Our framework enables us to translate basically any variational algorithm to solve influence diagrams. See <a href = \"http://www.ics.uci.edu/~qliu1/PDF/uai12_meu.pdf\">UAI2012.</a></p>\n</li>\n</ul>\n</div></div>\n<div class=\"infoblock\">\n<div class=\"blockcontent\">\n<p><b><a name='application'> Applications</a>.</b>  I am interested in applying these machine learning methods in many application areas.</p>\n<ul>\n<li><p>Natural language processing:</p>\n<ul>\n<li><p>How well can computers solve the SAT sentence completion question? This is the work I involved when I was interning in Microsoft Research Redmond, <a href = \"http://research.microsoft.com/apps/pubs/default.aspx?id=163344\">ACL2012.</a></p>\n</li>\n</ul>\n\n</li>\n</ul>\n<ul>\n<li><p>Sensor networks:</p>\n<ul>\n<li><p>Here is a distributed algorithm for learning parameters in sensor networks: <a href = \"http://www.ics.uci.edu/~qliu1/PDF/distributed_mple_fit.pdf\"> ICML2012</a>.</p>\n</li>\n<li><p>My algorithm for solving influence diagrams provides a powerful way to design optimal decentralized detection networks: <a href = \"http://www.ics.uci.edu/~qliu1/PDF/uai12_meu.pdf\"> UAI2012</a>.</p>\n</li>\n</ul>\n\n</li>\n</ul>\n<ul>\n<li><p>Bioinformatics:</p>\n<ul>\n<li><p><a href = \"http://www.pnas.org/content/109/29/11758.long\"> PNAS2012</a>; <a href = \"http://www.ics.uci.edu/~qliu1/PDF/aistats11.pdf\"> AISTATS2011</a>;   <a href = \"http://bioinformatics.oxfordjournals.org/content/26/6/770.full.pdf+html\">Bioinformatics2010 </a>. </p>\n</li>\n</ul>\n\n</li>\n</ul>\n</div></div>\n<p><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /></p>\n<p>\n<div id=\"clustrmaps-widget\"></div><script type=\"text/javascript\">var _clustrmaps = {'url' : 'http://www.ics.uci.edu/~qliu1', 'user' : 1080620, 'server' : '4', 'id' : 'clustrmaps-widget', 'version' : 1, 'date' : '2013-02-23', 'lang' : 'en', 'corners' : 'square' };(function (){ var s = document.createElement('script'); s.type = 'text/javascript'; s.async = true; s.src = 'http://www4.clustrmaps.com/counter/map.js'; var x = document.getElementsByTagName('script')[0]; x.parentNode.insertBefore(s, x);})();</script><noscript><a href=\"http://www4.clustrmaps.com/user/294107d2c\"><img src=\"http://www4.clustrmaps.com/stats/maps-no_clusters/www.ics.uci.edu-~qliu1-thumb.jpg\" alt=\"Locations of visitors to this page\" /></a></noscript>\n</p>\n<div id=\"footer\">\n<div id=\"footer-text\">\nPage generated 2014-07-25 19:13:38 PDT, by <a href=\"http://jemdoc.jaboc.net/\">jemdoc</a>.\n(<a href=\"index.jemdoc\">source</a>)\n</div>\n</div>\n</td>\n</tr>\n</table>\n</body>\n</html>\n", "id": 4386.0}